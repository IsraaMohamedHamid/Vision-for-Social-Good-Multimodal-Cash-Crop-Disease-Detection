{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from ft_transformer import FTTransformer\n",
    "from transformers import BertModel\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet18, attention_augmented_inceptionv3,attention_augmented_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA' \n",
    "crop_root = os.path.join(base_dir, 'color') # color tester\n",
    "split_root = os.path.join(base_dir, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "csv_path = os.path.join(base_dir, 'plant_disease_multimodal_dataset.csv')  # '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA/plant_disease_multimodal_dataset.csv'\n",
    "csv_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the image paths and labels from the features\n",
    "csv_image_paths = csv_data['Image Path'].values\n",
    "csv_labels = csv_data['Mapped Label'].values\n",
    "csv_features = csv_data.drop(columns=['Image Path', 'Mapped Label', 'Label']).values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove .DS_Store files\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file is an image file\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(base_dir, val_split=0.4, test_split=0.1):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "\n",
    "    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    for cls in classes:\n",
    "        print(f'Processing class: {cls}')\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "\n",
    "        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n",
    "\n",
    "        if len(images) == 0:\n",
    "            print(f\"No images found for class {cls}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Shuffle images to randomize the selection\n",
    "        random.shuffle(images)\n",
    "\n",
    "        try:\n",
    "            train, test = train_test_split(images, test_size=test_split)\n",
    "            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n",
    "        except ValueError as e:\n",
    "            print(f\"Not enough images to split for class {cls}: {e}\")\n",
    "            continue\n",
    "\n",
    "        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n",
    "        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n",
    "        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n",
    "\n",
    "    return train_files, val_files, test_files, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Corn_(maize)___healthy\n",
      "Processing class: Tomato___Target_Spot\n",
      "Processing class: Tomato___Late_blight\n",
      "Processing class: Tomato___Tomato_mosaic_virus\n",
      "Processing class: Pepper,_bell___healthy\n",
      "Processing class: Orange___Haunglongbing_(Citrus_greening)\n",
      "Processing class: Tomato___Leaf_Mold\n",
      "Processing class: Tomato___Bacterial_spot\n",
      "Processing class: Tomato___Early_blight\n",
      "Processing class: Corn_(maize)___Common_rust_\n",
      "Processing class: Tomato___healthy\n",
      "Processing class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "Processing class: Corn_(maize)___Northern_Leaf_Blight\n",
      "Processing class: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "Processing class: Pepper,_bell___Bacterial_spot\n",
      "Processing class: Tomato___Septoria_leaf_spot\n",
      "Processing class: Squash___Powdery_mildew\n",
      "Processing class: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "Processing class: Soybean___healthy\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_files, val_files, test_files, classes = split_data(crop_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 18444\n",
      "Validation files: 14769\n",
      "Test files: 3700\n"
     ]
    }
   ],
   "source": [
    "# Use the lists of file paths for your dataset loading and transformations\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard image sizes\n",
    "inception_size = 299\n",
    "other_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the data transformations\n",
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the datasets and data loaders and to map between the different modalities\n",
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, file_paths, csv_features, csv_labels, class_to_idx, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with image paths, CSV features, labels, class mapping, and optional transforms.\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list of tuples): List of (image_path, class_label) tuples.\n",
    "            csv_features (ndarray): Array of CSV feature rows.\n",
    "            csv_labels (ndarray): Array of CSV labels corresponding to csv_features.\n",
    "            class_to_idx (dict): Mapping from class labels to indices.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.csv_features = csv_features\n",
    "        self.csv_labels = csv_labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, csv_row, label) where image is the transformed image tensor,\n",
    "                   csv_row is the corresponding CSV feature row, and label is the class index.\n",
    "        \"\"\"\n",
    "        img_path, cls = self.file_paths[idx]  # Get image path and class label\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "        label = self.class_to_idx[cls]  # Map class label to index\n",
    "        csv_row = self.csv_features[idx]  # Get the corresponding CSV feature row\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply image transformations if provided\n",
    "        \n",
    "        return image, csv_row, label  # Return the image, CSV features, and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset_inception = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n",
    "val_dataset_inception = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n",
    "test_dataset_inception = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n",
    "test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for other models\n",
    "train_dataset_others = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['train'])\n",
    "val_dataset_others = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['val'])\n",
    "test_dataset_others = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['test'])\n",
    "\n",
    "train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n",
    "val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n",
    "test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV feature extractor models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TabNet CSV feature extractor\n",
    "class TabNetCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TabNetCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Desired output dimension of the feature extractor.\n",
    "        \"\"\"\n",
    "        super(TabNetCSVFeatureExtractor, self).__init__()\n",
    "        self.tabnet = TabNetClassifier(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=hidden_dim,  # Set output dimension to match desired feature size\n",
    "            n_d=hidden_dim,\n",
    "            n_a=hidden_dim,\n",
    "            n_steps=3,\n",
    "            gamma=1.3,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=1e-3,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_fn=None,\n",
    "            scheduler_params=None,\n",
    "            mask_type='sparsemax'\n",
    "        )\n",
    "        # Placeholder for a linear layer if needed to match dimensions\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure x is on CPU since TabNetClassifier might not support GPU\n",
    "        x = x.cpu().numpy()\n",
    "        # Predict on input features\n",
    "        predictions = self.tabnet.predict(x)\n",
    "        # Convert predictions to tensor\n",
    "        feature_output = torch.tensor(predictions, dtype=torch.float32)\n",
    "        # Optionally pass through a linear layer if needed\n",
    "        return self.linear(feature_output)\n",
    "\n",
    "# Define the FT-Transformer CSV feature extractor\n",
    "# class FTTransformerCSVFeatureExtractor(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         \"\"\"\n",
    "#         Initializes the FTTransformerCSVFeatureExtractor.\n",
    "\n",
    "#         Args:\n",
    "#             input_dim (int): Number of input features.\n",
    "#             hidden_dim (int): Number of hidden units.\n",
    "#         \"\"\"\n",
    "#         super(FTTransformerCSVFeatureExtractor, self).__init__()\n",
    "#         self.ft_transformer = FTTransformer(input_dim=input_dim, output_dim=hidden_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.ft_transformer(x)\n",
    "\n",
    "# Define the Multimodal Transformer (BERT-based) CSV feature extractor\n",
    "class BERTCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the BERTCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(BERTCSVFeatureExtractor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BERT expects tokenized input, so we need to process x appropriately\n",
    "        # Assuming x is tokenized and of shape (batch_size, seq_length)\n",
    "        output = self.bert(input_ids=x)[1]  # [1] corresponds to the pooled output\n",
    "        return self.fc(output)\n",
    "\n",
    "# Define the MLP CSV feature extractor\n",
    "class MLP_CSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the MLP_CSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(MLP_CSVFeatureExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Simple MLP CSV feature extractor\n",
    "class SimpleCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(SimpleCSVFeatureExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)\n",
    "\n",
    "class DeepCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DeepCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(DeepCSVFeatureExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)\n",
    "\n",
    "class ConvCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len):\n",
    "        \"\"\"\n",
    "        Initializes the ConvCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "            seq_len (int): Length of the input sequence.\n",
    "        \"\"\"\n",
    "        super(ConvCSVFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(hidden_dim * seq_len, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, model_name, base_model, csv_input_dim, csv_hidden_dim, num_classes, fusion_method, csv_model_type='simple', seq_len=None):\n",
    "        \"\"\"\n",
    "        Initializes the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the base model architecture.\n",
    "            base_model (nn.Module): The base model to be used.\n",
    "            csv_input_dim (int): Number of features in the CSV data.\n",
    "            csv_hidden_dim (int): Number of hidden units in the CSV feature extractor.\n",
    "            num_classes (int): Number of classes for classification.\n",
    "            fusion_method (str): Method of fusion ('early', 'intermediate', 'late').\n",
    "            csv_model_type (str): Type of CSV feature extractor ('simple', 'deep', 'conv').\n",
    "            seq_len (int, optional): Length of the input sequence for convolutional model.\n",
    "        \"\"\"\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.base_model = base_model\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # Initialize the base model and get the feature size\n",
    "        self.base_model, self.feature_size = self.initialize_base_model(model_name, base_model)\n",
    "\n",
    "        # Define the CSV feature extractor based on the type\n",
    "        if csv_model_type == 'simple':\n",
    "            self.csv_feature_extractor = SimpleCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim)\n",
    "        elif csv_model_type == 'deep':\n",
    "            self.csv_feature_extractor = DeepCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim)\n",
    "        elif csv_model_type == 'conv':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported csv_model_type\")\n",
    "\n",
    "        # Define additional layers for fusion based on the fusion method\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            # Calculate the new feature size after early fusion\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_base_model(self, model_name, base_model):\n",
    "        \"\"\"\n",
    "        Initializes the base model for fusion, replacing the final classification layer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the base model architecture.\n",
    "            base_model (nn.Module): The base model to be modified.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (modified_base_model, feature_size)\n",
    "        \"\"\"\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = base_model.fc.in_features  # Access in_features before replacing\n",
    "            base_model.aux_logits = False  # Disable auxiliary logits\n",
    "            base_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            base_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = base_model.fc.in_features  # Access in_features before replacing\n",
    "            base_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = base_model.classifier[6].in_features  # Access in_features before replacing\n",
    "            base_model.classifier[6] = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ViT':\n",
    "            # Generalized approach to identify and replace the classification head\n",
    "            if hasattr(base_model, 'heads'):\n",
    "                feature_size = base_model.heads.head.in_features\n",
    "                base_model.heads.head = nn.Identity()\n",
    "            elif hasattr(base_model, 'classifier'):\n",
    "                feature_size = base_model.classifier.in_features\n",
    "                base_model.classifier = nn.Identity()\n",
    "            elif hasattr(base_model, 'head'):\n",
    "                feature_size = base_model.head.in_features\n",
    "                base_model.head = nn.Identity()\n",
    "            else:\n",
    "                # Fallback: Inspect all attributes and find a suitable final layer\n",
    "                for attr_name in dir(base_model):\n",
    "                    attr = getattr(base_model, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(base_model, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = base_model.inception\n",
    "            feature_size = inception_model.fc.in_features  # Access in_features before replacing\n",
    "            inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "            inception_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            inception_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            # Access the VGG model within the wrapper\n",
    "            vgg_model = base_model.features\n",
    "            feature_size = vgg_model[-2].out_channels  # Assuming the penultimate layer is the feature extractor\n",
    "            # Replace the last layer with an identity function\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            # Access the ResNet model within the wrapper\n",
    "            resnet_model = base_model\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return base_model, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        \"\"\"\n",
    "        Forward pass for the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): Image tensor input.\n",
    "            csv (Tensor): CSV feature input.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        # Extract CSV features\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            # Early fusion: Concatenate features before passing through the base model\n",
    "            img = img.view(img.size(0), -1)  # Flatten the image tensor\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)  # Concatenate image and CSV features\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)  # Pass through the early fusion layer\n",
    "            output = self.classifier(img_csv_features)  # Classify the fused features\n",
    "        else:\n",
    "            img_features = self.base_model(img)  # Extract image features using the base model\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                # Check if img_features is of type InceptionOutputs and extract the tensor\n",
    "                if isinstance(img_features, tuple):  # Handle Inception model outputs\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)  # Flatten the image features\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)  # Concatenate image and CSV features\n",
    "                output = self.fusion_layer(combined_features)  # Classify the fused features\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)  # Pass image features through intermediate layer\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)  # Concatenate features\n",
    "                output = self.fusion_layer(combined_features)  # Classify the fused features\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output  # Return the output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel1(nn.Module):\n",
    "    def __init__(self, model_name, base_model, csv_input_dim, csv_hidden_dim, num_classes, fusion_method):\n",
    "        \"\"\"\n",
    "        Initializes the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the base model architecture.\n",
    "            base_model (nn.Module): The base model to be used.\n",
    "            csv_input_dim (int): Number of features in the CSV data.\n",
    "            csv_hidden_dim (int): Number of hidden units in the CSV feature extractor.\n",
    "            num_classes (int): Number of classes for classification.\n",
    "            fusion_method (str): Method of fusion ('early', 'intermediate', 'late').\n",
    "        \"\"\"\n",
    "        super(FusionModel1, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.base_model = base_model\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # Initialize the base model and get the feature size\n",
    "        self.base_model, self.feature_size = self.initialize_base_model(model_name, base_model)\n",
    "\n",
    "        # Define the CSV feature extractor\n",
    "        self.csv_feature_extractor = nn.Sequential(\n",
    "            nn.Linear(self.csv_input_dim, self.csv_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.csv_hidden_dim, self.csv_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Define additional layers for fusion based on the fusion method\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            # Calculate the new feature size after early fusion\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_base_model(self, model_name, base_model):\n",
    "        \"\"\"\n",
    "        Initializes the base model for fusion, replacing the final classification layer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the base model architecture.\n",
    "            base_model (nn.Module): The base model to be modified.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (modified_base_model, feature_size)\n",
    "        \"\"\"\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = base_model.fc.in_features  # Access in_features before replacing\n",
    "            base_model.aux_logits = False  # Disable auxiliary logits\n",
    "            base_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            base_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = base_model.fc.in_features  # Access in_features before replacing\n",
    "            base_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = base_model.classifier[6].in_features  # Access in_features before replacing\n",
    "            base_model.classifier[6] = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ViT':\n",
    "            # Generalized approach to identify and replace the classification head\n",
    "            if hasattr(base_model, 'heads'):\n",
    "                feature_size = base_model.heads.head.in_features\n",
    "                base_model.heads.head = nn.Identity()\n",
    "            elif hasattr(base_model, 'classifier'):\n",
    "                feature_size = base_model.classifier.in_features\n",
    "                base_model.classifier = nn.Identity()\n",
    "            elif hasattr(base_model, 'head'):\n",
    "                feature_size = base_model.head.in_features\n",
    "                base_model.head = nn.Identity()\n",
    "            else:\n",
    "                # Fallback: Inspect all attributes and find a suitable final layer\n",
    "                for attr_name in dir(base_model):\n",
    "                    attr = getattr(base_model, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(base_model, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = base_model.inception\n",
    "            feature_size = inception_model.fc.in_features  # Access in_features before replacing\n",
    "            inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "            inception_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            inception_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            # Access the VGG model within the wrapper\n",
    "            vgg_model = base_model.features\n",
    "            feature_size = vgg_model[-2].out_channels  # Assuming the penultimate layer is the feature extractor\n",
    "            # Replace the last layer with an identity function\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            # Access the ResNet model within the wrapper\n",
    "            resnet_model = base_model\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return base_model, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        \"\"\"\n",
    "        Forward pass for the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): Image tensor input.\n",
    "            csv (Tensor): CSV feature input.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        # Extract CSV features\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            # Early fusion: Concatenate features before passing through the base model\n",
    "            img = img.view(img.size(0), -1)  # Flatten the image tensor\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)  # Concatenate image and CSV features\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)  # Pass through the early fusion layer\n",
    "            output = self.classifier(img_csv_features)  # Classify the fused features\n",
    "        else:\n",
    "            img_features = self.base_model(img)  # Extract image features using the base model\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                # Check if img_features is of type InceptionOutputs and extract the tensor\n",
    "                if isinstance(img_features, tuple):  # Handle Inception model outputs\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)  # Flatten the image features\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)  # Concatenate image and CSV features\n",
    "                output = self.fusion_layer(combined_features)  # Classify the fused features\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)  # Pass image features through intermediate layer\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)  # Concatenate features\n",
    "                output = self.fusion_layer(combined_features)  # Classify the fused features\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output  # Return the output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion method model\n",
    "class FusionModel2(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_feature_extractor, num_classes, fusion_method):\n",
    "        \"\"\"\n",
    "        Initializes the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the image feature extractor architecture.\n",
    "            image_feature_extractor (nn.Module): The model for extracting features from images.\n",
    "            csv_feature_extractor (nn.Module): The model for extracting features from CSV data.\n",
    "            num_classes (int): Number of classes for classification.\n",
    "            fusion_method (str): Method of fusion ('early', 'intermediate', 'late').\n",
    "        \"\"\"\n",
    "        super(FusionModel2, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_feature_extractor = csv_feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # Initialize the image feature extractor and get the feature size\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        # Initialize the CSV feature extractor\n",
    "        # Determine csv_hidden_dim based on the CSV feature extractor\n",
    "        if hasattr(self.csv_feature_extractor, 'output_dim'):\n",
    "            self.csv_hidden_dim = self.csv_feature_extractor.output_dim\n",
    "        else:\n",
    "            # Default or custom handling if 'output_dim' is not present\n",
    "            self.csv_hidden_dim = 512  # Example default\n",
    "\n",
    "        # Define additional layers for fusion based on the fusion method\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            # Calculate the new feature size after early fusion\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        \"\"\"\n",
    "        Initializes the image feature extractor for fusion, replacing the final classification layer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the image feature extractor architecture.\n",
    "            image_feature_extractor (nn.Module): The model to be modified.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (modified_image_feature_extractor, feature_size)\n",
    "        \"\"\"\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.aux_logits = False  # Disable auxiliary logits\n",
    "            image_feature_extractor.AuxLogits = None  # Remove auxiliary logits\n",
    "            image_feature_extractor.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ViT':\n",
    "            # Generalized approach to identify and replace the classification head\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                # Fallback: Inspect all attributes and find a suitable final layer\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features  # Access in_features before replacing\n",
    "            inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "            inception_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            inception_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            # Access the VGG model within the wrapper\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels  # Assuming the penultimate layer is the feature extractor\n",
    "            # Replace the last layer with an identity function\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            # Access the ResNet model within the wrapper\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        # Extract features from the image using the image feature extractor\n",
    "        img_features = self.image_feature_extractor(img)\n",
    "        \n",
    "        # Handle specific cases for certain models\n",
    "        if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "            # Check if img_features is of type InceptionOutputs and extract the tensor\n",
    "            if isinstance(img_features, tuple):  # Handle Inception model outputs\n",
    "                img_features = img_features.logits\n",
    "                img_features = img_features.view(img_features.size(0), -1)\n",
    "        \n",
    "        # Extract features from the CSV data\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        # Perform fusion based on the specified method\n",
    "        if self.fusion_method == 'early':\n",
    "            # Early fusion: Concatenate features before passing through the base model\n",
    "            img = img.view(img.size(0), -1)  # Flatten the image tensor\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        elif self.fusion_method == 'late':\n",
    "            # Late fusion: Combine features after processing through image feature extractor and CSV extractor\n",
    "            combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "            output = self.fusion_layer(combined_features)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            # Intermediate fusion: Process image features through an intermediate layer before fusion\n",
    "            img_features = self.intermediate_layer(img_features)\n",
    "            combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "            output = self.fusion_layer(combined_features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache function\n",
    "def clear_cache():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.cache.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient accumulation\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=5e-4, momentum=0.9)\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs_img, inputs_csv, labels = data\n",
    "                inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "                outputs = model(inputs_img, inputs_csv)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_fusion_model(model, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    return train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method=fusion_method, num_epochs=num_epochs, initial_lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the fusion model\n",
    "def evaluate_fusion_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of classes\n",
    "num_classes_inception = len(class_to_idx)\n",
    "num_classes_others = len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features in the CSV data\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv input dimensions\n",
    "csv_input_dim = csv_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv hidden dimensions\n",
    "csv_hidden_dim = 256  # Adjust based on your model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results dictionary\n",
    "crop_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length for BERT\n",
    "seq_len = 128  # Adjust this value according to your dataset and BERT model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Fusion Method: intermediate --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- CNN Base Model: InceptionV3 ----------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor simple\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all combinations of fusion methods, CNN models, and CSV feature extractors\n",
    "for fusion_method in ['intermediate', 'late']:\n",
    "    # Define CSV feature extractor models\n",
    "    csv_feature_extractors = {\n",
    "        # 'tabnet': lambda: TabNetCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "        # 'ft_transformer': lambda: FTTransformerCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "        'bert': lambda: BERTCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "        'mlp': lambda: MLP_CSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "    }\n",
    "\n",
    "    # Define CNN models with pretrained weights\n",
    "    cnn_feature_extractors = {\n",
    "        'InceptionV3': models.inception_v3(pretrained=True).to(device),\n",
    "        'ResNet152': models.resnet152(pretrained=True).to(device),\n",
    "        'VGG19': models.vgg19(pretrained=True).to(device),\n",
    "        'ViT': ViT(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_classes=num_classes_others,\n",
    "            dim=1024,\n",
    "            depth=6,\n",
    "            heads=16,\n",
    "            mlp_dim=2048,\n",
    "            dropout=0.1,\n",
    "            emb_dropout=0.1\n",
    "        ).to(device),\n",
    "        \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n",
    "        # 'AttentionAugmentedVGG19': attention_augmented_vgg('VGG19', num_classes=num_classes_others).to(device),\n",
    "        # \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n",
    "    }\n",
    "\n",
    "    # Disable auxiliary logits for InceptionV3\n",
    "    if 'InceptionV3' in cnn_feature_extractors:\n",
    "        cnn_feature_extractors['InceptionV3'].aux_logits = False\n",
    "\n",
    "    print(f'-------------------------- Fusion Method: {fusion_method} --------------------------')\n",
    "    for model_name, base_model in cnn_feature_extractors.items():\n",
    "        base_model.to(device)  # Ensure the CNN model is on the correct device\n",
    "\n",
    "        # Loop over each CSV feature extractor model type\n",
    "        # for csv_model_type, csv_feature_extractor_factory in csv_feature_extractors.items():\n",
    "        \n",
    "        print(f'---------------- CNN Base Model: {model_name} ----------------')\n",
    "        for csv_model_type in ['simple','deep','conv']:\n",
    "            print(f'Training {model_name} with {fusion_method} fusion and CSV extractor {csv_model_type}')\n",
    "\n",
    "            # Initialize the CSV feature extractor using the factory function\n",
    "            # csv_feature_extractor = csv_feature_extractor_factory()\n",
    "\n",
    "            # Create the FusionModel\n",
    "            fusion_model = FusionModel(\n",
    "                model_name=model_name,\n",
    "                base_model=base_model,\n",
    "                csv_input_dim=csv_input_dim,\n",
    "                csv_hidden_dim=csv_hidden_dim,\n",
    "                # image_feature_extractor=base_model,\n",
    "                # csv_feature_extractor=csv_feature_extractor,\n",
    "                num_classes=num_classes_others,\n",
    "                fusion_method=fusion_method,\n",
    "                csv_model_type=csv_model_type, \n",
    "                seq_len=seq_len\n",
    "            )\n",
    "\n",
    "            # Train the FusionModel\n",
    "            model = create_and_train_fusion_model(\n",
    "                fusion_model,\n",
    "                train_loader_others,\n",
    "                val_loader_others,\n",
    "                num_classes_others,\n",
    "                csv_input_dim,\n",
    "                device,\n",
    "                fusion_method,\n",
    "                initial_lr=0.001\n",
    "            )\n",
    "\n",
    "            # Evaluate the trained FusionModel\n",
    "            test_loss, test_accuracy = evaluate_fusion_model(model, test_loader_others, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "            # Store results for this model configuration\n",
    "            crop_results[f\"{model_name}_{fusion_method}_{csv_model_type}\"] = {\n",
    "                'model': model,\n",
    "                'model_name': model_name,\n",
    "                'fusion_method': fusion_method,\n",
    "                'csv_model_type': csv_model_type,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }\n",
    "            print(f'{model_name} with {fusion_method} fusion and CSV extractor {csv_model_type} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "            # Clean up: delete the model to free up memory (optional)\n",
    "            del model\n",
    "            clear_cache()\n",
    "        print('\\n')\n",
    "    print('----------------------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/RESULTS/Multimodal\"\n",
    "results_folder = os.path.join(results_base_dir, 'T1')\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(os.path.join(results_folder, filename))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_info in results.items():\n",
    "    print(fusion_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "def plot_accuracy_comparison(results):\n",
    "    accuracies = [result['test_accuracy'] for result in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.show()\n",
    "    save_figure(fig, 'all_fusion_accuracy_comparison.png')\n",
    "\n",
    "    if fusion_method == 'late':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'late_accuracy_comparison.png')\n",
    "    elif fusion_method == 'intermediate':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'intermediate_accuracy_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "plot_accuracy_comparison(crop_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader_inception, test_loader_others):\n",
    "        \n",
    "    metrics_data = []\n",
    "    \n",
    "    for model_name, model_info in results.items():\n",
    "\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        device = next(model.parameters()).device  # Get the device of the model\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, csv_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)  # Display the DataFrame in Jupyter Notebook\n",
    "    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, num_images=5):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    class_labels = list(test_loader.dataset.class_to_idx.keys())\n",
    "    \n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images]  # Move tensors to the model's device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "    # fig.suptitle(f'{model_name} - Classification Results', fontsize=28)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))  # Move tensor back to CPU for visualization\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')  # Access CPU tensor for labels\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_classification_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "    \n",
    "    print(f'Displaying results for {model_name}')\n",
    "    display_classification_results(results[model_name]['model'], test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader, model_name):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predicted, target_names=list(test_loader.dataset.class_to_idx.keys()))\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(report)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "        \n",
    "    print(f'Displaying classification report for {model_name}')\n",
    "    display_classification_report([model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    # fig.suptitle(f'{model_name} - Confusion Matrix\\n', fontsize=28, y=0.83)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "    fig.delaxes(fig.axes[1])  # Delete colorbar\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Predicted Label', fontsize=50)\n",
    "    plt.ylabel('True Label', fontsize=50)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all labels and predictions\n",
    "def get_all_labels_and_preds(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrices\n",
    "def generate_confusion_matrices(results, test_loader_inception, test_loader_others):\n",
    "\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n",
    "        plot_confusion_matrix(labels, pred_labels, classes, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrices(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most incorrect predictions\n",
    "def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n",
    "    rows = int(np.ceil(np.sqrt(n_images)))\n",
    "    cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 20))\n",
    "    # fig.suptitle(f'{model_name} - Most Incorrect\\n', fontsize=28)\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        if i >= len(incorrect):\n",
    "            break\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n",
    "                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_most_incorrect.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu())\n",
    "\n",
    "    return all_images, all_labels, all_preds, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images to display\n",
    "N_IMAGES = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to get the details\n",
    "def plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36):\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n",
    "        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n",
    "        incorrect_examples = []\n",
    "\n",
    "        for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "            if not correct:\n",
    "                incorrect_examples.append((image, label, prob))\n",
    "\n",
    "    incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n",
    "    plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_incorrect_predictions(crop_results, test_loader_inception, test_loader_others, N_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, manifold\n",
    "\n",
    "def get_representations(model, iterator):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            labels.append(y)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(data, n_components=2):\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_representations(data, labels, classes, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    # fig.suptitle(f'{model_name} - PCA', fontsize=28, y=0.95)\n",
    "    ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='hsv')\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_pca.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, labels = get_representations(model, train_loader)\n",
    "for model_name in crop_results.keys():\n",
    "    output_pca_data = get_pca(outputs)\n",
    "    plot_representations(output_pca_data, labels, classes)  # Adjusted to pass only three arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne(data, n_components=2, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "    tsne = manifold.TSNE(n_components=n_components, random_state=0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in crop_results.keys():\n",
    "    output_tsne_data = get_tsne(outputs)\n",
    "    plot_representations(output_tsne_data, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot filtered images\n",
    "def plot_filtered_images(images, filters, model_name, n_filters=None, normalize=True):\n",
    "    images = torch.cat([i.unsqueeze(0) for i in images], dim=0).cpu()\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    if n_filters is not None:\n",
    "        filters = filters[:n_filters]\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    filtered_images = F.conv2d(images, filters)\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    # fig.suptitle(f'{model_name} - Filtered Images', fontsize=28, y=0.8)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        image = images[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters))\n",
    "        ax.imshow(image.permute(1, 2, 0).numpy())\n",
    "        ax.set_title('Original')\n",
    "        ax.axis('off')\n",
    "\n",
    "        for j in range(n_filters):\n",
    "            image = filtered_images[i][j]\n",
    "            if normalize:\n",
    "                image = normalize_image(image)\n",
    "            ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters) + j + 1)\n",
    "            ax.imshow(image.numpy(), cmap='bone')\n",
    "            ax.set_title(f'Filter {j + 1}')\n",
    "            ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(hspace=-0.7)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filtered_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage within the existing loop\n",
    "conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n",
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None  # No convolutional filters in models like ViT\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filtered_images(images, filters, model_name, n_filters=N_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(filters, normalize=True):\n",
    "    filters = filters.cpu()\n",
    "    n_filters = filters.shape[0]\n",
    "    rows = int(np.sqrt(n_filters))\n",
    "    cols = int(np.sqrt(n_filters))\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    # fig.suptitle(f'{model_name} - Filters', fontsize=28, y=0.95)\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        image = filters[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(wspace=-0.9)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage within the existing loop\n",
    "conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n",
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None  # No convolutional filters in models like ViT\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filters(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all results\n",
    "def generate_all_results(results, test_loader):\n",
    "    plot_accuracy_comparison(results)\n",
    "    display_model_metrics_table(results, test_loader)\n",
    "    generate_confusion_matrices(results, test_loader)\n",
    "    plot_most_incorrect_predictions(results, test_loader, n_images=36)\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        model = model_info['model']\n",
    "        display_classification_results(model, test_loader, model_name)\n",
    "        display_classification_report(model, test_loader, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
