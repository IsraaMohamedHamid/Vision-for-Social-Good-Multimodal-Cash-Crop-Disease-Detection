{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from ft_transformer import FTTransformer\n",
    "from transformers import BertModel\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet18, attention_augmented_inceptionv3,attention_augmented_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA' \n",
    "crop_root = os.path.join(base_dir, 'color') # color tester\n",
    "split_root = os.path.join(base_dir, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "csv_path = os.path.join(base_dir, 'plant_disease_multimodal_dataset.csv')  # '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA/plant_disease_multimodal_dataset.csv'\n",
    "csv_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the image paths and labels from the features\n",
    "csv_image_paths = csv_data['Image Path'].values\n",
    "csv_labels = csv_data['Mapped Label'].values\n",
    "csv_features = csv_data.drop(columns=['Image Path', 'Mapped Label', 'Label']).values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove .DS_Store files\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file is an image file\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(base_dir, val_split=0.4, test_split=0.1):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "\n",
    "    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    for cls in classes:\n",
    "        print(f'Processing class: {cls}')\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "\n",
    "        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n",
    "\n",
    "        if len(images) == 0:\n",
    "            print(f\"No images found for class {cls}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Shuffle images to randomize the selection\n",
    "        random.shuffle(images)\n",
    "\n",
    "        try:\n",
    "            train, test = train_test_split(images, test_size=test_split)\n",
    "            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n",
    "        except ValueError as e:\n",
    "            print(f\"Not enough images to split for class {cls}: {e}\")\n",
    "            continue\n",
    "\n",
    "        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n",
    "        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n",
    "        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n",
    "\n",
    "    return train_files, val_files, test_files, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Strawberry___healthy\n",
      "Processing class: Grape___Black_rot\n",
      "Processing class: Potato___Early_blight\n",
      "Processing class: Blueberry___healthy\n",
      "Processing class: Corn_(maize)___healthy\n",
      "Processing class: Tomato___Target_Spot\n",
      "Processing class: Peach___healthy\n",
      "Processing class: Potato___Late_blight\n",
      "Processing class: Tomato___Late_blight\n",
      "Processing class: Tomato___Tomato_mosaic_virus\n",
      "Processing class: Pepper,_bell___healthy\n",
      "Processing class: Orange___Haunglongbing_(Citrus_greening)\n",
      "Processing class: Tomato___Leaf_Mold\n",
      "Processing class: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "Processing class: Cherry_(including_sour)___Powdery_mildew\n",
      "Processing class: Apple___Cedar_apple_rust\n",
      "Processing class: Tomato___Bacterial_spot\n",
      "Processing class: Grape___healthy\n",
      "Processing class: Tomato___Early_blight\n",
      "Processing class: Corn_(maize)___Common_rust_\n",
      "Processing class: Grape___Esca_(Black_Measles)\n",
      "Processing class: Raspberry___healthy\n",
      "Processing class: Tomato___healthy\n",
      "Processing class: Cherry_(including_sour)___healthy\n",
      "Processing class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "Processing class: Apple___Apple_scab\n",
      "Processing class: Corn_(maize)___Northern_Leaf_Blight\n",
      "Processing class: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "Processing class: Peach___Bacterial_spot\n",
      "Processing class: Pepper,_bell___Bacterial_spot\n",
      "Processing class: Tomato___Septoria_leaf_spot\n",
      "Processing class: Squash___Powdery_mildew\n",
      "Processing class: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "Processing class: Apple___Black_rot\n",
      "Processing class: Apple___healthy\n",
      "Processing class: Strawberry___Leaf_scorch\n",
      "Processing class: Potato___healthy\n",
      "Processing class: Soybean___healthy\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_files, val_files, test_files, classes = split_data(crop_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 27124\n",
      "Validation files: 21728\n",
      "Test files: 5447\n"
     ]
    }
   ],
   "source": [
    "# Use the lists of file paths for your dataset loading and transformations\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard image sizes\n",
    "inception_size = 299\n",
    "other_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the data transformations\n",
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the datasets and data loaders and to map between the different modalities\n",
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, file_paths, csv_features, csv_labels, class_to_idx, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with image paths, CSV features, labels, class mapping, and optional transforms.\n",
    "        \n",
    "        Args:\n",
    "            file_paths (list of tuples): List of (image_path, class_label) tuples.\n",
    "            csv_features (ndarray): Array of CSV feature rows.\n",
    "            csv_labels (ndarray): Array of CSV labels corresponding to csv_features.\n",
    "            class_to_idx (dict): Mapping from class labels to indices.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.csv_features = csv_features\n",
    "        self.csv_labels = csv_labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, csv_row, label) where image is the transformed image tensor,\n",
    "                   csv_row is the corresponding CSV feature row, and label is the class index.\n",
    "        \"\"\"\n",
    "        img_path, cls = self.file_paths[idx]  # Get image path and class label\n",
    "        image = Image.open(img_path).convert('RGB')  # Open image and convert to RGB\n",
    "        label = self.class_to_idx[cls]  # Map class label to index\n",
    "        csv_row = self.csv_features[idx]  # Get the corresponding CSV feature row\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply image transformations if provided\n",
    "        \n",
    "        return image, csv_row, label  # Return the image, CSV features, and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset_inception = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n",
    "val_dataset_inception = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n",
    "test_dataset_inception = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n",
    "test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for other models\n",
    "train_dataset_others = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['train'])\n",
    "val_dataset_others = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['val'])\n",
    "test_dataset_others = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['test'])\n",
    "\n",
    "train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n",
    "val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n",
    "test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV feature extractor models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TabNet CSV feature extractor\n",
    "class TabNetCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the TabNetCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Desired output dimension of the feature extractor.\n",
    "        \"\"\"\n",
    "        super(TabNetCSVFeatureExtractor, self).__init__()\n",
    "        self.tabnet = TabNetClassifier(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=hidden_dim,  # Set output dimension to match desired feature size\n",
    "            n_d=hidden_dim,\n",
    "            n_a=hidden_dim,\n",
    "            n_steps=3,\n",
    "            gamma=1.3,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=1e-3,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_fn=None,\n",
    "            scheduler_params=None,\n",
    "            mask_type='sparsemax'\n",
    "        )\n",
    "        # Placeholder for a linear layer if needed to match dimensions\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure x is on CPU since TabNetClassifier might not support GPU\n",
    "        x = x.cpu().numpy()\n",
    "        # Predict on input features\n",
    "        predictions = self.tabnet.predict(x)\n",
    "        # Convert predictions to tensor\n",
    "        feature_output = torch.tensor(predictions, dtype=torch.float32)\n",
    "        # Optionally pass through a linear layer if needed\n",
    "        return self.linear(feature_output)\n",
    "\n",
    "# Define the FT-Transformer CSV feature extractor\n",
    "# class FTTransformerCSVFeatureExtractor(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         \"\"\"\n",
    "#         Initializes the FTTransformerCSVFeatureExtractor.\n",
    "\n",
    "#         Args:\n",
    "#             input_dim (int): Number of input features.\n",
    "#             hidden_dim (int): Number of hidden units.\n",
    "#         \"\"\"\n",
    "#         super(FTTransformerCSVFeatureExtractor, self).__init__()\n",
    "#         self.ft_transformer = FTTransformer(input_dim=input_dim, output_dim=hidden_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.ft_transformer(x)\n",
    "\n",
    "# Define the Multimodal Transformer (BERT-based) CSV feature extractor\n",
    "class BERTCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the BERTCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(BERTCSVFeatureExtractor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BERT expects tokenized input, so we need to process x appropriately\n",
    "        # Assuming x is tokenized and of shape (batch_size, seq_length)\n",
    "        output = self.bert(input_ids=x)[1]  # [1] corresponds to the pooled output\n",
    "        return self.fc(output)\n",
    "\n",
    "# Define the MLP CSV feature extractor\n",
    "class MLP_CSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initializes the MLP_CSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "        \"\"\"\n",
    "        super(MLP_CSVFeatureExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the Deep MLP CSV feature extractor\n",
    "class CSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=10):\n",
    "        \"\"\"\n",
    "        Initializes the DeepCSVFeatureExtractor.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features.\n",
    "            hidden_dim (int): Number of hidden units.\n",
    "            num_layers (int): Number of hidden layers.\n",
    "        \"\"\"\n",
    "        super(CSVFeatureExtractor, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.extractor = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)\n",
    "\n",
    "# Define the Convolutional CSV feature extractor\n",
    "class ConvCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, num_layers=2):\n",
    "        super(ConvCSVFeatureExtractor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the output size after the convolutional layers\n",
    "        conv_output_size = hidden_dim * (seq_len - (num_layers * 2) + 1)  # Adjust for padding\n",
    "\n",
    "        self.fc = nn.Linear(conv_output_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel2(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_input_dim, csv_hidden_dim, num_classes, fusion_method, csv_model_type='simple', seq_len=None):\n",
    "        super(FusionModel2, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # Initialize the base model and get the feature size\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        # Define the CSV feature extractor based on the type\n",
    "        if csv_model_type == 'simple':\n",
    "            self.csv_feature_extractor = CSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=25)\n",
    "        elif csv_model_type == 'deep':\n",
    "            self.csv_feature_extractor = CSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=50)\n",
    "        elif csv_model_type == 'conv':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported csv_model_type\")\n",
    "\n",
    "        # Define additional layers for fusion based on the fusion method\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.aux_logits = False\n",
    "            image_feature_extractor.AuxLogits = None\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()\n",
    "        elif model_name == 'ViT':\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features\n",
    "            inception_model.aux_logits = False\n",
    "            inception_model.AuxLogits = None\n",
    "            inception_model.fc = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        else:\n",
    "            img_features = self.image_feature_extractor(img)\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                if isinstance(img_features, tuple):\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion method model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_feature_extractor, num_classes, fusion_method):\n",
    "        \"\"\"\n",
    "        Initializes the FusionModel.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the image feature extractor architecture.\n",
    "            image_feature_extractor (nn.Module): The model for extracting features from images.\n",
    "            csv_feature_extractor (nn.Module): The model for extracting features from CSV data.\n",
    "            num_classes (int): Number of classes for classification.\n",
    "            fusion_method (str): Method of fusion ('early', 'intermediate', 'late').\n",
    "        \"\"\"\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_feature_extractor = csv_feature_extractor\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        # Initialize the image feature extractor and get the feature size\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        # Initialize the CSV feature extractor\n",
    "        # Determine csv_hidden_dim based on the CSV feature extractor\n",
    "        if hasattr(self.csv_feature_extractor, 'output_dim'):\n",
    "            self.csv_hidden_dim = self.csv_feature_extractor.output_dim\n",
    "        else:\n",
    "            # Default or custom handling if 'output_dim' is not present\n",
    "            self.csv_hidden_dim = 512  # Example default\n",
    "\n",
    "        # Define additional layers for fusion based on the fusion method\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            # Calculate the new feature size after early fusion\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        \"\"\"\n",
    "        Initializes the image feature extractor for fusion, replacing the final classification layer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the image feature extractor architecture.\n",
    "            image_feature_extractor (nn.Module): The model to be modified.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (modified_image_feature_extractor, feature_size)\n",
    "        \"\"\"\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.aux_logits = False  # Disable auxiliary logits\n",
    "            image_feature_extractor.AuxLogits = None  # Remove auxiliary logits\n",
    "            image_feature_extractor.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features  # Access in_features before replacing\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'ViT':\n",
    "            # Generalized approach to identify and replace the classification head\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                # Fallback: Inspect all attributes and find a suitable final layer\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features  # Access in_features before replacing\n",
    "            inception_model.aux_logits = False  # Disable auxiliary logits\n",
    "            inception_model.AuxLogits = None  # Remove auxiliary logits\n",
    "            inception_model.fc = nn.Identity()  # Replace the final fully connected layer with identity\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            # Access the VGG model within the wrapper\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels  # Assuming the penultimate layer is the feature extractor\n",
    "            # Replace the last layer with an identity function\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            # Access the ResNet model within the wrapper\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        # Extract features from the image using the image feature extractor\n",
    "        img_features = self.image_feature_extractor(img)\n",
    "        \n",
    "        # Handle specific cases for certain models\n",
    "        if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "            # Check if img_features is of type InceptionOutputs and extract the tensor\n",
    "            if isinstance(img_features, tuple):  # Handle Inception model outputs\n",
    "                img_features = img_features.logits\n",
    "                img_features = img_features.view(img_features.size(0), -1)\n",
    "        \n",
    "        # Extract features from the CSV data\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        # Perform fusion based on the specified method\n",
    "        if self.fusion_method == 'early':\n",
    "            # Early fusion: Concatenate features before passing through the base model\n",
    "            img = img.view(img.size(0), -1)  # Flatten the image tensor\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        elif self.fusion_method == 'late':\n",
    "            # Late fusion: Combine features after processing through image feature extractor and CSV extractor\n",
    "            combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "            output = self.fusion_layer(combined_features)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            # Intermediate fusion: Process image features through an intermediate layer before fusion\n",
    "            img_features = self.intermediate_layer(img_features)\n",
    "            combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "            output = self.fusion_layer(combined_features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache function\n",
    "def clear_cache():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.cache.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient accumulation\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=5e-4, momentum=0.9)\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs_img, inputs_csv, labels = data\n",
    "                inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "                outputs = model(inputs_img, inputs_csv)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_fusion_model(model, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    return train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method=fusion_method, num_epochs=num_epochs, initial_lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the fusion model\n",
    "def evaluate_fusion_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of classes\n",
    "num_classes_inception = len(class_to_idx)\n",
    "num_classes_others = len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features in the CSV data\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv input dimensions\n",
    "csv_input_dim = csv_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv hidden dimensions\n",
    "csv_hidden_dim = 256  # Adjust based on your model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results dictionary\n",
    "crop_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define sequence length for BERT\n",
    "seq_len = csv_features.shape[0]  # Adjust this value according to your dataset and BERT model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Fusion Method: intermediate --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- CSV Model Type: simple ----------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 0.7313, Train Accuracy: 79.05%, Val Loss: 0.2837, Val Accuracy: 91.55%\n",
      "Epoch 2/40, Train Loss: 0.3041, Train Accuracy: 90.79%, Val Loss: 0.2622, Val Accuracy: 91.68%\n",
      "Epoch 3/40, Train Loss: 0.2164, Train Accuracy: 93.35%, Val Loss: 0.1650, Val Accuracy: 95.18%\n",
      "Epoch 4/40, Train Loss: 0.2396, Train Accuracy: 93.21%, Val Loss: 0.2309, Val Accuracy: 93.93%\n",
      "Epoch 5/40, Train Loss: 0.1483, Train Accuracy: 95.52%, Val Loss: 0.0898, Val Accuracy: 97.29%\n",
      "Epoch 6/40, Train Loss: 0.1630, Train Accuracy: 95.46%, Val Loss: 0.1115, Val Accuracy: 96.64%\n",
      "Epoch 7/40, Train Loss: 0.1397, Train Accuracy: 96.05%, Val Loss: 0.1078, Val Accuracy: 96.72%\n",
      "Epoch 8/40, Train Loss: 0.1204, Train Accuracy: 96.57%, Val Loss: 0.1558, Val Accuracy: 95.26%\n",
      "Epoch 9/40, Train Loss: 0.1005, Train Accuracy: 97.29%, Val Loss: 0.0510, Val Accuracy: 98.50%\n",
      "Epoch 10/40, Train Loss: 0.0909, Train Accuracy: 97.36%, Val Loss: 0.1046, Val Accuracy: 97.08%\n",
      "Epoch 11/40, Train Loss: 0.1656, Train Accuracy: 95.85%, Val Loss: 0.0917, Val Accuracy: 97.32%\n",
      "Epoch 12/40, Train Loss: 0.0648, Train Accuracy: 98.04%, Val Loss: 0.0519, Val Accuracy: 98.46%\n",
      "Epoch 13/40, Train Loss: 0.0795, Train Accuracy: 97.84%, Val Loss: 0.1631, Val Accuracy: 95.50%\n",
      "Epoch 14/40, Train Loss: 0.0730, Train Accuracy: 97.88%, Val Loss: 0.0509, Val Accuracy: 98.60%\n",
      "Epoch 15/40, Train Loss: 0.0569, Train Accuracy: 98.36%, Val Loss: 0.0926, Val Accuracy: 97.57%\n",
      "Epoch 16/40, Train Loss: 0.0980, Train Accuracy: 97.70%, Val Loss: 2.2606, Val Accuracy: 74.22%\n",
      "Epoch 17/40, Train Loss: 0.0928, Train Accuracy: 97.69%, Val Loss: 0.0428, Val Accuracy: 98.78%\n",
      "Epoch 18/40, Train Loss: 0.0430, Train Accuracy: 98.78%, Val Loss: 0.0488, Val Accuracy: 98.76%\n",
      "Epoch 19/40, Train Loss: 0.0792, Train Accuracy: 97.98%, Val Loss: 0.0871, Val Accuracy: 97.98%\n",
      "Epoch 20/40, Train Loss: 0.0471, Train Accuracy: 98.72%, Val Loss: 0.0804, Val Accuracy: 97.90%\n",
      "Epoch 21/40, Train Loss: 0.0498, Train Accuracy: 98.70%, Val Loss: 0.0473, Val Accuracy: 98.67%\n",
      "Epoch 22/40, Train Loss: 0.0483, Train Accuracy: 98.62%, Val Loss: 0.0623, Val Accuracy: 98.43%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "InceptionV3 with intermediate fusion and CSV extractor simple Test Loss: 0.0502, Test Accuracy: 98.77%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 0.8021, Train Accuracy: 76.05%, Val Loss: 3.3622, Val Accuracy: 82.40%\n",
      "Epoch 2/40, Train Loss: 0.2646, Train Accuracy: 91.55%, Val Loss: 0.2264, Val Accuracy: 94.26%\n",
      "Epoch 3/40, Train Loss: 0.2023, Train Accuracy: 93.48%, Val Loss: 0.2482, Val Accuracy: 92.42%\n",
      "Epoch 4/40, Train Loss: 0.1549, Train Accuracy: 94.99%, Val Loss: 0.3066, Val Accuracy: 90.94%\n",
      "Epoch 5/40, Train Loss: 0.1261, Train Accuracy: 95.89%, Val Loss: 0.2744, Val Accuracy: 93.35%\n",
      "Epoch 6/40, Train Loss: 0.1178, Train Accuracy: 96.21%, Val Loss: 0.0874, Val Accuracy: 97.19%\n",
      "Epoch 7/40, Train Loss: 0.0938, Train Accuracy: 96.88%, Val Loss: 0.2470, Val Accuracy: 94.12%\n",
      "Epoch 8/40, Train Loss: 0.0917, Train Accuracy: 97.04%, Val Loss: 0.1305, Val Accuracy: 95.84%\n",
      "Epoch 9/40, Train Loss: 0.0830, Train Accuracy: 97.30%, Val Loss: 0.3180, Val Accuracy: 91.42%\n",
      "Epoch 10/40, Train Loss: 0.0689, Train Accuracy: 97.84%, Val Loss: 0.1185, Val Accuracy: 96.62%\n",
      "Epoch 11/40, Train Loss: 0.0697, Train Accuracy: 97.78%, Val Loss: 0.1773, Val Accuracy: 95.66%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "ResNet152 with intermediate fusion and CSV extractor simple Test Loss: 0.1733, Test Accuracy: 95.94%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 2.8491, Train Accuracy: 23.00%, Val Loss: 2.0887, Val Accuracy: 40.86%\n",
      "Epoch 2/40, Train Loss: 1.6271, Train Accuracy: 52.48%, Val Loss: 1.2592, Val Accuracy: 61.33%\n",
      "Epoch 3/40, Train Loss: 1.1202, Train Accuracy: 66.08%, Val Loss: 0.9501, Val Accuracy: 71.03%\n",
      "Epoch 4/40, Train Loss: 0.9204, Train Accuracy: 72.29%, Val Loss: 0.7808, Val Accuracy: 76.23%\n",
      "Epoch 5/40, Train Loss: 0.8129, Train Accuracy: 75.52%, Val Loss: 0.7369, Val Accuracy: 79.01%\n",
      "Epoch 6/40, Train Loss: 0.7573, Train Accuracy: 77.35%, Val Loss: 0.6609, Val Accuracy: 79.64%\n",
      "Epoch 7/40, Train Loss: 0.6879, Train Accuracy: 79.28%, Val Loss: 0.6137, Val Accuracy: 81.52%\n",
      "Epoch 8/40, Train Loss: 0.5969, Train Accuracy: 82.09%, Val Loss: 0.5710, Val Accuracy: 83.18%\n",
      "Epoch 9/40, Train Loss: 0.6441, Train Accuracy: 81.40%, Val Loss: 0.5076, Val Accuracy: 84.61%\n",
      "Epoch 10/40, Train Loss: 1.5613, Train Accuracy: 63.92%, Val Loss: 2.9254, Val Accuracy: 18.79%\n",
      "Epoch 11/40, Train Loss: 2.5881, Train Accuracy: 27.86%, Val Loss: 1.7024, Val Accuracy: 49.36%\n",
      "Epoch 12/40, Train Loss: 1.4811, Train Accuracy: 55.98%, Val Loss: 1.0481, Val Accuracy: 68.35%\n",
      "Epoch 13/40, Train Loss: 1.0659, Train Accuracy: 67.87%, Val Loss: 0.8586, Val Accuracy: 73.40%\n",
      "Epoch 14/40, Train Loss: 0.8693, Train Accuracy: 73.42%, Val Loss: 0.6693, Val Accuracy: 78.95%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "VGG19 with intermediate fusion and CSV extractor simple Test Loss: 0.6577, Test Accuracy: 79.22%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 2.5868, Train Accuracy: 28.58%, Val Loss: 2.4548, Val Accuracy: 32.19%\n",
      "Epoch 2/40, Train Loss: 2.6560, Train Accuracy: 26.68%, Val Loss: 2.5604, Val Accuracy: 28.79%\n",
      "Epoch 3/40, Train Loss: 2.4692, Train Accuracy: 30.06%, Val Loss: 2.3904, Val Accuracy: 31.82%\n",
      "Epoch 4/40, Train Loss: 2.6369, Train Accuracy: 27.03%, Val Loss: 2.8946, Val Accuracy: 22.41%\n",
      "Epoch 5/40, Train Loss: 2.7102, Train Accuracy: 24.94%, Val Loss: 2.5525, Val Accuracy: 28.25%\n",
      "Epoch 6/40, Train Loss: 2.6064, Train Accuracy: 26.82%, Val Loss: 2.4552, Val Accuracy: 29.56%\n",
      "Epoch 7/40, Train Loss: 2.5423, Train Accuracy: 28.00%, Val Loss: 2.5245, Val Accuracy: 28.90%\n",
      "Epoch 8/40, Train Loss: 2.5800, Train Accuracy: 27.56%, Val Loss: 2.4642, Val Accuracy: 29.06%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "ViT with intermediate fusion and CSV extractor simple Test Loss: 2.4675, Test Accuracy: 29.43%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 0.7259, Train Accuracy: 79.06%, Val Loss: 0.2621, Val Accuracy: 92.21%\n",
      "Epoch 2/40, Train Loss: 0.3122, Train Accuracy: 90.55%, Val Loss: 0.3898, Val Accuracy: 89.26%\n",
      "Epoch 3/40, Train Loss: 0.2178, Train Accuracy: 93.66%, Val Loss: 0.3134, Val Accuracy: 90.13%\n",
      "Epoch 4/40, Train Loss: 0.1948, Train Accuracy: 94.15%, Val Loss: 0.1509, Val Accuracy: 95.47%\n",
      "Epoch 5/40, Train Loss: 0.1911, Train Accuracy: 94.49%, Val Loss: 0.3131, Val Accuracy: 91.53%\n",
      "Epoch 6/40, Train Loss: 0.1820, Train Accuracy: 95.00%, Val Loss: 0.1063, Val Accuracy: 96.87%\n",
      "Epoch 7/40, Train Loss: 0.1210, Train Accuracy: 96.48%, Val Loss: 0.0764, Val Accuracy: 97.64%\n",
      "Epoch 8/40, Train Loss: 0.1620, Train Accuracy: 95.77%, Val Loss: 0.1619, Val Accuracy: 95.89%\n",
      "Epoch 9/40, Train Loss: 0.1097, Train Accuracy: 96.81%, Val Loss: 0.0877, Val Accuracy: 97.30%\n",
      "Epoch 10/40, Train Loss: 0.0889, Train Accuracy: 97.36%, Val Loss: 0.1263, Val Accuracy: 96.23%\n",
      "Epoch 11/40, Train Loss: 0.0818, Train Accuracy: 97.57%, Val Loss: 0.1328, Val Accuracy: 96.65%\n",
      "Epoch 12/40, Train Loss: 0.1557, Train Accuracy: 96.17%, Val Loss: 0.0565, Val Accuracy: 98.28%\n",
      "Epoch 13/40, Train Loss: 0.0610, Train Accuracy: 98.26%, Val Loss: 0.0724, Val Accuracy: 97.82%\n",
      "Epoch 14/40, Train Loss: 0.1081, Train Accuracy: 97.43%, Val Loss: 0.1330, Val Accuracy: 96.45%\n",
      "Epoch 15/40, Train Loss: 0.0694, Train Accuracy: 98.05%, Val Loss: 0.0540, Val Accuracy: 98.59%\n",
      "Epoch 16/40, Train Loss: 0.0571, Train Accuracy: 98.43%, Val Loss: 0.0759, Val Accuracy: 98.04%\n",
      "Epoch 17/40, Train Loss: 0.0627, Train Accuracy: 98.28%, Val Loss: 0.1525, Val Accuracy: 96.48%\n",
      "Epoch 18/40, Train Loss: 0.1796, Train Accuracy: 96.09%, Val Loss: 0.0717, Val Accuracy: 97.80%\n",
      "Epoch 19/40, Train Loss: 0.0386, Train Accuracy: 98.79%, Val Loss: 0.0502, Val Accuracy: 98.63%\n",
      "Epoch 20/40, Train Loss: 0.0490, Train Accuracy: 98.58%, Val Loss: 0.0634, Val Accuracy: 98.31%\n",
      "Epoch 21/40, Train Loss: 0.0463, Train Accuracy: 98.64%, Val Loss: 0.0657, Val Accuracy: 98.22%\n",
      "Epoch 22/40, Train Loss: 0.1007, Train Accuracy: 97.59%, Val Loss: 0.0525, Val Accuracy: 98.62%\n",
      "Epoch 23/40, Train Loss: 0.0358, Train Accuracy: 98.97%, Val Loss: 0.0453, Val Accuracy: 98.60%\n",
      "Epoch 24/40, Train Loss: 0.0305, Train Accuracy: 99.12%, Val Loss: 0.0889, Val Accuracy: 98.17%\n",
      "Epoch 25/40, Train Loss: 0.1472, Train Accuracy: 96.92%, Val Loss: 0.0692, Val Accuracy: 98.12%\n",
      "Epoch 26/40, Train Loss: 0.0559, Train Accuracy: 98.48%, Val Loss: 0.0527, Val Accuracy: 98.66%\n",
      "Epoch 27/40, Train Loss: 0.0399, Train Accuracy: 98.97%, Val Loss: 0.0443, Val Accuracy: 98.73%\n",
      "Epoch 28/40, Train Loss: 0.0255, Train Accuracy: 99.25%, Val Loss: 0.0760, Val Accuracy: 98.32%\n",
      "Epoch 29/40, Train Loss: 0.0343, Train Accuracy: 99.06%, Val Loss: 0.0425, Val Accuracy: 98.86%\n",
      "Epoch 30/40, Train Loss: 0.1034, Train Accuracy: 97.98%, Val Loss: 0.0974, Val Accuracy: 97.37%\n",
      "Epoch 31/40, Train Loss: 0.0356, Train Accuracy: 98.88%, Val Loss: 0.0409, Val Accuracy: 98.93%\n",
      "Epoch 32/40, Train Loss: 0.0739, Train Accuracy: 98.55%, Val Loss: 0.0472, Val Accuracy: 98.77%\n",
      "Epoch 33/40, Train Loss: 0.0240, Train Accuracy: 99.37%, Val Loss: 0.0530, Val Accuracy: 98.76%\n",
      "Epoch 34/40, Train Loss: 0.0310, Train Accuracy: 99.20%, Val Loss: 0.0962, Val Accuracy: 98.18%\n",
      "Epoch 35/40, Train Loss: 0.0317, Train Accuracy: 99.20%, Val Loss: 0.0695, Val Accuracy: 98.04%\n",
      "Epoch 36/40, Train Loss: 0.0307, Train Accuracy: 99.11%, Val Loss: 0.0469, Val Accuracy: 98.92%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor simple Test Loss: 0.0406, Test Accuracy: 98.95%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------- CSV Model Type: deep ----------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor deep\n",
      "Epoch 1/40, Train Loss: 0.7102, Train Accuracy: 79.65%, Val Loss: 0.2941, Val Accuracy: 90.80%\n",
      "Epoch 2/40, Train Loss: 0.2789, Train Accuracy: 91.62%, Val Loss: 0.3151, Val Accuracy: 91.32%\n",
      "Epoch 3/40, Train Loss: 0.2373, Train Accuracy: 92.77%, Val Loss: 0.1708, Val Accuracy: 94.53%\n",
      "Epoch 4/40, Train Loss: 0.1688, Train Accuracy: 94.79%, Val Loss: 0.1502, Val Accuracy: 95.42%\n",
      "Epoch 5/40, Train Loss: 0.1771, Train Accuracy: 94.95%, Val Loss: 0.2080, Val Accuracy: 94.77%\n",
      "Epoch 6/40, Train Loss: 0.1470, Train Accuracy: 95.60%, Val Loss: 0.0880, Val Accuracy: 97.22%\n",
      "Epoch 7/40, Train Loss: 0.1288, Train Accuracy: 96.18%, Val Loss: 0.1228, Val Accuracy: 96.19%\n",
      "Epoch 8/40, Train Loss: 0.1041, Train Accuracy: 96.87%, Val Loss: 0.1778, Val Accuracy: 95.39%\n",
      "Epoch 9/40, Train Loss: 0.1600, Train Accuracy: 95.87%, Val Loss: 0.0714, Val Accuracy: 97.93%\n",
      "Epoch 10/40, Train Loss: 0.0735, Train Accuracy: 97.76%, Val Loss: 0.0595, Val Accuracy: 98.26%\n",
      "Epoch 11/40, Train Loss: 0.1358, Train Accuracy: 96.69%, Val Loss: 0.0863, Val Accuracy: 97.67%\n",
      "Epoch 12/40, Train Loss: 0.0642, Train Accuracy: 97.99%, Val Loss: 0.1101, Val Accuracy: 97.03%\n",
      "Epoch 13/40, Train Loss: 0.1205, Train Accuracy: 96.80%, Val Loss: 0.0835, Val Accuracy: 97.74%\n",
      "Epoch 14/40, Train Loss: 0.0531, Train Accuracy: 98.34%, Val Loss: 0.0697, Val Accuracy: 98.16%\n",
      "Epoch 15/40, Train Loss: 0.0610, Train Accuracy: 98.27%, Val Loss: 0.0558, Val Accuracy: 98.44%\n",
      "Epoch 16/40, Train Loss: 0.0599, Train Accuracy: 98.33%, Val Loss: 0.1290, Val Accuracy: 97.19%\n",
      "Epoch 17/40, Train Loss: 0.1046, Train Accuracy: 97.43%, Val Loss: 0.0525, Val Accuracy: 98.55%\n",
      "Epoch 18/40, Train Loss: 0.0440, Train Accuracy: 98.73%, Val Loss: 0.0637, Val Accuracy: 98.43%\n",
      "Epoch 19/40, Train Loss: 0.0537, Train Accuracy: 98.46%, Val Loss: 0.1268, Val Accuracy: 98.01%\n",
      "Epoch 20/40, Train Loss: 0.1038, Train Accuracy: 97.57%, Val Loss: 0.0516, Val Accuracy: 98.57%\n",
      "Epoch 21/40, Train Loss: 0.0350, Train Accuracy: 98.98%, Val Loss: 0.0437, Val Accuracy: 98.69%\n",
      "Epoch 22/40, Train Loss: 0.0469, Train Accuracy: 98.66%, Val Loss: 0.0571, Val Accuracy: 98.30%\n",
      "Epoch 23/40, Train Loss: 0.0869, Train Accuracy: 98.09%, Val Loss: 0.0480, Val Accuracy: 98.66%\n",
      "Epoch 24/40, Train Loss: 0.0307, Train Accuracy: 99.09%, Val Loss: 0.0602, Val Accuracy: 98.63%\n",
      "Epoch 25/40, Train Loss: 0.0715, Train Accuracy: 98.37%, Val Loss: 0.2702, Val Accuracy: 93.71%\n",
      "Epoch 26/40, Train Loss: 0.1070, Train Accuracy: 97.41%, Val Loss: 0.0678, Val Accuracy: 98.06%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "InceptionV3 with intermediate fusion and CSV extractor deep Test Loss: 0.0638, Test Accuracy: 98.26%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor deep\n",
      "Epoch 1/40, Train Loss: 0.7809, Train Accuracy: 76.76%, Val Loss: 0.3678, Val Accuracy: 88.06%\n",
      "Epoch 2/40, Train Loss: 0.2667, Train Accuracy: 91.31%, Val Loss: 0.7621, Val Accuracy: 85.42%\n",
      "Epoch 3/40, Train Loss: 0.2369, Train Accuracy: 92.61%, Val Loss: 1.4064, Val Accuracy: 71.84%\n",
      "Epoch 4/40, Train Loss: 0.1432, Train Accuracy: 95.24%, Val Loss: 0.1358, Val Accuracy: 95.65%\n",
      "Epoch 5/40, Train Loss: 0.1428, Train Accuracy: 95.34%, Val Loss: 0.1840, Val Accuracy: 94.53%\n",
      "Epoch 6/40, Train Loss: 0.1437, Train Accuracy: 95.54%, Val Loss: 0.3697, Val Accuracy: 89.60%\n",
      "Epoch 7/40, Train Loss: 0.1465, Train Accuracy: 95.43%, Val Loss: 0.0903, Val Accuracy: 97.02%\n",
      "Epoch 8/40, Train Loss: 0.0728, Train Accuracy: 97.51%, Val Loss: 0.1414, Val Accuracy: 96.18%\n",
      "Epoch 9/40, Train Loss: 0.1107, Train Accuracy: 96.58%, Val Loss: 0.0730, Val Accuracy: 97.66%\n",
      "Epoch 10/40, Train Loss: 0.0676, Train Accuracy: 97.82%, Val Loss: 0.0944, Val Accuracy: 97.01%\n",
      "Epoch 11/40, Train Loss: 0.0959, Train Accuracy: 97.11%, Val Loss: 0.0871, Val Accuracy: 97.23%\n",
      "Epoch 12/40, Train Loss: 0.0731, Train Accuracy: 97.78%, Val Loss: 0.0726, Val Accuracy: 97.89%\n",
      "Epoch 13/40, Train Loss: 0.0991, Train Accuracy: 97.08%, Val Loss: 0.1866, Val Accuracy: 94.55%\n",
      "Epoch 14/40, Train Loss: 0.0589, Train Accuracy: 98.04%, Val Loss: 0.1089, Val Accuracy: 96.81%\n",
      "Epoch 15/40, Train Loss: 0.0488, Train Accuracy: 98.52%, Val Loss: 0.0656, Val Accuracy: 98.16%\n",
      "Epoch 16/40, Train Loss: 0.0521, Train Accuracy: 98.39%, Val Loss: 0.2419, Val Accuracy: 94.01%\n",
      "Epoch 17/40, Train Loss: 0.0531, Train Accuracy: 98.33%, Val Loss: 0.0809, Val Accuracy: 97.67%\n",
      "Epoch 18/40, Train Loss: 0.0413, Train Accuracy: 98.69%, Val Loss: 0.1227, Val Accuracy: 97.19%\n",
      "Epoch 19/40, Train Loss: 0.0639, Train Accuracy: 98.14%, Val Loss: 0.0461, Val Accuracy: 98.62%\n",
      "Epoch 20/40, Train Loss: 0.0304, Train Accuracy: 98.99%, Val Loss: 0.1755, Val Accuracy: 96.16%\n",
      "Epoch 21/40, Train Loss: 0.0324, Train Accuracy: 98.97%, Val Loss: 0.0666, Val Accuracy: 98.10%\n",
      "Epoch 22/40, Train Loss: 0.0391, Train Accuracy: 98.85%, Val Loss: 0.0686, Val Accuracy: 98.01%\n",
      "Epoch 23/40, Train Loss: 0.0520, Train Accuracy: 98.46%, Val Loss: 0.0683, Val Accuracy: 98.39%\n",
      "Epoch 24/40, Train Loss: 0.0345, Train Accuracy: 98.94%, Val Loss: 0.1295, Val Accuracy: 96.62%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "ResNet152 with intermediate fusion and CSV extractor deep Test Loss: 0.1229, Test Accuracy: 96.75%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor deep\n",
      "Epoch 1/40, Train Loss: 2.8040, Train Accuracy: 24.20%, Val Loss: 2.3701, Val Accuracy: 34.88%\n",
      "Epoch 2/40, Train Loss: 1.6143, Train Accuracy: 52.92%, Val Loss: 1.3236, Val Accuracy: 59.26%\n",
      "Epoch 3/40, Train Loss: 1.0821, Train Accuracy: 67.03%, Val Loss: 0.9722, Val Accuracy: 71.74%\n",
      "Epoch 4/40, Train Loss: 0.8802, Train Accuracy: 73.41%, Val Loss: 0.8819, Val Accuracy: 73.04%\n",
      "Epoch 5/40, Train Loss: 0.7979, Train Accuracy: 75.81%, Val Loss: 0.6783, Val Accuracy: 79.33%\n",
      "Epoch 6/40, Train Loss: 0.7049, Train Accuracy: 78.36%, Val Loss: 0.5827, Val Accuracy: 81.97%\n",
      "Epoch 7/40, Train Loss: 0.6262, Train Accuracy: 80.84%, Val Loss: 0.5293, Val Accuracy: 83.76%\n",
      "Epoch 8/40, Train Loss: 0.6164, Train Accuracy: 81.26%, Val Loss: 0.5232, Val Accuracy: 84.17%\n",
      "Epoch 9/40, Train Loss: 0.5962, Train Accuracy: 81.82%, Val Loss: 0.5276, Val Accuracy: 83.73%\n",
      "Epoch 10/40, Train Loss: 0.5994, Train Accuracy: 82.30%, Val Loss: 0.6140, Val Accuracy: 80.91%\n",
      "Epoch 11/40, Train Loss: 0.5183, Train Accuracy: 84.43%, Val Loss: 0.4650, Val Accuracy: 85.69%\n",
      "Epoch 12/40, Train Loss: 0.4766, Train Accuracy: 85.35%, Val Loss: 0.5539, Val Accuracy: 83.50%\n",
      "Epoch 13/40, Train Loss: 0.4849, Train Accuracy: 85.52%, Val Loss: 0.4993, Val Accuracy: 85.14%\n",
      "Epoch 14/40, Train Loss: 0.4593, Train Accuracy: 85.95%, Val Loss: 0.5121, Val Accuracy: 84.73%\n",
      "Epoch 15/40, Train Loss: 0.4646, Train Accuracy: 86.70%, Val Loss: 0.9692, Val Accuracy: 72.67%\n",
      "Epoch 16/40, Train Loss: 0.4521, Train Accuracy: 86.13%, Val Loss: 0.4148, Val Accuracy: 87.75%\n",
      "Epoch 17/40, Train Loss: 0.4155, Train Accuracy: 87.78%, Val Loss: 0.4369, Val Accuracy: 86.94%\n",
      "Epoch 18/40, Train Loss: 0.3797, Train Accuracy: 88.65%, Val Loss: 0.4414, Val Accuracy: 86.92%\n",
      "Epoch 19/40, Train Loss: 0.3611, Train Accuracy: 89.18%, Val Loss: 0.4998, Val Accuracy: 85.15%\n",
      "Epoch 20/40, Train Loss: 0.7821, Train Accuracy: 82.01%, Val Loss: 0.7585, Val Accuracy: 77.99%\n",
      "Epoch 21/40, Train Loss: 0.4684, Train Accuracy: 86.17%, Val Loss: 0.4946, Val Accuracy: 85.35%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "VGG19 with intermediate fusion and CSV extractor deep Test Loss: 0.5012, Test Accuracy: 85.06%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor deep\n",
      "Epoch 1/40, Train Loss: 2.9313, Train Accuracy: 21.68%, Val Loss: 2.6436, Val Accuracy: 27.53%\n",
      "Epoch 2/40, Train Loss: 2.9165, Train Accuracy: 21.34%, Val Loss: 2.8383, Val Accuracy: 23.90%\n",
      "Epoch 3/40, Train Loss: 2.7295, Train Accuracy: 24.45%, Val Loss: 2.7552, Val Accuracy: 26.43%\n",
      "Epoch 4/40, Train Loss: 2.7316, Train Accuracy: 24.87%, Val Loss: 2.8756, Val Accuracy: 22.98%\n",
      "Epoch 5/40, Train Loss: 2.7458, Train Accuracy: 24.35%, Val Loss: 2.4709, Val Accuracy: 30.60%\n",
      "Epoch 6/40, Train Loss: 2.5656, Train Accuracy: 27.82%, Val Loss: 2.6098, Val Accuracy: 27.29%\n",
      "Epoch 7/40, Train Loss: 2.5216, Train Accuracy: 28.90%, Val Loss: 2.4579, Val Accuracy: 30.21%\n",
      "Epoch 8/40, Train Loss: 2.4605, Train Accuracy: 30.37%, Val Loss: 2.4173, Val Accuracy: 30.42%\n",
      "Epoch 9/40, Train Loss: 2.5237, Train Accuracy: 28.46%, Val Loss: 2.3994, Val Accuracy: 31.07%\n",
      "Epoch 10/40, Train Loss: 2.4915, Train Accuracy: 29.47%, Val Loss: 2.4363, Val Accuracy: 30.73%\n",
      "Epoch 11/40, Train Loss: 2.4671, Train Accuracy: 30.00%, Val Loss: 2.3854, Val Accuracy: 31.21%\n",
      "Epoch 12/40, Train Loss: 2.4543, Train Accuracy: 30.71%, Val Loss: 2.4965, Val Accuracy: 29.19%\n",
      "Epoch 13/40, Train Loss: 2.4795, Train Accuracy: 30.12%, Val Loss: 2.5230, Val Accuracy: 27.87%\n",
      "Epoch 14/40, Train Loss: 2.5081, Train Accuracy: 29.28%, Val Loss: 2.5178, Val Accuracy: 27.92%\n",
      "Epoch 15/40, Train Loss: 2.5129, Train Accuracy: 29.22%, Val Loss: 2.4217, Val Accuracy: 30.03%\n",
      "Epoch 16/40, Train Loss: 2.4803, Train Accuracy: 29.99%, Val Loss: 2.3984, Val Accuracy: 31.88%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "ViT with intermediate fusion and CSV extractor deep Test Loss: 2.3860, Test Accuracy: 31.60%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor deep\n",
      "Epoch 1/40, Train Loss: 0.7717, Train Accuracy: 77.95%, Val Loss: 0.3532, Val Accuracy: 88.78%\n",
      "Epoch 2/40, Train Loss: 0.2965, Train Accuracy: 91.09%, Val Loss: 0.2140, Val Accuracy: 93.61%\n",
      "Epoch 3/40, Train Loss: 0.2487, Train Accuracy: 92.62%, Val Loss: 0.1844, Val Accuracy: 94.42%\n",
      "Epoch 4/40, Train Loss: 0.1778, Train Accuracy: 94.57%, Val Loss: 0.0842, Val Accuracy: 97.34%\n",
      "Epoch 5/40, Train Loss: 0.1831, Train Accuracy: 94.61%, Val Loss: 0.1150, Val Accuracy: 96.60%\n",
      "Epoch 6/40, Train Loss: 0.1379, Train Accuracy: 95.86%, Val Loss: 0.1060, Val Accuracy: 96.71%\n",
      "Epoch 7/40, Train Loss: 0.1245, Train Accuracy: 96.27%, Val Loss: 0.0907, Val Accuracy: 97.23%\n",
      "Epoch 8/40, Train Loss: 0.1328, Train Accuracy: 96.15%, Val Loss: 0.1942, Val Accuracy: 94.38%\n",
      "Epoch 9/40, Train Loss: 0.0994, Train Accuracy: 97.12%, Val Loss: 0.1215, Val Accuracy: 97.08%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor deep Test Loss: 0.1109, Test Accuracy: 97.39%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------- CSV Model Type: conv ----------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor conv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all combinations of fusion methods, CNN models, and CSV feature extractors\n",
    "for fusion_method in ['intermediate', 'late']:\n",
    "\n",
    "    print(f'-------------------------- Fusion Method: {fusion_method} --------------------------')\n",
    "    for  csv_model_type in ['simple','deep','conv']:\n",
    "        # Loop over each CSV feature extractor model type\n",
    "        # for csv_model_type, csv_feature_extractor_factory in csv_feature_extractors.items():\n",
    "\n",
    "        # Define CSV feature extractor models\n",
    "        csv_feature_extractors = {\n",
    "            # 'tabnet': lambda: TabNetCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "            # 'ft_transformer': lambda: FTTransformerCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "            'bert': lambda: BERTCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "            'mlp': lambda: MLP_CSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "        }\n",
    "\n",
    "        # Define CNN models with pretrained weights\n",
    "        cnn_feature_extractors = {\n",
    "            'InceptionV3': models.inception_v3(pretrained=True).to(device),\n",
    "            'ResNet152': models.resnet152(pretrained=True).to(device),\n",
    "            'VGG19': models.vgg19(pretrained=True).to(device),\n",
    "            'ViT': ViT(\n",
    "                image_size=224,     # Increased image size (old: 224)\n",
    "                patch_size=16,      # Patch size remains the same; try smaller if needed (old: 16)\n",
    "                num_classes=num_classes_others,\n",
    "                dim=256,           # Increased model dimensionality (old: 1024)\n",
    "                depth=6,           # Increased depth (old: 6)\n",
    "                heads=24,           # Increased number of attention heads (old: 16)\n",
    "                mlp_dim=2048,       # Increased MLP dimension (old: 2048)\n",
    "                dropout=0.1,\n",
    "                emb_dropout=0.1\n",
    "            ).to(device),\n",
    "            \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n",
    "            # 'AttentionAugmentedVGG19': attention_augmented_vgg('VGG19', num_classes=num_classes_others).to(device),\n",
    "            # \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n",
    "        }\n",
    "\n",
    "        # Disable auxiliary logits for InceptionV3\n",
    "        if 'InceptionV3' in cnn_feature_extractors:\n",
    "            cnn_feature_extractors['InceptionV3'].aux_logits = False\n",
    "        \n",
    "        print(f'---------------- CSV Model Type: {csv_model_type} ----------------')\n",
    "        for model_name, image_feature_extractor in cnn_feature_extractors.items():\n",
    "            image_feature_extractor.to(device)  # Ensure the CNN model is on the correct device\n",
    "            print(f'Training {model_name} with {fusion_method} fusion and CSV extractor {csv_model_type}')\n",
    "\n",
    "            # Initialize the CSV feature extractor using the factory function\n",
    "            # csv_feature_extractor = csv_feature_extractor_factory()\n",
    "\n",
    "            # Create the FusionModel\n",
    "            fusion_model = FusionModel2(\n",
    "                model_name=model_name,\n",
    "                image_feature_extractor=image_feature_extractor,\n",
    "                csv_input_dim=csv_input_dim,\n",
    "                csv_hidden_dim=csv_hidden_dim,\n",
    "                # csv_feature_extractor=csv_feature_extractor,\n",
    "                num_classes=num_classes_others,\n",
    "                fusion_method=fusion_method,\n",
    "                csv_model_type=csv_model_type, \n",
    "                seq_len=seq_len\n",
    "            ).to(device)\n",
    "\n",
    "            # Train the FusionModel\n",
    "            model = create_and_train_fusion_model(\n",
    "                fusion_model,\n",
    "                train_loader_others,\n",
    "                val_loader_others,\n",
    "                num_classes_others,\n",
    "                csv_input_dim,\n",
    "                device,\n",
    "                fusion_method,\n",
    "                initial_lr=0.001\n",
    "            ).to(device)\n",
    "\n",
    "            # Evaluate the trained FusionModel\n",
    "            test_loss, test_accuracy = evaluate_fusion_model(model, test_loader_others, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "            # Store results for this model configuration\n",
    "            crop_results[f\"{model_name}_{fusion_method}_{csv_model_type}\"] = {\n",
    "                'model': model,\n",
    "                'model_name': model_name,\n",
    "                'fusion_method': fusion_method,\n",
    "                'csv_model_type': csv_model_type,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }\n",
    "            print(f'{model_name} with {fusion_method} fusion and CSV extractor {csv_model_type} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "            # Clean up: delete the model to free up memory (optional)\n",
    "            del model\n",
    "            clear_cache()\n",
    "            print('\\n')\n",
    "        print('\\n')\n",
    "    print('----------------------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/RESULTS/Multimodal\"\n",
    "results_folder = os.path.join(results_base_dir, 'T1')\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(os.path.join(results_folder, filename))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_info in results.items():\n",
    "    print(fusion_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "def plot_accuracy_comparison(results):\n",
    "    accuracies = [result['test_accuracy'] for result in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.show()\n",
    "    save_figure(fig, 'all_fusion_accuracy_comparison.png')\n",
    "\n",
    "    if fusion_method == 'late':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'late_accuracy_comparison.png')\n",
    "    elif fusion_method == 'intermediate':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'intermediate_accuracy_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "plot_accuracy_comparison(crop_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader_inception, test_loader_others):\n",
    "        \n",
    "    metrics_data = []\n",
    "    \n",
    "    for model_name, model_info in results.items():\n",
    "\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        device = next(model.parameters()).device  # Get the device of the model\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, csv_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)  # Display the DataFrame in Jupyter Notebook\n",
    "    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, num_images=5):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    class_labels = list(test_loader.dataset.class_to_idx.keys())\n",
    "    \n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images]  # Move tensors to the model's device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "    # fig.suptitle(f'{model_name} - Classification Results', fontsize=28)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))  # Move tensor back to CPU for visualization\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')  # Access CPU tensor for labels\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_classification_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "    \n",
    "    print(f'Displaying results for {model_name}')\n",
    "    display_classification_results(results[model_name]['model'], test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader, model_name):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predicted, target_names=list(test_loader.dataset.class_to_idx.keys()))\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(report)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "        \n",
    "    print(f'Displaying classification report for {model_name}')\n",
    "    display_classification_report([model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    # fig.suptitle(f'{model_name} - Confusion Matrix\\n', fontsize=28, y=0.83)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "    fig.delaxes(fig.axes[1])  # Delete colorbar\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Predicted Label', fontsize=50)\n",
    "    plt.ylabel('True Label', fontsize=50)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all labels and predictions\n",
    "def get_all_labels_and_preds(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrices\n",
    "def generate_confusion_matrices(results, test_loader_inception, test_loader_others):\n",
    "\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n",
    "        plot_confusion_matrix(labels, pred_labels, classes, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrices(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most incorrect predictions\n",
    "def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n",
    "    rows = int(np.ceil(np.sqrt(n_images)))\n",
    "    cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 20))\n",
    "    # fig.suptitle(f'{model_name} - Most Incorrect\\n', fontsize=28)\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        if i >= len(incorrect):\n",
    "            break\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n",
    "                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_most_incorrect.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu())\n",
    "\n",
    "    return all_images, all_labels, all_preds, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of images to display\n",
    "N_IMAGES = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to get the details\n",
    "def plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36):\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:  # Adjust model names as needed\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n",
    "        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n",
    "        incorrect_examples = []\n",
    "\n",
    "        for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "            if not correct:\n",
    "                incorrect_examples.append((image, label, prob))\n",
    "\n",
    "    incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n",
    "    plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_incorrect_predictions(crop_results, test_loader_inception, test_loader_others, N_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, manifold\n",
    "\n",
    "def get_representations(model, iterator):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            labels.append(y)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(data, n_components=2):\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_representations(data, labels, classes, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    # fig.suptitle(f'{model_name} - PCA', fontsize=28, y=0.95)\n",
    "    ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='hsv')\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_pca.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, labels = get_representations(model, train_loader)\n",
    "for model_name in crop_results.keys():\n",
    "    output_pca_data = get_pca(outputs)\n",
    "    plot_representations(output_pca_data, labels, classes)  # Adjusted to pass only three arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne(data, n_components=2, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "    tsne = manifold.TSNE(n_components=n_components, random_state=0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in crop_results.keys():\n",
    "    output_tsne_data = get_tsne(outputs)\n",
    "    plot_representations(output_tsne_data, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot filtered images\n",
    "def plot_filtered_images(images, filters, model_name, n_filters=None, normalize=True):\n",
    "    images = torch.cat([i.unsqueeze(0) for i in images], dim=0).cpu()\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    if n_filters is not None:\n",
    "        filters = filters[:n_filters]\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    filtered_images = F.conv2d(images, filters)\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    # fig.suptitle(f'{model_name} - Filtered Images', fontsize=28, y=0.8)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        image = images[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters))\n",
    "        ax.imshow(image.permute(1, 2, 0).numpy())\n",
    "        ax.set_title('Original')\n",
    "        ax.axis('off')\n",
    "\n",
    "        for j in range(n_filters):\n",
    "            image = filtered_images[i][j]\n",
    "            if normalize:\n",
    "                image = normalize_image(image)\n",
    "            ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters) + j + 1)\n",
    "            ax.imshow(image.numpy(), cmap='bone')\n",
    "            ax.set_title(f'Filter {j + 1}')\n",
    "            ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(hspace=-0.7)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filtered_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage within the existing loop\n",
    "conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n",
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None  # No convolutional filters in models like ViT\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filtered_images(images, filters, model_name, n_filters=N_FILTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(filters, normalize=True):\n",
    "    filters = filters.cpu()\n",
    "    n_filters = filters.shape[0]\n",
    "    rows = int(np.sqrt(n_filters))\n",
    "    cols = int(np.sqrt(n_filters))\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "    # fig.suptitle(f'{model_name} - Filters', fontsize=28, y=0.95)\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        image = filters[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(wspace=-0.9)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage within the existing loop\n",
    "conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n",
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None  # No convolutional filters in models like ViT\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filters(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all results\n",
    "def generate_all_results(results, test_loader):\n",
    "    plot_accuracy_comparison(results)\n",
    "    display_model_metrics_table(results, test_loader)\n",
    "    generate_confusion_matrices(results, test_loader)\n",
    "    plot_most_incorrect_predictions(results, test_loader, n_images=36)\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        model = model_info['model']\n",
    "        display_classification_results(model, test_loader, model_name)\n",
    "        display_classification_report(model, test_loader, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
