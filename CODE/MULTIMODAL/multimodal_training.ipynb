{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from ft_transformer import FTTransformer\n",
    "from transformers import BertModel\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet18, attention_augmented_inceptionv3, attention_augmented_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA'\n",
    "crop_root = os.path.join(base_dir, 'color')\n",
    "split_root = os.path.join(base_dir, 'split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "csv_path = '/mnt/data/multimodal_data.csv'\n",
    "csv_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the image paths and labels from the features\n",
    "csv_image_paths = csv_data['RGB_Image'].values\n",
    "csv_labels = csv_data['Label'].values\n",
    "csv_features = csv_data.drop(columns=['Date', 'Tree_ID', 'Orchard_Mapping_Image', 'UAV_Image', 'RGB_Image', 'Multispectral_RGB_Image', 'Multispectral_REG_Image', 'Multispectral_RED_Image', 'Multispectral_NIR_Image', 'Multispectral_GRE_Image', 'Label']).values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove .DS_Store files\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file is an image file\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(base_dir, val_split=0.4, test_split=0.1):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "\n",
    "    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    for cls in classes:\n",
    "        print(f'Processing class: {cls}')\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "\n",
    "        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n",
    "\n",
    "        if len(images) == 0:\n",
    "            print(f\"No images found for class {cls}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Shuffle images to randomize the selection\n",
    "        random.shuffle(images)\n",
    "\n",
    "        try:\n",
    "            train, test = train_test_split(images, test_size=test_split)\n",
    "            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n",
    "        except ValueError as e:\n",
    "            print(f\"Not enough images to split for class {cls}: {e}\")\n",
    "            continue\n",
    "\n",
    "        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n",
    "        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n",
    "        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n",
    "\n",
    "    return train_files, val_files, test_files, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_files, val_files, test_files, classes = split_data(crop_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the lists of file paths for your dataset loading and transformations\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard image sizes\n",
    "inception_size = 299\n",
    "other_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the data transformations\n",
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create the datasets and data loaders and to map between the different modalities\n",
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, file_paths, csv_features, csv_labels, class_to_idx, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.csv_features = csv_features\n",
    "        self.csv_labels = csv_labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, cls = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_to_idx[cls]\n",
    "        csv_row = self.csv_features[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, csv_row, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to indices\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "train_dataset_inception = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n",
    "val_dataset_inception = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n",
    "test_dataset_inception = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n",
    "test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for other models\n",
    "train_dataset_others = CustomMultimodalDataset(train_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['train'])\n",
    "val_dataset_others = CustomMultimodalDataset(val_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['val'])\n",
    "test_dataset_others = CustomMultimodalDataset(test_files, csv_features, csv_labels, class_to_idx, transform=data_transforms['Others']['test'])\n",
    "\n",
    "train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n",
    "val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n",
    "test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CSV feature extractor models\n",
    "class TabNetCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(TabNetCSVFeatureExtractor, self).__init__()\n",
    "        self.tabnet = TabNetClassifier(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            n_d=hidden_dim,\n",
    "            n_a=hidden_dim,\n",
    "            n_steps=3,\n",
    "            gamma=1.3,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=1e-3,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_fn=None,\n",
    "            scheduler_params=None,\n",
    "            mask_type='sparsemax'\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.cpu().numpy()\n",
    "        predictions = self.tabnet.predict(x)\n",
    "        feature_output = torch.tensor(predictions, dtype=torch.float32)\n",
    "        return self.linear(feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multimodal Transformer (BERT-based) CSV feature extractor\n",
    "class BERTCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(BERTCSVFeatureExtractor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.bert(input_ids=x)[1]\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP CSV feature extractor\n",
    "class MLP_CSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP_CSVFeatureExtractor, self).__init__()\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep MLP CSV feature extractor\n",
    "class CSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=10):\n",
    "        super(CSVFeatureExtractor, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.extractor = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional CSV feature extractor\n",
    "class ConvCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, num_layers=2):\n",
    "        super(ConvCSVFeatureExtractor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        conv_output_size = hidden_dim * (seq_len - (num_layers * 2) + 1)\n",
    "\n",
    "        self.fc = nn.Linear(conv_output_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel2(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_input_dim, csv_hidden_dim, num_classes, fusion_method, csv_model_type='simple', seq_len=None):\n",
    "        super(FusionModel2, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        if csv_model_type == 'simple':\n",
    "            self.csv_feature_extractor = CSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=25)\n",
    "        elif csv_model_type == 'deep':\n",
    "            self.csv_feature_extractor = CSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=50)\n",
    "        elif csv_model_type == 'conv':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=2)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported csv_model_type\")\n",
    "\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.aux_logits = False\n",
    "            image_feature_extractor.AuxLogits = None\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()\n",
    "        elif model_name == 'ViT':\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features\n",
    "            inception_model.aux_logits = False\n",
    "            inception_model.AuxLogits = None\n",
    "            inception_model.fc = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        else:\n",
    "            img_features = self.image_feature_extractor(img)\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                if isinstance(img_features, tuple):\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache function\n",
    "def clear_cache():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.cache.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient accumulation\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs_img, inputs_csv, labels = data\n",
    "                inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "                outputs = model(inputs_img, inputs_csv)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_fusion_model(model, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    return train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method=fusion_method, num_epochs=num_epochs, initial_lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the fusion model\n",
    "def evaluate_fusion_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "num_classes_inception = len(class_to_idx)\n",
    "num_classes_others = len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features in the CSV data\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv input dimensions\n",
    "csv_input_dim = csv_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv hidden dimensions\n",
    "csv_hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results dictionary\n",
    "crop_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence length for BERT\n",
    "seq_len = csv_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all combinations of fusion methods, CNN models, and CSV feature extractors\n",
    "for fusion_method in ['intermediate', 'late']:\n",
    "    print(f'-------------------------- Fusion Method: {fusion_method} --------------------------')\n",
    "    for csv_model_type in ['simple', 'deep', 'conv']:\n",
    "        csv_feature_extractors = {\n",
    "            'bert': lambda: BERTCSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "            'mlp': lambda: MLP_CSVFeatureExtractor(csv_input_dim, csv_hidden_dim),\n",
    "        }\n",
    "\n",
    "        cnn_feature_extractors = {\n",
    "            'InceptionV3': models.inception_v3(pretrained=True).to(device),\n",
    "            'ResNet152': models.resnet152(pretrained=True).to(device),\n",
    "            'VGG19': models.vgg19(pretrained=True).to(device),\n",
    "            'ViT': ViT(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_classes=num_classes_others,\n",
    "                dim=256,\n",
    "                depth=6,\n",
    "                heads=24,\n",
    "                mlp_dim=2048,\n",
    "                dropout=0.1,\n",
    "                emb_dropout=0.1\n",
    "            ).to(device),\n",
    "            \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n",
    "        }\n",
    "\n",
    "        if 'InceptionV3' in cnn_feature_extractors:\n",
    "            cnn_feature_extractors['InceptionV3'].aux_logits = False\n",
    "        \n",
    "        print(f'---------------- CSV Model Type: {csv_model_type} ----------------')\n",
    "        for model_name, image_feature_extractor in cnn_feature_extractors.items():\n",
    "            image_feature_extractor.to(device)\n",
    "            print(f'Training {model_name} with {fusion_method} fusion and CSV extractor {csv_model_type}')\n",
    "\n",
    "            fusion_model = FusionModel2(\n",
    "                model_name=model_name,\n",
    "                image_feature_extractor=image_feature_extractor,\n",
    "                csv_input_dim=csv_input_dim,\n",
    "                csv_hidden_dim=csv_hidden_dim,\n",
    "                num_classes=num_classes_others,\n",
    "                fusion_method=fusion_method,\n",
    "                csv_model_type=csv_model_type,\n",
    "                seq_len=seq_len\n",
    "            ).to(device)\n",
    "\n",
    "            model = create_and_train_fusion_model(\n",
    "                fusion_model,\n",
    "                train_loader_others,\n",
    "                val_loader_others,\n",
    "                num_classes_others,\n",
    "                csv_input_dim,\n",
    "                device,\n",
    "                fusion_method,\n",
    "                initial_lr=0.001\n",
    "            ).to(device)\n",
    "\n",
    "            test_loss, test_accuracy = evaluate_fusion_model(model, test_loader_others, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "            crop_results[f\"{model_name}_{fusion_method}_{csv_model_type}\"] = {\n",
    "                'model': model,\n",
    "                'model_name': model_name,\n",
    "                'fusion_method': fusion_method,\n",
    "                'csv_model_type': csv_model_type,\n",
    "                'test_loss': test_loss,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }\n",
    "            print(f'{model_name} with {fusion_method} fusion and CSV extractor {csv_model_type} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "            del model\n",
    "            clear_cache()\n",
    "            print('\\n')\n",
    "        print('\\n')\n",
    "    print('----------------------------------------------------------------------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results folder\n",
    "results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/RESULTS/Multimodal\"\n",
    "results_folder = os.path.join(results_base_dir, 'T1')\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(os.path.join(results_folder, filename))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "def plot_accuracy_comparison(results):\n",
    "    accuracies = [result['test_accuracy'] for result in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.show()\n",
    "    save_figure(fig, 'all_fusion_accuracy_comparison.png')\n",
    "\n",
    "    if fusion_method == 'late':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'late_accuracy_comparison.png')\n",
    "    elif fusion_method == 'intermediate':\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        plt.bar(model_names, accuracies)\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.show()\n",
    "        save_figure(fig, 'intermediate_accuracy_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "plot_accuracy_comparison(crop_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader_inception, test_loader_others):\n",
    "    metrics_data = []\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, csv_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "\n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)\n",
    "    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, model_name, num_images=5):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    class_labels = list(test_loader.dataset.class_to_idx.keys())\n",
    "    \n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_classification_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "    \n",
    "    print(f'Displaying results for {model_name}')\n",
    "    display_classification_results(crop_results[model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader, model_name):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predicted, target_names=list(test_loader.dataset.class_to_idx.keys()))\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "        \n",
    "    print(f'Displaying classification report for {model_name}')\n",
    "    display_classification_report([model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "    fig.delaxes(fig.axes[1])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Predicted Label', fontsize=50)\n",
    "    plt.ylabel('True Label', fontsize=50)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_labels_and_preds(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrices(results, test_loader_inception, test_loader_others):\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n",
    "        plot_confusion_matrix(labels, pred_labels, classes, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrices(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most incorrect predictions\n",
    "def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n",
    "    rows = int(np.ceil(np.sqrt(n_images)))\n",
    "    cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 20))\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        if i >= len(incorrect):\n",
    "            break\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n",
    "                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_most_incorrect.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu())\n",
    "\n",
    "    return all_images, all_labels, all_preds, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36):\n",
    "    classes = list(test_loader.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "            test_loader = test_loader_inception\n",
    "        else:\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n",
    "        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n",
    "        incorrect_examples = []\n",
    "\n",
    "        for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "            if not correct:\n",
    "                incorrect_examples.append((image, label, prob))\n",
    "\n",
    "    incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n",
    "    plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_incorrect_predictions(crop_results, test_loader_inception, test_loader_others, N_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representations(model, iterator):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            labels.append(y)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    return outputs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(data, n_components=2):\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_representations(data, labels, classes, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "        labels = labels[:n_images]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='hsv')\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_pca.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, labels = get_representations(model, train_loader)\n",
    "for model_name in crop_results.keys():\n",
    "    output_pca_data = get_pca(outputs)\n",
    "    plot_representations(output_pca_data, labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne(data, n_components=2, n_images=None):\n",
    "    if n_images is not None:\n",
    "        data = data[:n_images]\n",
    "    tsne = manifold.TSNE(n_components=n_components, random_state=0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    return tsne_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in crop_results.keys():\n",
    "    output_tsne_data = get_tsne(outputs)\n",
    "    plot_representations(output_tsne_data, labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot filtered images\n",
    "def plot_filtered_images(images, filters, model_name, n_filters=None, normalize=True):\n",
    "    images = torch.cat([i.unsqueeze(0) for i in images], dim=0).cpu()\n",
    "    filters = filters.cpu()\n",
    "\n",
    "    if n_filters is not None:\n",
    "        filters = filters[:n_filters]\n",
    "\n",
    "    n_images = images.shape[0]\n",
    "    n_filters = filters.shape[0]\n",
    "\n",
    "    filtered_images = F.conv2d(images, filters)\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        image = images[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters))\n",
    "        ax.imshow(image.permute(1, 2, 0).numpy())\n",
    "        ax.set_title('Original')\n",
    "        ax.axis('off')\n",
    "\n",
    "        for j in range(n_filters):\n",
    "            image = filtered_images[i][j]\n",
    "            if normalize:\n",
    "                image = normalize_image(image)\n",
    "            ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters) + j + 1)\n",
    "            ax.imshow(image.numpy(), cmap='bone')\n",
    "            ax.set_title(f'Filter {j + 1}')\n",
    "            ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(hspace=-0.7)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filtered_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 7\n",
    "conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']\n",
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filtered_images(images, filters, model_name, n_filters=N_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filters(filters, normalize=True):\n",
    "    filters = filters.cpu()\n",
    "    n_filters = filters.shape[0]\n",
    "    rows = int(np.sqrt(n_filters))\n",
    "    cols = int(np.sqrt(n_filters))\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 15))\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        image = filters[i]\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.imshow(image.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "    fig.subplots_adjust(wspace=-0.9)\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_filters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_info in crop_results.items():\n",
    "    model = model_info['model']\n",
    "    if model_name in conv_models:\n",
    "        if hasattr(model, 'conv1'):\n",
    "            filters = model.conv1.weight.data\n",
    "        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n",
    "            filters = model.features[0].weight.data\n",
    "        else:\n",
    "            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n",
    "            filters = None\n",
    "    else:\n",
    "        filters = None\n",
    "\n",
    "    if filters is not None:\n",
    "        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n",
    "        plot_filters(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all results\n",
    "def generate_all_results(results, test_loader):\n",
    "    plot_accuracy_comparison(results)\n",
    "    display_model_metrics_table(results, test_loader)\n",
    "    generate_confusion_matrices(results, test_loader)\n",
    "    plot_most_incorrect_predictions(results, test_loader, n_images=36)\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        model = model_info['model']\n",
    "        display_classification_results(model, test_loader, model_name)\n",
    "        display_classification_report(model, test_loader, model_name)\n",
    "\n",
    "generate_all_results(crop_results, test_loader_others)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
