{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet18, attention_augmented_inceptionv3, attention_augmented_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = ['04_11_21',\n",
    "'14_09_21',\n",
    "'14_09_22',\n",
    "'15_07_22',\n",
    "'25_05_22',\n",
    "'27_07_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA/Peach/'\n",
    "date = date_list[0]\n",
    "date_dir = os.path.join(base_dir, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for the images\n",
    "uav_dir = os.path.join(date_dir, \"Aerial_UAV_Photos\")\n",
    "rgb_dir = os.path.join(date_dir, \"Ground_RGB_Photos\")\n",
    "multispectral_dir = os.path.join(date_dir, \"Ground_Multispectral_Photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "multimodal_data_path = os.path.join(base_dir, \"combined_multimodal_data.csv\")\n",
    "multimodal_df = pd.read_csv(multimodal_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tree_ID</th>\n",
       "      <th>Orchard_Mapping_Image</th>\n",
       "      <th>Aerial_UAV_Image</th>\n",
       "      <th>Ground_RGB_Image</th>\n",
       "      <th>Ground_RGB_Image_with_Bounding_Boxes</th>\n",
       "      <th>Ground_RGB_Image_Annotations</th>\n",
       "      <th>Multispectral_RGB_Image</th>\n",
       "      <th>Multispectral_REG_Image</th>\n",
       "      <th>Multispectral_RED_Image</th>\n",
       "      <th>...</th>\n",
       "      <th>Multispectral_NDRE</th>\n",
       "      <th>Multispectral_SAVI</th>\n",
       "      <th>Multispectral_GNDVI</th>\n",
       "      <th>Multispectral_RVI</th>\n",
       "      <th>Multispectral_TVI</th>\n",
       "      <th>Multispectral_NDVI_Image</th>\n",
       "      <th>Multispectral_GNDVI_Image</th>\n",
       "      <th>Multispectral_NDRE_Image</th>\n",
       "      <th>Multispectral_SAVI_Image</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-1</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054946</td>\n",
       "      <td>0.578614</td>\n",
       "      <td>0.308877</td>\n",
       "      <td>798.861030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-2</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.661596</td>\n",
       "      <td>0.420410</td>\n",
       "      <td>815.203405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-4</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085527</td>\n",
       "      <td>0.585869</td>\n",
       "      <td>0.422484</td>\n",
       "      <td>361.728795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-3</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059559</td>\n",
       "      <td>0.602647</td>\n",
       "      <td>0.414713</td>\n",
       "      <td>442.763371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>28-10</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>0.628999</td>\n",
       "      <td>0.412634</td>\n",
       "      <td>789.667606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-17</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-18</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-19</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-20</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-21</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5328 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Tree_ID                              Orchard_Mapping_Image  \\\n",
       "0     04_11_21    29-1  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     04_11_21    29-2  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     04_11_21    29-4  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     04_11_21    29-3  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     04_11_21   28-10  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...        ...     ...                                                ...   \n",
       "5323  27_07_21   30-17  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5324  27_07_21   30-18  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5325  27_07_21   30-19  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5326  27_07_21   30-20  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5327  27_07_21   30-21  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "\n",
       "                                       Aerial_UAV_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5324  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5325  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5326  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5327  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "\n",
       "                                       Ground_RGB_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "     Ground_RGB_Image_with_Bounding_Boxes Ground_RGB_Image_Annotations  \\\n",
       "0                                     NaN                          NaN   \n",
       "1                                     NaN                          NaN   \n",
       "2                                     NaN                          NaN   \n",
       "3                                     NaN                          NaN   \n",
       "4                                     NaN                          NaN   \n",
       "...                                   ...                          ...   \n",
       "5323                                  NaN                          NaN   \n",
       "5324                                  NaN                          NaN   \n",
       "5325                                  NaN                          NaN   \n",
       "5326                                  NaN                          NaN   \n",
       "5327                                  NaN                          NaN   \n",
       "\n",
       "                                Multispectral_RGB_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                                Multispectral_REG_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                                Multispectral_RED_Image  ...  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "...                                                 ...  ...   \n",
       "5323                                                NaN  ...   \n",
       "5324                                                NaN  ...   \n",
       "5325                                                NaN  ...   \n",
       "5326                                                NaN  ...   \n",
       "5327                                                NaN  ...   \n",
       "\n",
       "     Multispectral_NDRE Multispectral_SAVI Multispectral_GNDVI  \\\n",
       "0              0.054946           0.578614            0.308877   \n",
       "1              0.098110           0.661596            0.420410   \n",
       "2              0.085527           0.585869            0.422484   \n",
       "3              0.059559           0.602647            0.414713   \n",
       "4              0.075293           0.628999            0.412634   \n",
       "...                 ...                ...                 ...   \n",
       "5323                NaN                NaN                 NaN   \n",
       "5324                NaN                NaN                 NaN   \n",
       "5325                NaN                NaN                 NaN   \n",
       "5326                NaN                NaN                 NaN   \n",
       "5327                NaN                NaN                 NaN   \n",
       "\n",
       "     Multispectral_RVI  Multispectral_TVI  \\\n",
       "0           798.861030                NaN   \n",
       "1           815.203405                NaN   \n",
       "2           361.728795                NaN   \n",
       "3           442.763371                NaN   \n",
       "4           789.667606                NaN   \n",
       "...                ...                ...   \n",
       "5323               NaN                NaN   \n",
       "5324               NaN                NaN   \n",
       "5325               NaN                NaN   \n",
       "5326               NaN                NaN   \n",
       "5327               NaN                NaN   \n",
       "\n",
       "                               Multispectral_NDVI_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                              Multispectral_GNDVI_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                               Multispectral_NDRE_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                               Multispectral_SAVI_Image      date  \n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "...                                                 ...       ...  \n",
       "5323                                                NaN  27_07_21  \n",
       "5324                                                NaN  27_07_21  \n",
       "5325                                                NaN  27_07_21  \n",
       "5326                                                NaN  27_07_21  \n",
       "5327                                                NaN  27_07_21  \n",
       "\n",
       "[5328 rows x 38 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Tree_ID', 'Orchard_Mapping_Image', 'Aerial_UAV_Image',\n",
       "       'Ground_RGB_Image', 'Ground_RGB_Image_with_Bounding_Boxes',\n",
       "       'Ground_RGB_Image_Annotations', 'Multispectral_RGB_Image',\n",
       "       'Multispectral_REG_Image', 'Multispectral_RED_Image',\n",
       "       'Multispectral_NIR_Image', 'Multispectral_GRE_Image',\n",
       "       'Multispectral_RGB_Bounding_Box_Image',\n",
       "       'Multispectral_RGB_Bounding_Box_Annotation', 'Label', 'UAV_NDVI',\n",
       "       'UAV_EVI', 'UAV_NDRE', 'UAV_SAVI', 'UAV_GNDVI', 'UAV_RVI', 'UAV_TVI',\n",
       "       'x1', 'x2', 'y1', 'y2', 'Multispectral_NDVI', 'Multispectral_EVI',\n",
       "       'Multispectral_NDRE', 'Multispectral_SAVI', 'Multispectral_GNDVI',\n",
       "       'Multispectral_RVI', 'Multispectral_TVI', 'Multispectral_NDVI_Image',\n",
       "       'Multispectral_GNDVI_Image', 'Multispectral_NDRE_Image',\n",
       "       'Multispectral_SAVI_Image', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image columns\n",
    "image_columns = [\n",
    "    'Ground_RGB_Image_with_Bounding_Boxes','Ground_RGB_Image_Annotations', 'Multispectral_NDVI_Image','Multispectral_RGB_Bounding_Box_Image', 'Multispectral_RGB_Bounding_Box_Annotation',\n",
    "    # 'Multispectral_RGB_Image', 'Multispectral_REG_Image', 'Multispectral_RED_Image', 'Multispectral_NIR_Image', 'Multispectral_GRE_Image',\n",
    "    # 'Multispectral_NDVI_Image', 'Multispectral_GNDVI_Image', 'Multispectral_NDRE_Image', 'Multispectral_SAVI_Image'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "feature_columns = [\n",
    "    # 'UAV_NDVI', 'UAV_EVI', 'UAV_NDRE', 'UAV_SAVI', 'UAV_GNDVI', 'UAV_RVI', 'UAV_TVI', 'Multispectral_SAVI',\n",
    "    'Multispectral_NDVI', 'Multispectral_GNDVI', 'Multispectral_SAVI',\n",
    "    # 'Multispectral_RVI' 'Multispectral_EVI', 'Multispectral_NDRE',\n",
    "]\n",
    "csv_features = multimodal_df[feature_columns].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with label = 0\n",
    "multimodal_df = multimodal_df[multimodal_df['Label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset to ensure all image paths are strings\n",
    "def is_valid_image_path(path):\n",
    "    return isinstance(path, str) and path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if all image columns contain valid strings\n",
    "def valid_image_paths(row):\n",
    "    return all(isinstance(row[col], str) for col in image_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "valid_rows = multimodal_df.apply(valid_image_paths, axis=1)\n",
    "multimodal_df = multimodal_df[valid_rows]\n",
    "multimodal_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and exclude non-numeric columns from the features\n",
    "numeric_columns = multimodal_df.select_dtypes(include=[np.number]).columns\n",
    "csv_image_paths = multimodal_df[image_columns].values\n",
    "csv_labels = multimodal_df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files if necessary (if working in a directory with such files)\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets directly from the CSV\n",
    "train_df, test_df = train_test_split(multimodal_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 96\n",
      "Validation files: 32\n",
      "Test files: 32\n"
     ]
    }
   ],
   "source": [
    "# Use the lists of file paths for your dataset loading and transformations\n",
    "print(f\"Train files: {len(train_df)}\")\n",
    "print(f\"Validation files: {len(val_df)}\")\n",
    "print(f\"Test files: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard image sizes\n",
    "inception_size = 299\n",
    "other_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the data transformations\n",
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, image_columns, feature_columns, class_to_idx, transform=None):\n",
    "        self.df = df\n",
    "        self.image_columns = image_columns\n",
    "        self.feature_columns = feature_columns\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define the mapping from integers to class names\n",
    "        self.label_to_class_name = {\n",
    "            0: 'Healthy',\n",
    "            1: 'Grapholita molesta',\n",
    "            2: 'Anarsia lineatella',\n",
    "            3: 'Dead Tree'\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        images = []\n",
    "        for img_col in self.image_columns:\n",
    "            img_path = row[img_col]\n",
    "            \n",
    "            # Ensure the path is a string and points to a valid image file\n",
    "            if isinstance(img_path, str) and img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                    \n",
    "                    # Handle bounding boxes for specific image columns\n",
    "                    if img_col == 'Ground_RGB_Image_with_Bounding_Boxes':\n",
    "                        annotation_path = row['Ground_RGB_Image_Annotations']\n",
    "                        if os.path.exists(annotation_path):\n",
    "                            try:\n",
    "                                with open(annotation_path, 'r') as f:\n",
    "                                    bbox_data = json.load(f)\n",
    "                                \n",
    "                                # Apply each bounding box found in the annotation\n",
    "                                for region in bbox_data.get('regions', []):\n",
    "                                    x1, y1 = min(region['points']['x']), min(region['points']['y'])\n",
    "                                    x2, y2 = max(region['points']['x']), max(region['points']['y'])\n",
    "                                    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "                                    \n",
    "                                    if self.transform:\n",
    "                                        cropped_image = self.transform(cropped_image)\n",
    "                                    images.append(cropped_image)\n",
    "                                    \n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Skipping invalid JSON file for {img_col} at index {idx}: {annotation_path}\")\n",
    "                        else:\n",
    "                            print(f\"Annotation file not found for {img_col} at index {idx}: {annotation_path}\")\n",
    "                    \n",
    "                    elif img_col == 'Multispectral_RGB_Bounding_Box_Image':\n",
    "                        annotation_path = row['Multispectral_RGB_Bounding_Box_Annotation']\n",
    "                        if os.path.exists(annotation_path):\n",
    "                            try:\n",
    "                                with open(annotation_path, 'r') as f:\n",
    "                                    bbox_data = json.load(f)\n",
    "                                \n",
    "                                # Apply each bounding box found in the annotation\n",
    "                                for region in bbox_data.get('regions', []):\n",
    "                                    x1, y1 = min(region['points']['x']), min(region['points']['y'])\n",
    "                                    x2, y2 = max(region['points']['x']), max(region['points']['y'])\n",
    "                                    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "                                    \n",
    "                                    if self.transform:\n",
    "                                        cropped_image = self.transform(cropped_image)\n",
    "                                    images.append(cropped_image)\n",
    "                                    \n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Skipping invalid JSON file for {img_col} at index {idx}: {annotation_path}\")\n",
    "                        else:\n",
    "                            print(f\"Annotation file not found for {img_col} at index {idx}: {annotation_path}\")\n",
    "\n",
    "                    # Apply transformation to the image if no bounding boxes are required\n",
    "                    elif self.transform:\n",
    "                        image = self.transform(image)\n",
    "                        images.append(image)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Could not open image at path {img_path} at index {idx}: {e}\")\n",
    "            else:\n",
    "                continue  # Skip non-image files\n",
    "\n",
    "        if not images:\n",
    "            raise ValueError(f\"No valid images found for index {idx}\")\n",
    "\n",
    "        # Stack images into a tensor and average along the channel dimension\n",
    "        images = torch.stack(images).mean(dim=0)  # Average the images along the channel dimension to maintain 3 channels\n",
    "\n",
    "        # Extract features from the DataFrame\n",
    "        csv_row = row[self.feature_columns].values.astype(np.float32)\n",
    "        \n",
    "        # Convert the integer label to its corresponding class name\n",
    "        label_int = row['Label']\n",
    "        label_name = self.label_to_class_name.get(label_int, None)\n",
    "        if label_name is None or label_name not in self.class_to_idx:\n",
    "            raise KeyError(f\"Label '{label_int}' not found in label_to_class_name mapping or class_to_idx dictionary.\")\n",
    "        label = self.class_to_idx[label_name]\n",
    "\n",
    "        return images, torch.tensor(csv_row, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to indices\n",
    "classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "class_to_idx = {\n",
    "    'Healthy': 0,\n",
    "    'Grapholita molesta': 1,\n",
    "    'Anarsia lineatella': 2,\n",
    "    'Dead Tree': 3\n",
    "}\n",
    "\n",
    "# Assuming the Label column contains integers (0, 1, 2, 3)\n",
    "label_to_class = {0: 'Healthy', 1: 'Grapholita molesta', 2: 'Anarsia lineatella', 3: 'Dead Tree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the datasets\n",
    "train_dataset_inception = CustomMultimodalDataset(train_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n",
    "val_dataset_inception = CustomMultimodalDataset(val_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n",
    "test_dataset_inception = CustomMultimodalDataset(test_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "train_loader_inception = DataLoader(train_dataset_inception, batch_size=16, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_dataset_inception, batch_size=16, shuffle=True)\n",
    "test_loader_inception = DataLoader(test_dataset_inception, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for other models\n",
    "train_dataset_others = CustomMultimodalDataset(train_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['train'])\n",
    "val_dataset_others = CustomMultimodalDataset(val_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['val'])\n",
    "test_dataset_others = CustomMultimodalDataset(test_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['test'])\n",
    "\n",
    "train_loader_others = DataLoader(train_dataset_others, batch_size=16, shuffle=True)\n",
    "val_loader_others = DataLoader(val_dataset_others, batch_size=16, shuffle=True)\n",
    "test_loader_others = DataLoader(test_dataset_others, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep MLP CSV feature extractor\n",
    "class MLPCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=10):\n",
    "        super(MLPCSVFeatureExtractor, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.extractor = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:  # Ensure x is at least 2D\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != self.extractor[0].in_features:\n",
    "            raise ValueError(f\"Expected input with {self.extractor[0].in_features} features, but got {x.size(1)}\")\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional CSV feature extractor\n",
    "class ConvCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, num_layers=2):\n",
    "        super(ConvCSVFeatureExtractor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        layers = [nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the size of the output after the convolutions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, seq_len).unsqueeze(1)  # (batch_size=1, channels=1, seq_len)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            conv_output_size = dummy_output.view(1, -1).size(1)  # Flatten the output\n",
    "\n",
    "        # Initialize the fully connected layer with the correct input size\n",
    "        self.fc = nn.Linear(conv_output_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Adding channel dimension for Conv1D\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        # print(f'Shape before FC layer: {x.shape}')  # Print the shape for debugging\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_input_dim, csv_hidden_dim, num_classes, fusion_method, csv_model_type='medium', seq_len=None):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        if csv_model_type == 'small':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=2)\n",
    "        elif csv_model_type == 'medium':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=50)\n",
    "        elif csv_model_type == 'deep':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=100)\n",
    "        elif csv_model_type == 'conv_small':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=2)\n",
    "        elif csv_model_type == 'conv_medium':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=50)\n",
    "        elif csv_model_type == 'conv_deep':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=100)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported csv_model_type\")\n",
    "\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.aux_logits = False\n",
    "            image_feature_extractor.AuxLogits = None\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()\n",
    "        elif model_name == 'ViT':\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features\n",
    "            inception_model.aux_logits = False\n",
    "            inception_model.AuxLogits = None\n",
    "            inception_model.fc = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        # Ensure csv_features has the correct shape\n",
    "        if len(csv_features.shape) == 1:\n",
    "            csv_features = csv_features.unsqueeze(0)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        else:\n",
    "            img_features = self.image_feature_extractor(img)\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                if isinstance(img_features, tuple):\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache function\n",
    "def clear_cache():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.cache.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(cnn_model_name, csv_model_name, fusion_mehod, model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, num_epochs=40, initial_lr=0.001, save_path = ''):\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run(run_name=f\"{cnn_model_name}_{csv_model_name}_{fusion_mehod}\"):\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_name\", cnn_model_name)\n",
    "        mlflow.log_param(\"fusion_method\", fusion_mehod)\n",
    "        mlflow.log_param(\"csv_model_type\", csv_model_name)\n",
    "        mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "        mlflow.log_param(\"learning_rate\", initial_lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs_img, inputs_csv, labels = data\n",
    "                inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs_img, inputs_csv)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs_img, inputs_csv, labels = data\n",
    "                    inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "                    outputs = model(inputs_img, inputs_csv)\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        outputs = outputs[0]\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_accuracy\", train_accuracy, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "            # Save the best checkpoint\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'train_accuracy': train_accuracy,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(save_path, f'{cnn_model_name}_{csv_model_name}_{fusion_mehod}_model_checkpoint.pth'))\n",
    "                print(f\"Checkpoint saved to model_checkpoint.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                    break\n",
    "\n",
    "        # Log the trained model\n",
    "        mlflow.pytorch.log_model(model, f\"{cnn_model_name}_{csv_model_name}_{fusion_mehod}_model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_fusion_model(cnn_model_name, csv_model_name, fusion_mehod, model, train_loader, val_loader, num_classes, csv_input_dim, device, num_epochs=40, initial_lr=0.001, save_path=None):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    return train_model(cnn_model_name, csv_model_name, fusion_mehod, model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, num_epochs=num_epochs, initial_lr=initial_lr, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the fusion model\n",
    "def evaluate_fusion_model(model, test_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_confidences = [[] for _ in range(num_classes)]\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate confidence level\n",
    "            softmax_outputs = F.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(softmax_outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                class_label = labels[i].item()\n",
    "                all_confidences[class_label].append(softmax_outputs[i, class_label].item())\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate average confidence per class\n",
    "    avg_confidences_per_class = [np.mean(confidences) if confidences else 0 for confidences in all_confidences]\n",
    "\n",
    "    return test_loss, test_accuracy, avg_confidences_per_class, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "num_classes_inception = len(class_to_idx)\n",
    "num_classes_others = len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features in the CSV data\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv input dimensions\n",
    "csv_input_dim = csv_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv hidden dimensions\n",
    "csv_hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results dictionary\n",
    "crop_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = len(feature_columns)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results folder\n",
    "results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/RESULTS/Multimodal\"\n",
    "results_folder = os.path.join(results_base_dir,'Peach', 'CNN+BB', 'T2+NoES+BS016')\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------- Fusion Method: intermediate ----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- CSV Model Type: small --------------------------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor small\n",
      "InceptionV3 with intermediate fusion and CSV extractor small Test Loss: 0.3684, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor small\n",
      "ResNet152 with intermediate fusion and CSV extractor small Test Loss: 0.6574, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor small\n",
      "VGG19 with intermediate fusion and CSV extractor small Test Loss: 0.8472, Test Accuracy: 59.38%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor small\n",
      "ViT with intermediate fusion and CSV extractor small Test Loss: 0.3478, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor small\n",
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor small Test Loss: 0.3300, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedResNet18 with intermediate fusion and CSV extractor small\n",
      "AttentionAugmentedResNet18 with intermediate fusion and CSV extractor small Test Loss: 0.4056, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- CSV Model Type: medium --------------------------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor medium\n",
      "InceptionV3 with intermediate fusion and CSV extractor medium Test Loss: 0.5515, Test Accuracy: 87.50%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor medium\n",
      "ResNet152 with intermediate fusion and CSV extractor medium Test Loss: 0.2363, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor medium\n",
      "VGG19 with intermediate fusion and CSV extractor medium Test Loss: 0.3769, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor medium\n",
      "ViT with intermediate fusion and CSV extractor medium Test Loss: 0.2296, Test Accuracy: 96.88%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor medium\n",
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor medium Test Loss: 3.1540, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedResNet18 with intermediate fusion and CSV extractor medium\n",
      "AttentionAugmentedResNet18 with intermediate fusion and CSV extractor medium Test Loss: 0.2936, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- CSV Model Type: conv_small --------------------------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor conv_small\n",
      "InceptionV3 with intermediate fusion and CSV extractor conv_small Test Loss: 0.8972, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor conv_small\n",
      "ResNet152 with intermediate fusion and CSV extractor conv_small Test Loss: 0.4638, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor conv_small\n",
      "VGG19 with intermediate fusion and CSV extractor conv_small Test Loss: 0.3126, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor conv_small\n",
      "ViT with intermediate fusion and CSV extractor conv_small Test Loss: 0.3349, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor conv_small\n",
      "Epoch 1/40, Train Loss: 0.8037, Train Accuracy: 73.96%, Val Loss: 11.9470, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.2850, Train Accuracy: 91.67%, Val Loss: 0.8356, Val Accuracy: 84.38%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 3/40, Train Loss: 0.3069, Train Accuracy: 90.62%, Val Loss: 1.1418, Val Accuracy: 87.50%\n",
      "Epoch 4/40, Train Loss: 0.6213, Train Accuracy: 85.42%, Val Loss: 5.3600, Val Accuracy: 68.75%\n",
      "Epoch 5/40, Train Loss: 0.5608, Train Accuracy: 84.38%, Val Loss: 19.8746, Val Accuracy: 18.75%\n",
      "Epoch 6/40, Train Loss: 0.4629, Train Accuracy: 82.29%, Val Loss: 9.1656, Val Accuracy: 62.50%\n",
      "Epoch 7/40, Train Loss: 0.2926, Train Accuracy: 89.58%, Val Loss: 3.2982, Val Accuracy: 75.00%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/24 19:59:47 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor conv_small Test Loss: 2.9762, Test Accuracy: 71.88%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedResNet18 with intermediate fusion and CSV extractor conv_small\n",
      "Epoch 1/40, Train Loss: 1.0878, Train Accuracy: 72.92%, Val Loss: 1.8535, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.3592, Train Accuracy: 86.46%, Val Loss: 3.0153, Val Accuracy: 93.75%\n",
      "Epoch 3/40, Train Loss: 0.3812, Train Accuracy: 80.21%, Val Loss: 2.3764, Val Accuracy: 93.75%\n",
      "Epoch 4/40, Train Loss: 0.3962, Train Accuracy: 88.54%, Val Loss: 0.2689, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 5/40, Train Loss: 0.5055, Train Accuracy: 79.17%, Val Loss: 0.1748, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 6/40, Train Loss: 0.4148, Train Accuracy: 83.33%, Val Loss: 0.2032, Val Accuracy: 93.75%\n",
      "Epoch 7/40, Train Loss: 0.2876, Train Accuracy: 89.58%, Val Loss: 0.2436, Val Accuracy: 93.75%\n",
      "Epoch 8/40, Train Loss: 0.2061, Train Accuracy: 91.67%, Val Loss: 0.1348, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 9/40, Train Loss: 0.2796, Train Accuracy: 87.50%, Val Loss: 0.1664, Val Accuracy: 96.88%\n",
      "Epoch 10/40, Train Loss: 0.4284, Train Accuracy: 85.42%, Val Loss: 0.2092, Val Accuracy: 90.62%\n",
      "Epoch 11/40, Train Loss: 0.2553, Train Accuracy: 91.67%, Val Loss: 0.5239, Val Accuracy: 84.38%\n",
      "Epoch 12/40, Train Loss: 0.1772, Train Accuracy: 93.75%, Val Loss: 0.2289, Val Accuracy: 93.75%\n",
      "Epoch 13/40, Train Loss: 0.4213, Train Accuracy: 85.42%, Val Loss: 0.1737, Val Accuracy: 90.62%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(11272) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/24 21:01:22 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionAugmentedResNet18 with intermediate fusion and CSV extractor conv_small Test Loss: 0.3607, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- CSV Model Type: conv_medium --------------------------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 0.6522, Train Accuracy: 75.00%, Val Loss: 2.0429, Val Accuracy: 71.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.3304, Train Accuracy: 84.38%, Val Loss: 10.1900, Val Accuracy: 68.75%\n",
      "Epoch 3/40, Train Loss: 0.2268, Train Accuracy: 93.75%, Val Loss: 0.1074, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 4/40, Train Loss: 0.3006, Train Accuracy: 89.58%, Val Loss: 0.7810, Val Accuracy: 84.38%\n",
      "Epoch 5/40, Train Loss: 0.1617, Train Accuracy: 93.75%, Val Loss: 0.0947, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 6/40, Train Loss: 0.1914, Train Accuracy: 92.71%, Val Loss: 0.5900, Val Accuracy: 96.88%\n",
      "Epoch 7/40, Train Loss: 0.6004, Train Accuracy: 80.21%, Val Loss: 10.7425, Val Accuracy: 65.62%\n",
      "Epoch 8/40, Train Loss: 0.3087, Train Accuracy: 91.67%, Val Loss: 14.1229, Val Accuracy: 78.12%\n",
      "Epoch 9/40, Train Loss: 0.9136, Train Accuracy: 78.12%, Val Loss: 2.7208, Val Accuracy: 90.62%\n",
      "Epoch 10/40, Train Loss: 0.4638, Train Accuracy: 86.46%, Val Loss: 2.2590, Val Accuracy: 87.50%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(12986) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/24 21:43:30 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3 with intermediate fusion and CSV extractor conv_medium Test Loss: 2.8001, Test Accuracy: 90.62%\n",
      "\n",
      "\n",
      "Training ResNet152 with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 0.8556, Train Accuracy: 68.75%, Val Loss: 1161.5923, Val Accuracy: 34.38%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.3004, Train Accuracy: 86.46%, Val Loss: 906.2516, Val Accuracy: 68.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 3/40, Train Loss: 0.3851, Train Accuracy: 85.42%, Val Loss: 546.2539, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 4/40, Train Loss: 0.2863, Train Accuracy: 91.67%, Val Loss: 109.1504, Val Accuracy: 6.25%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 5/40, Train Loss: 0.2136, Train Accuracy: 94.79%, Val Loss: 43.6733, Val Accuracy: 21.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 6/40, Train Loss: 0.2561, Train Accuracy: 90.62%, Val Loss: 0.8497, Val Accuracy: 81.25%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 7/40, Train Loss: 0.3204, Train Accuracy: 88.54%, Val Loss: 1.0880, Val Accuracy: 84.38%\n",
      "Epoch 8/40, Train Loss: 0.8424, Train Accuracy: 86.46%, Val Loss: 0.0915, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 9/40, Train Loss: 0.4160, Train Accuracy: 87.50%, Val Loss: 1.1719, Val Accuracy: 65.62%\n",
      "Epoch 10/40, Train Loss: 0.2439, Train Accuracy: 90.62%, Val Loss: 0.0421, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 11/40, Train Loss: 0.1827, Train Accuracy: 93.75%, Val Loss: 0.0628, Val Accuracy: 96.88%\n",
      "Epoch 12/40, Train Loss: 0.4943, Train Accuracy: 80.21%, Val Loss: 0.1149, Val Accuracy: 96.88%\n",
      "Epoch 13/40, Train Loss: 0.1838, Train Accuracy: 93.75%, Val Loss: 0.1049, Val Accuracy: 96.88%\n",
      "Epoch 14/40, Train Loss: 0.1961, Train Accuracy: 93.75%, Val Loss: 0.2153, Val Accuracy: 93.75%\n",
      "Epoch 15/40, Train Loss: 0.0902, Train Accuracy: 95.83%, Val Loss: 0.8003, Val Accuracy: 75.00%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(14752) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/24 22:45:55 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet152 with intermediate fusion and CSV extractor conv_medium Test Loss: 0.5376, Test Accuracy: 84.38%\n",
      "\n",
      "\n",
      "Training VGG19 with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 11.8923, Train Accuracy: 36.46%, Val Loss: 1.2074, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 1.4736, Train Accuracy: 53.12%, Val Loss: 1.1217, Val Accuracy: 34.38%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 3/40, Train Loss: 1.2143, Train Accuracy: 52.08%, Val Loss: 1.0326, Val Accuracy: 18.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 4/40, Train Loss: 0.9942, Train Accuracy: 48.96%, Val Loss: 0.7788, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 5/40, Train Loss: 1.1280, Train Accuracy: 54.17%, Val Loss: 1.0320, Val Accuracy: 34.38%\n",
      "Epoch 6/40, Train Loss: 1.3283, Train Accuracy: 41.67%, Val Loss: 0.9429, Val Accuracy: 62.50%\n",
      "Epoch 7/40, Train Loss: 1.0814, Train Accuracy: 54.17%, Val Loss: 0.7854, Val Accuracy: 62.50%\n",
      "Epoch 8/40, Train Loss: 0.8156, Train Accuracy: 63.54%, Val Loss: 1.0112, Val Accuracy: 62.50%\n",
      "Epoch 9/40, Train Loss: 1.0005, Train Accuracy: 59.38%, Val Loss: 0.6382, Val Accuracy: 68.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 10/40, Train Loss: 0.6376, Train Accuracy: 80.21%, Val Loss: 0.3382, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 11/40, Train Loss: 0.5898, Train Accuracy: 82.29%, Val Loss: 0.2257, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 12/40, Train Loss: 0.5036, Train Accuracy: 79.17%, Val Loss: 0.2411, Val Accuracy: 93.75%\n",
      "Epoch 13/40, Train Loss: 0.4516, Train Accuracy: 84.38%, Val Loss: 0.2156, Val Accuracy: 90.62%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 14/40, Train Loss: 0.4894, Train Accuracy: 86.46%, Val Loss: 0.3005, Val Accuracy: 93.75%\n",
      "Epoch 15/40, Train Loss: 1.6968, Train Accuracy: 77.08%, Val Loss: 0.6919, Val Accuracy: 90.62%\n",
      "Epoch 16/40, Train Loss: 0.7659, Train Accuracy: 58.33%, Val Loss: 0.4492, Val Accuracy: 87.50%\n",
      "Epoch 17/40, Train Loss: 0.7281, Train Accuracy: 80.21%, Val Loss: 1.2470, Val Accuracy: 9.38%\n",
      "Epoch 18/40, Train Loss: 1.0642, Train Accuracy: 54.17%, Val Loss: 0.4463, Val Accuracy: 90.62%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(17286) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/25 00:05:17 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19 with intermediate fusion and CSV extractor conv_medium Test Loss: 0.5104, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "Training ViT with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 0.8411, Train Accuracy: 77.08%, Val Loss: 0.5027, Val Accuracy: 87.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.5257, Train Accuracy: 89.58%, Val Loss: 0.6762, Val Accuracy: 84.38%\n",
      "Epoch 3/40, Train Loss: 0.3557, Train Accuracy: 89.58%, Val Loss: 0.3377, Val Accuracy: 90.62%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 4/40, Train Loss: 0.3258, Train Accuracy: 90.62%, Val Loss: 0.4675, Val Accuracy: 87.50%\n",
      "Epoch 5/40, Train Loss: 0.2319, Train Accuracy: 90.62%, Val Loss: 0.4168, Val Accuracy: 90.62%\n",
      "Epoch 6/40, Train Loss: 0.2111, Train Accuracy: 91.67%, Val Loss: 0.4440, Val Accuracy: 90.62%\n",
      "Epoch 7/40, Train Loss: 0.1874, Train Accuracy: 92.71%, Val Loss: 0.5999, Val Accuracy: 90.62%\n",
      "Epoch 8/40, Train Loss: 0.1578, Train Accuracy: 93.75%, Val Loss: 0.8062, Val Accuracy: 90.62%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18474) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/25 00:31:02 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT with intermediate fusion and CSV extractor conv_medium Test Loss: 0.7882, Test Accuracy: 81.25%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 0.7852, Train Accuracy: 72.92%, Val Loss: 19.0833, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.6674, Train Accuracy: 78.12%, Val Loss: 9.6629, Val Accuracy: 62.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 3/40, Train Loss: 0.3009, Train Accuracy: 86.46%, Val Loss: 2.3552, Val Accuracy: 71.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 4/40, Train Loss: 0.1114, Train Accuracy: 93.75%, Val Loss: 0.3461, Val Accuracy: 90.62%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 5/40, Train Loss: 0.1176, Train Accuracy: 96.88%, Val Loss: 0.2105, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 6/40, Train Loss: 0.0919, Train Accuracy: 96.88%, Val Loss: 0.3559, Val Accuracy: 93.75%\n",
      "Epoch 7/40, Train Loss: 0.2562, Train Accuracy: 93.75%, Val Loss: 1.0178, Val Accuracy: 93.75%\n",
      "Epoch 8/40, Train Loss: 0.1822, Train Accuracy: 95.83%, Val Loss: 0.3003, Val Accuracy: 87.50%\n",
      "Epoch 9/40, Train Loss: 0.1704, Train Accuracy: 92.71%, Val Loss: 0.3835, Val Accuracy: 90.62%\n",
      "Epoch 10/40, Train Loss: 0.1278, Train Accuracy: 94.79%, Val Loss: 0.5800, Val Accuracy: 90.62%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(19651) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/25 00:56:41 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionAugmentedInceptionV3 with intermediate fusion and CSV extractor conv_medium Test Loss: 1.3394, Test Accuracy: 71.88%\n",
      "\n",
      "\n",
      "Training AttentionAugmentedResNet18 with intermediate fusion and CSV extractor conv_medium\n",
      "Epoch 1/40, Train Loss: 0.9418, Train Accuracy: 63.54%, Val Loss: 0.6806, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 2/40, Train Loss: 0.4529, Train Accuracy: 86.46%, Val Loss: 0.2910, Val Accuracy: 93.75%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 3/40, Train Loss: 0.3985, Train Accuracy: 84.38%, Val Loss: 0.4055, Val Accuracy: 90.62%\n",
      "Epoch 4/40, Train Loss: 0.3231, Train Accuracy: 86.46%, Val Loss: 0.2497, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 5/40, Train Loss: 0.3598, Train Accuracy: 89.58%, Val Loss: 0.1323, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 6/40, Train Loss: 0.3079, Train Accuracy: 89.58%, Val Loss: 0.1461, Val Accuracy: 93.75%\n",
      "Epoch 7/40, Train Loss: 0.2481, Train Accuracy: 90.62%, Val Loss: 0.0904, Val Accuracy: 96.88%\n",
      "Checkpoint saved to model_checkpoint.pth\n",
      "Epoch 8/40, Train Loss: 0.3644, Train Accuracy: 86.46%, Val Loss: 0.1648, Val Accuracy: 93.75%\n",
      "Epoch 9/40, Train Loss: 0.2046, Train Accuracy: 90.62%, Val Loss: 0.1233, Val Accuracy: 93.75%\n",
      "Epoch 10/40, Train Loss: 0.4373, Train Accuracy: 86.46%, Val Loss: 0.1112, Val Accuracy: 96.88%\n",
      "Epoch 11/40, Train Loss: 0.2304, Train Accuracy: 91.67%, Val Loss: 0.1088, Val Accuracy: 96.88%\n",
      "Epoch 12/40, Train Loss: 0.3103, Train Accuracy: 89.58%, Val Loss: 0.1533, Val Accuracy: 93.75%\n",
      "Early stopping due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(20671) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:17: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "2024/08/25 01:31:44 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionAugmentedResNet18 with intermediate fusion and CSV extractor conv_medium Test Loss: 0.2379, Test Accuracy: 93.75%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------- Fusion Method: late ----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- CSV Model Type: small --------------------------------\n",
      "Training InceptionV3 with late fusion and CSV extractor small\n",
      "Epoch 1/40, Train Loss: 0.5688, Train Accuracy: 80.21%, Val Loss: 4.6170, Val Accuracy: 37.50%\n",
      "Checkpoint saved to model_checkpoint.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 63\u001b[0m\n\u001b[1;32m     57\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# trained_model.eval()\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# print(trained_model.eval())\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_train_fusion_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsv_model_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfusion_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfusion_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader_others\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader_others\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes_others\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsv_input_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_folder\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Evaluate the loaded model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m test_loss, test_accuracy, all_confidences, all_predictions, all_labels \u001b[38;5;241m=\u001b[39m evaluate_fusion_model(\n\u001b[1;32m     80\u001b[0m     fusion_model, test_loader_others, nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), device, \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     81\u001b[0m )\n",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m, in \u001b[0;36mcreate_and_train_fusion_model\u001b[0;34m(cnn_model_name, csv_model_name, fusion_mehod, model, train_loader, val_loader, num_classes, csv_input_dim, device, num_epochs, initial_lr, save_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39minitial_lr)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfusion_mehod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_input_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(cnn_model_name, csv_model_name, fusion_mehod, model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, num_epochs, initial_lr, save_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_csv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[63], line 30\u001b[0m, in \u001b[0;36mCustomMultimodalDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img_path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m img_path\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tiff\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bmp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# Handle bounding boxes for specific image columns\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img_col \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround_RGB_Image_with_Bounding_Boxes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate the models\n",
    "with open(os.path.join(results_folder, 'train_val_test_results8.txt'), 'w') as f:\n",
    "    for fusion_method in ['intermediate', 'late']:\n",
    "        output_line = f'---------------------------------------------------- Fusion Method: {fusion_method} ----------------------------------------------------'\n",
    "        print(output_line)\n",
    "        f.write(output_line + '\\n')\n",
    "\n",
    "        for csv_model_type in ['small','medium','conv_small', 'conv_medium']:\n",
    "            cnn_feature_extractors = {\n",
    "                'InceptionV3': models.inception_v3(pretrained=True).to(device),\n",
    "                'ResNet152': models.resnet152(pretrained=True).to(device),\n",
    "                'VGG19': models.vgg19(pretrained=True).to(device),\n",
    "                'ViT': ViT(\n",
    "                    image_size=224,\n",
    "                    patch_size=16,\n",
    "                    num_classes=num_classes_others,\n",
    "                    dim=256,\n",
    "                    depth=6,\n",
    "                    heads=24,\n",
    "                    mlp_dim=2048,\n",
    "                    dropout=0.1,\n",
    "                    emb_dropout=0.1\n",
    "                ).to(device),\n",
    "                \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n",
    "                \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n",
    "            }\n",
    "\n",
    "            if 'InceptionV3' in cnn_feature_extractors:\n",
    "                cnn_feature_extractors['InceptionV3'].aux_logits = False\n",
    "\n",
    "            output_line = f'-------------------------------- CSV Model Type: {csv_model_type} --------------------------------'\n",
    "            print(output_line)\n",
    "            f.write(output_line + '\\n')\n",
    "\n",
    "            for model_name, image_feature_extractor in cnn_feature_extractors.items():\n",
    "                output_line = f'Training {model_name} with {fusion_method} fusion and CSV extractor {csv_model_type}'\n",
    "                print(output_line)\n",
    "                f.write(output_line + '\\n')\n",
    "\n",
    "                # Initialize and train the fusion model\n",
    "                fusion_model = FusionModel(\n",
    "                    model_name=model_name,\n",
    "                    image_feature_extractor=image_feature_extractor,\n",
    "                    csv_input_dim=csv_input_dim,\n",
    "                    csv_hidden_dim=csv_hidden_dim,\n",
    "                    num_classes=num_classes_others,\n",
    "                    fusion_method=fusion_method,\n",
    "                    csv_model_type=csv_model_type,\n",
    "                    seq_len=seq_len\n",
    "                ).to(device)\n",
    "\n",
    "                checkpoint_path = os.path.join(results_folder, f'{model_name}_{csv_model_type}_{fusion_method}_model_checkpoint.pth')\n",
    "                if os.path.exists(checkpoint_path):\n",
    "                    checkpoint = torch.load(checkpoint_path)\n",
    "                    fusion_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    trained_model = fusion_model\n",
    "                    train_loss = checkpoint['train_loss']\n",
    "                    # trained_model.eval()\n",
    "\n",
    "                    # print(trained_model.eval())\n",
    "\n",
    "                else:\n",
    "                    trained_model = create_and_train_fusion_model(\n",
    "                        model_name,\n",
    "                        csv_model_type,\n",
    "                        fusion_method,\n",
    "                        fusion_model,\n",
    "                        train_loader_others,\n",
    "                        val_loader_others,\n",
    "                        num_classes_others,\n",
    "                        csv_input_dim,\n",
    "                        device,\n",
    "                        num_epochs=40,\n",
    "                        initial_lr=0.001,\n",
    "                        save_path=results_folder\n",
    "                    )\n",
    "\n",
    "                # Evaluate the loaded model\n",
    "                test_loss, test_accuracy, all_confidences, all_predictions, all_labels = evaluate_fusion_model(\n",
    "                    fusion_model, test_loader_others, nn.CrossEntropyLoss(), device, 4\n",
    "                )\n",
    "\n",
    "                # Store the results\n",
    "                crop_results[f\"{model_name}_{fusion_method}_{csv_model_type}\"] = {\n",
    "                    'model': trained_model,\n",
    "                    'model_name': model_name,\n",
    "                    'fusion_method': fusion_method,\n",
    "                    'csv_model_type': csv_model_type,\n",
    "                    'test_loss': test_loss,\n",
    "                    'test_accuracy': test_accuracy\n",
    "                }\n",
    "\n",
    "                output_line = f'{model_name} with {fusion_method} fusion and CSV extractor {csv_model_type} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%'\n",
    "                print(output_line)\n",
    "                f.write(output_line + '\\n')\n",
    "\n",
    "                # Clear GPU memory\n",
    "                del fusion_model\n",
    "                del trained_model\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                print('\\n')\n",
    "                f.write('\\n')\n",
    "\n",
    "            print('\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        f.write('----------------------------------------------------------------------------------------------------------------------\\n')\n",
    "        print('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(os.path.join(results_folder, filename))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "def plot_accuracy_comparison(results):\n",
    "    accuracies = [result['test_accuracy'] for result in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=90)  # Make x-axis labels diagonal\n",
    "    plt.show()\n",
    "    save_figure(fig, 'all_fusion_accuracy_comparison.png')\n",
    "\n",
    "    for fusion_method in ['late', 'intermediate']:\n",
    "        # Filter models based on the fusion method\n",
    "        filtered_results = {model: details for model, details in results.items() if details['fusion_method'] == fusion_method}\n",
    "\n",
    "        if filtered_results:  # Check if there are models with this fusion method\n",
    "            accuracies = [details['test_accuracy'] for details in filtered_results.values()]\n",
    "            model_names = list(filtered_results.keys())\n",
    "\n",
    "            fig = plt.figure(figsize=(20, 10))\n",
    "            plt.bar(model_names, accuracies)\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.xlabel('Model')\n",
    "            plt.xticks(rotation=90)  # Make x-axis labels diagonal\n",
    "            plt.title(f'Accuracy Comparison for {fusion_method.capitalize()} Fusion')\n",
    "            plt.show()\n",
    "            save_figure(fig, f'{fusion_method}_accuracy_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "plot_accuracy_comparison(crop_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader_inception, test_loader_others):\n",
    "    metrics_data = []\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, csv_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "\n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)\n",
    "    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, model_name, num_images=5):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    class_labels = list(test_loader.dataset.class_to_idx.keys())\n",
    "\n",
    "    # Get a batch of images, CSV inputs, and labels\n",
    "    batch = next(iter(test_loader))\n",
    "    images, csv_inputs, labels = batch\n",
    "\n",
    "    images, csv_inputs, labels = images[:num_images].to(device), csv_inputs[:num_images].to(device), labels[:num_images]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, csv_inputs)\n",
    "        softmax_outputs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(softmax_outputs, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"True: {class_labels[labels[i]]}\\nPred: {class_labels[predicted[i].cpu()]}\\nConf: {confidence[i].cpu():.2f}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_classification_results_with_confidence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "    \n",
    "    print(f'Displaying results for {model_name}')\n",
    "    display_classification_results(crop_results[model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader, model_name):\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch in test_loader:\n",
    "        images, csv_features, labels = batch\n",
    "        images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, csv_features)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Ensure target names are in the correct order\n",
    "    target_names = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "    labels_list = [class_to_idx[name] for name in target_names]  # Get indices in the specified order\n",
    "\n",
    "    print(f'Classification Report for {model_name}:')\n",
    "    report = classification_report(all_labels, all_predicted, labels=labels_list, target_names=target_names)\n",
    "    print(report)\n",
    "\n",
    "    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "        \n",
    "    print(f'Displaying classification report for {model_name}')\n",
    "    display_classification_report(crop_results[model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Plotting\n",
    "def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n",
    "    # Ensure the classes are in the correct order\n",
    "    classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels, labels=[class_to_idx[cls] for cls in classes])\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "    fig.delaxes(fig.axes[1])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Predicted Label', fontsize=50)\n",
    "    plt.ylabel('True Label', fontsize=50)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_confusion_matrix.png')\n",
    "\n",
    "def get_all_labels_and_preds(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, csv_features, labels = data\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, csv_features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "            all_preds.extend(preds.cpu().numpy().astype(int))\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def generate_confusion_matrices(results, test_loader_inception, test_loader_others):\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "        model = model_info['model']\n",
    "        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n",
    "        plot_confusion_matrix(labels, pred_labels, classes, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrices(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most incorrect predictions\n",
    "def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n",
    "    rows = int(np.ceil(np.sqrt(n_images)))\n",
    "    cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 20))\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        if i >= len(incorrect):\n",
    "            break\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n",
    "                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_most_incorrect.png')\n",
    "\n",
    "\n",
    "def get_all_details(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "            outputs = model(images, csv_features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu())\n",
    "\n",
    "    return all_images, all_labels, all_preds, all_probs\n",
    "\n",
    "def plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36):\n",
    "    classes = list(test_loader_inception.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n",
    "        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n",
    "        incorrect_examples = []\n",
    "\n",
    "        for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "            if not correct:\n",
    "                incorrect_examples.append((image, label, prob))\n",
    "\n",
    "        incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n",
    "        plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_incorrect_predictions(crop_results, test_loader_inception, test_loader_others, n_images=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all results\n",
    "def generate_all_results(results, test_loader_inception, test_loader_others):\n",
    "    plot_accuracy_comparison(results)\n",
    "    display_model_metrics_table(results, test_loader_inception, test_loader_others)\n",
    "    generate_confusion_matrices(results, test_loader_inception, test_loader_others)\n",
    "    plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36)\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        model = model_info['model']\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "        display_classification_results(model, test_loader, model_name)\n",
    "        display_classification_report(model, test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the generate_all_results function\n",
    "generate_all_results(crop_results, test_loader_inception, test_loader_others)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
