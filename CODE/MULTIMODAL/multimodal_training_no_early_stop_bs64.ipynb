{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "# from ft_transformer import FTTransformer\n",
    "from transformers import BertModel\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet18, attention_augmented_inceptionv3, attention_augmented_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = ['04_11_21',\n",
    "'14_09_21',\n",
    "'14_09_22',\n",
    "'15_07_22',\n",
    "'25_05_22',\n",
    "'27_07_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/DATA/Peach/'\n",
    "date = date_list[0]\n",
    "date_dir = os.path.join(base_dir, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for the images\n",
    "uav_dir = os.path.join(date_dir, \"Aerial_UAV_Photos\")\n",
    "rgb_dir = os.path.join(date_dir, \"Ground_RGB_Photos\")\n",
    "multispectral_dir = os.path.join(date_dir, \"Ground_Multispectral_Photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "multimodal_data_path = os.path.join(base_dir, \"combined_multimodal_data.csv\")\n",
    "multimodal_df = pd.read_csv(multimodal_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tree_ID</th>\n",
       "      <th>Orchard_Mapping_Image</th>\n",
       "      <th>Aerial_UAV_Image</th>\n",
       "      <th>Ground_RGB_Image</th>\n",
       "      <th>Ground_RGB_Image_with_Bounding_Boxes</th>\n",
       "      <th>Ground_RGB_Image_Annotations</th>\n",
       "      <th>Multispectral_RGB_Image</th>\n",
       "      <th>Multispectral_REG_Image</th>\n",
       "      <th>Multispectral_RED_Image</th>\n",
       "      <th>...</th>\n",
       "      <th>Multispectral_NDRE</th>\n",
       "      <th>Multispectral_SAVI</th>\n",
       "      <th>Multispectral_GNDVI</th>\n",
       "      <th>Multispectral_RVI</th>\n",
       "      <th>Multispectral_TVI</th>\n",
       "      <th>Multispectral_NDVI_Image</th>\n",
       "      <th>Multispectral_GNDVI_Image</th>\n",
       "      <th>Multispectral_NDRE_Image</th>\n",
       "      <th>Multispectral_SAVI_Image</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-1</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054946</td>\n",
       "      <td>0.578614</td>\n",
       "      <td>0.308877</td>\n",
       "      <td>798.861030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-2</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>0.661596</td>\n",
       "      <td>0.420410</td>\n",
       "      <td>815.203405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-4</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085527</td>\n",
       "      <td>0.585869</td>\n",
       "      <td>0.422484</td>\n",
       "      <td>361.728795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>29-3</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059559</td>\n",
       "      <td>0.602647</td>\n",
       "      <td>0.414713</td>\n",
       "      <td>442.763371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04_11_21</td>\n",
       "      <td>28-10</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075293</td>\n",
       "      <td>0.628999</td>\n",
       "      <td>0.412634</td>\n",
       "      <td>789.667606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>04_11_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-17</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-18</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-19</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-20</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>27_07_21</td>\n",
       "      <td>30-21</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>/Users/izzymohamed/Desktop/Vision For Social G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27_07_21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5328 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Tree_ID                              Orchard_Mapping_Image  \\\n",
       "0     04_11_21    29-1  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     04_11_21    29-2  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     04_11_21    29-4  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     04_11_21    29-3  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     04_11_21   28-10  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...        ...     ...                                                ...   \n",
       "5323  27_07_21   30-17  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5324  27_07_21   30-18  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5325  27_07_21   30-19  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5326  27_07_21   30-20  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5327  27_07_21   30-21  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "\n",
       "                                       Aerial_UAV_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5324  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5325  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5326  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "5327  /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "\n",
       "                                       Ground_RGB_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                   Ground_RGB_Image_with_Bounding_Boxes  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                           Ground_RGB_Image_Annotations  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                                Multispectral_RGB_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                                Multispectral_REG_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                                Multispectral_RED_Image  ...  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...  ...   \n",
       "...                                                 ...  ...   \n",
       "5323                                                NaN  ...   \n",
       "5324                                                NaN  ...   \n",
       "5325                                                NaN  ...   \n",
       "5326                                                NaN  ...   \n",
       "5327                                                NaN  ...   \n",
       "\n",
       "     Multispectral_NDRE Multispectral_SAVI Multispectral_GNDVI  \\\n",
       "0              0.054946           0.578614            0.308877   \n",
       "1              0.098110           0.661596            0.420410   \n",
       "2              0.085527           0.585869            0.422484   \n",
       "3              0.059559           0.602647            0.414713   \n",
       "4              0.075293           0.628999            0.412634   \n",
       "...                 ...                ...                 ...   \n",
       "5323                NaN                NaN                 NaN   \n",
       "5324                NaN                NaN                 NaN   \n",
       "5325                NaN                NaN                 NaN   \n",
       "5326                NaN                NaN                 NaN   \n",
       "5327                NaN                NaN                 NaN   \n",
       "\n",
       "     Multispectral_RVI  Multispectral_TVI  \\\n",
       "0           798.861030                NaN   \n",
       "1           815.203405                NaN   \n",
       "2           361.728795                NaN   \n",
       "3           442.763371                NaN   \n",
       "4           789.667606                NaN   \n",
       "...                ...                ...   \n",
       "5323               NaN                NaN   \n",
       "5324               NaN                NaN   \n",
       "5325               NaN                NaN   \n",
       "5326               NaN                NaN   \n",
       "5327               NaN                NaN   \n",
       "\n",
       "                               Multispectral_NDVI_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                              Multispectral_GNDVI_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                               Multispectral_NDRE_Image  \\\n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...   \n",
       "...                                                 ...   \n",
       "5323                                                NaN   \n",
       "5324                                                NaN   \n",
       "5325                                                NaN   \n",
       "5326                                                NaN   \n",
       "5327                                                NaN   \n",
       "\n",
       "                               Multispectral_SAVI_Image      date  \n",
       "0     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "1     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "2     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "3     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "4     /Users/izzymohamed/Desktop/Vision For Social G...  04_11_21  \n",
       "...                                                 ...       ...  \n",
       "5323                                                NaN  27_07_21  \n",
       "5324                                                NaN  27_07_21  \n",
       "5325                                                NaN  27_07_21  \n",
       "5326                                                NaN  27_07_21  \n",
       "5327                                                NaN  27_07_21  \n",
       "\n",
       "[5328 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multimodal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Tree_ID', 'Orchard_Mapping_Image', 'Aerial_UAV_Image',\n",
       "       'Ground_RGB_Image', 'Ground_RGB_Image_with_Bounding_Boxes',\n",
       "       'Ground_RGB_Image_Annotations', 'Multispectral_RGB_Image',\n",
       "       'Multispectral_REG_Image', 'Multispectral_RED_Image',\n",
       "       'Multispectral_NIR_Image', 'Multispectral_GRE_Image',\n",
       "       'Multispectral_RGB_Bounding_Box_Image',\n",
       "       'Multispectral_RGB_Bounding_Box_Annotation', 'Label', 'UAV_NDVI',\n",
       "       'UAV_EVI', 'UAV_NDRE', 'UAV_SAVI', 'UAV_GNDVI', 'UAV_RVI', 'UAV_TVI',\n",
       "       'x1', 'x2', 'y1', 'y2', 'Multispectral_NDVI', 'Multispectral_EVI',\n",
       "       'Multispectral_NDRE', 'Multispectral_SAVI', 'Multispectral_GNDVI',\n",
       "       'Multispectral_RVI', 'Multispectral_TVI', 'Multispectral_NDVI_Image',\n",
       "       'Multispectral_GNDVI_Image', 'Multispectral_NDRE_Image',\n",
       "       'Multispectral_SAVI_Image', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multimodal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image columns\n",
    "image_columns = [\n",
    "    'Ground_RGB_Image_with_Bounding_Boxes','Ground_RGB_Image_Annotations', 'Multispectral_NDVI_Image','Multispectral_RGB_Bounding_Box_Image', 'Multispectral_RGB_Bounding_Box_Annotation',\n",
    "    # 'Multispectral_RGB_Image', 'Multispectral_REG_Image', 'Multispectral_RED_Image', 'Multispectral_NIR_Image', 'Multispectral_GRE_Image',\n",
    "    # 'Multispectral_NDVI_Image', 'Multispectral_GNDVI_Image', 'Multispectral_NDRE_Image', 'Multispectral_SAVI_Image'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "feature_columns = [\n",
    "    # 'UAV_NDVI', 'UAV_EVI', 'UAV_NDRE', 'UAV_SAVI', 'UAV_GNDVI', 'UAV_RVI', 'UAV_TVI', 'Multispectral_SAVI',\n",
    "    'Multispectral_NDVI', 'Multispectral_GNDVI', 'Multispectral_SAVI',\n",
    "    # 'Multispectral_RVI' 'Multispectral_EVI', 'Multispectral_NDRE',\n",
    "]\n",
    "csv_features = multimodal_df[feature_columns].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with label = 0\n",
    "multimodal_df = multimodal_df[multimodal_df['Label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset to ensure all image paths are strings\n",
    "def is_valid_image_path(path):\n",
    "    return isinstance(path, str) and path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if all image columns contain valid strings\n",
    "def valid_image_paths(row):\n",
    "    return all(isinstance(row[col], str) for col in image_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "valid_rows = multimodal_df.apply(valid_image_paths, axis=1)\n",
    "multimodal_df = multimodal_df[valid_rows]\n",
    "multimodal_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and exclude non-numeric columns from the features\n",
    "numeric_columns = multimodal_df.select_dtypes(include=[np.number]).columns\n",
    "csv_image_paths = multimodal_df[image_columns].values\n",
    "csv_labels = multimodal_df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files if necessary (if working in a directory with such files)\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets directly from the CSV\n",
    "train_df, test_df = train_test_split(multimodal_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 96\n",
      "Validation files: 32\n",
      "Test files: 32\n"
     ]
    }
   ],
   "source": [
    "# Use the lists of file paths for your dataset loading and transformations\n",
    "print(f\"Train files: {len(train_df)}\")\n",
    "print(f\"Validation files: {len(val_df)}\")\n",
    "print(f\"Test files: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard image sizes\n",
    "inception_size = 299\n",
    "other_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the data transformations\n",
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((inception_size, inception_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((other_size, other_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, image_columns, feature_columns, class_to_idx, transform=None):\n",
    "        self.df = df\n",
    "        self.image_columns = image_columns\n",
    "        self.feature_columns = feature_columns\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define the mapping from integers to class names\n",
    "        self.label_to_class_name = {\n",
    "            0: 'Healthy',\n",
    "            1: 'Grapholita molesta',\n",
    "            2: 'Anarsia lineatella',\n",
    "            3: 'Dead Tree'\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        images = []\n",
    "        for img_col in self.image_columns:\n",
    "            img_path = row[img_col]\n",
    "            \n",
    "            # Ensure the path is a string and points to a valid image file\n",
    "            if isinstance(img_path, str) and img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                    \n",
    "                    # Handle bounding boxes for specific image columns\n",
    "                    if img_col == 'Ground_RGB_Image_with_Bounding_Boxes':\n",
    "                        annotation_path = row['Ground_RGB_Image_Annotations']\n",
    "                        if os.path.exists(annotation_path):\n",
    "                            try:\n",
    "                                with open(annotation_path, 'r') as f:\n",
    "                                    bbox_data = json.load(f)\n",
    "                                \n",
    "                                # Apply each bounding box found in the annotation\n",
    "                                for region in bbox_data.get('regions', []):\n",
    "                                    x1, y1 = min(region['points']['x']), min(region['points']['y'])\n",
    "                                    x2, y2 = max(region['points']['x']), max(region['points']['y'])\n",
    "                                    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "                                    \n",
    "                                    if self.transform:\n",
    "                                        cropped_image = self.transform(cropped_image)\n",
    "                                    images.append(cropped_image)\n",
    "                                    \n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Skipping invalid JSON file for {img_col} at index {idx}: {annotation_path}\")\n",
    "                        else:\n",
    "                            print(f\"Annotation file not found for {img_col} at index {idx}: {annotation_path}\")\n",
    "                    \n",
    "                    elif img_col == 'Multispectral_RGB_Bounding_Box_Image':\n",
    "                        annotation_path = row['Multispectral_RGB_Bounding_Box_Annotation']\n",
    "                        if os.path.exists(annotation_path):\n",
    "                            try:\n",
    "                                with open(annotation_path, 'r') as f:\n",
    "                                    bbox_data = json.load(f)\n",
    "                                \n",
    "                                # Apply each bounding box found in the annotation\n",
    "                                for region in bbox_data.get('regions', []):\n",
    "                                    x1, y1 = min(region['points']['x']), min(region['points']['y'])\n",
    "                                    x2, y2 = max(region['points']['x']), max(region['points']['y'])\n",
    "                                    cropped_image = image.crop((x1, y1, x2, y2))\n",
    "                                    \n",
    "                                    if self.transform:\n",
    "                                        cropped_image = self.transform(cropped_image)\n",
    "                                    images.append(cropped_image)\n",
    "                                    \n",
    "                            except json.JSONDecodeError:\n",
    "                                print(f\"Skipping invalid JSON file for {img_col} at index {idx}: {annotation_path}\")\n",
    "                        else:\n",
    "                            print(f\"Annotation file not found for {img_col} at index {idx}: {annotation_path}\")\n",
    "\n",
    "                    # Apply transformation to the image if no bounding boxes are required\n",
    "                    elif self.transform:\n",
    "                        image = self.transform(image)\n",
    "                        images.append(image)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Could not open image at path {img_path} at index {idx}: {e}\")\n",
    "            else:\n",
    "                continue  # Skip non-image files\n",
    "\n",
    "        if not images:\n",
    "            raise ValueError(f\"No valid images found for index {idx}\")\n",
    "\n",
    "        # Stack images into a tensor and average along the channel dimension\n",
    "        images = torch.stack(images).mean(dim=0)  # Average the images along the channel dimension to maintain 3 channels\n",
    "\n",
    "        # Extract features from the DataFrame\n",
    "        csv_row = row[self.feature_columns].values.astype(np.float32)\n",
    "        \n",
    "        # Convert the integer label to its corresponding class name\n",
    "        label_int = row['Label']\n",
    "        label_name = self.label_to_class_name.get(label_int, None)\n",
    "        if label_name is None or label_name not in self.class_to_idx:\n",
    "            raise KeyError(f\"Label '{label_int}' not found in label_to_class_name mapping or class_to_idx dictionary.\")\n",
    "        label = self.class_to_idx[label_name]\n",
    "\n",
    "        return images, torch.tensor(csv_row, dtype=torch.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class names to indices\n",
    "classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "class_to_idx = {\n",
    "    'Healthy': 0,\n",
    "    'Grapholita molesta': 1,\n",
    "    'Anarsia lineatella': 2,\n",
    "    'Dead Tree': 3\n",
    "}\n",
    "\n",
    "# Assuming the Label column contains integers (0, 1, 2, 3)\n",
    "label_to_class = {0: 'Healthy', 1: 'Grapholita molesta', 2: 'Anarsia lineatella', 3: 'Dead Tree'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the datasets\n",
    "train_dataset_inception = CustomMultimodalDataset(train_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n",
    "val_dataset_inception = CustomMultimodalDataset(val_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n",
    "test_dataset_inception = CustomMultimodalDataset(test_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "train_loader_inception = DataLoader(train_dataset_inception, batch_size=64, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_dataset_inception, batch_size=64, shuffle=True)\n",
    "test_loader_inception = DataLoader(test_dataset_inception, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for other models\n",
    "train_dataset_others = CustomMultimodalDataset(train_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['train'])\n",
    "val_dataset_others = CustomMultimodalDataset(val_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['val'])\n",
    "test_dataset_others = CustomMultimodalDataset(test_df, image_columns, feature_columns, class_to_idx, transform=data_transforms['Others']['test'])\n",
    "\n",
    "train_loader_others = DataLoader(train_dataset_others, batch_size=64, shuffle=True)\n",
    "val_loader_others = DataLoader(val_dataset_others, batch_size=64, shuffle=True)\n",
    "test_loader_others = DataLoader(test_dataset_others, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modalities Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep MLP CSV feature extractor\n",
    "class MLPCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=10):\n",
    "        super(MLPCSVFeatureExtractor, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.extractor = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:  # Ensure x is at least 2D\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != self.extractor[0].in_features:\n",
    "            raise ValueError(f\"Expected input with {self.extractor[0].in_features} features, but got {x.size(1)}\")\n",
    "        return self.extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional CSV feature extractor\n",
    "class ConvCSVFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, num_layers=2):\n",
    "        super(ConvCSVFeatureExtractor, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        layers = [nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, padding=1), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the size of the output after the convolutions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, seq_len).unsqueeze(1)  # (batch_size=1, channels=1, seq_len)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            conv_output_size = dummy_output.view(1, -1).size(1)  # Flatten the output\n",
    "\n",
    "        # Initialize the fully connected layer with the correct input size\n",
    "        self.fc = nn.Linear(conv_output_size, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Adding channel dimension for Conv1D\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        # print(f'Shape before FC layer: {x.shape}')  # Print the shape for debugging\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fusion methods for combining image and CSV features\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, model_name, image_feature_extractor, csv_input_dim, csv_hidden_dim, num_classes, fusion_method, csv_model_type='simple', seq_len=None):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        self.csv_input_dim = csv_input_dim\n",
    "        self.csv_hidden_dim = csv_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_method = fusion_method\n",
    "\n",
    "        self.image_feature_extractor, self.feature_size = self.initialize_image_feature_extractor(model_name, image_feature_extractor)\n",
    "\n",
    "        if csv_model_type == 'small':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=2)\n",
    "        elif csv_model_type == 'simple':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=50)\n",
    "        elif csv_model_type == 'deep':\n",
    "            self.csv_feature_extractor = MLPCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, num_layers=100)\n",
    "        elif csv_model_type == 'conv_small':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=2)\n",
    "        elif csv_model_type == 'conv_simple':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=50)\n",
    "        elif csv_model_type == 'conv_deep':\n",
    "            if seq_len is None:\n",
    "                raise ValueError(\"seq_len must be provided for the convolutional model\")\n",
    "            self.csv_feature_extractor = ConvCSVFeatureExtractor(self.csv_input_dim, self.csv_hidden_dim, seq_len, num_layers=100)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported csv_model_type\")\n",
    "\n",
    "        if self.fusion_method == 'late':\n",
    "            self.fusion_layer = nn.Linear(self.feature_size + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'intermediate':\n",
    "            self.intermediate_layer = nn.Linear(self.feature_size, 512)\n",
    "            self.fusion_layer = nn.Linear(512 + self.csv_hidden_dim, self.num_classes)\n",
    "        elif self.fusion_method == 'early':\n",
    "            early_fusion_size = self.feature_size + self.csv_hidden_dim\n",
    "            self.early_fusion_layer = nn.Linear(early_fusion_size, self.feature_size)\n",
    "            self.classifier = nn.Linear(self.feature_size, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported fusion method\")\n",
    "\n",
    "    def initialize_image_feature_extractor(self, model_name, image_feature_extractor):\n",
    "        if model_name == 'InceptionV3':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.aux_logits = False\n",
    "            image_feature_extractor.AuxLogits = None\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'ResNet152' or model_name == 'AttentionAugmentedResNet18':\n",
    "            feature_size = image_feature_extractor.fc.in_features\n",
    "            image_feature_extractor.fc = nn.Identity()\n",
    "        elif model_name == 'VGG19' or model_name == 'AttentionAugmentedVGG19':\n",
    "            feature_size = image_feature_extractor.classifier[6].in_features\n",
    "            image_feature_extractor.classifier[6] = nn.Identity()\n",
    "        elif model_name == 'ViT':\n",
    "            if hasattr(image_feature_extractor, 'heads'):\n",
    "                feature_size = image_feature_extractor.heads.head.in_features\n",
    "                image_feature_extractor.heads.head = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'classifier'):\n",
    "                feature_size = image_feature_extractor.classifier.in_features\n",
    "                image_feature_extractor.classifier = nn.Identity()\n",
    "            elif hasattr(image_feature_extractor, 'head'):\n",
    "                feature_size = image_feature_extractor.head.in_features\n",
    "                image_feature_extractor.head = nn.Identity()\n",
    "            else:\n",
    "                for attr_name in dir(image_feature_extractor):\n",
    "                    attr = getattr(image_feature_extractor, attr_name)\n",
    "                    if isinstance(attr, nn.Linear):\n",
    "                        feature_size = attr.in_features\n",
    "                        setattr(image_feature_extractor, attr_name, nn.Identity())\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported ViT model structure for model: {model_name}\")\n",
    "        elif model_name == 'AttentionAugmentedInceptionV3':\n",
    "            inception_model = image_feature_extractor.inception\n",
    "            feature_size = inception_model.fc.in_features\n",
    "            inception_model.aux_logits = False\n",
    "            inception_model.AuxLogits = None\n",
    "            inception_model.fc = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedVGG19':\n",
    "            vgg_model = image_feature_extractor.features\n",
    "            feature_size = vgg_model[-2].out_channels\n",
    "            vgg_model[-1] = nn.Identity()\n",
    "        elif model_name == 'AttentionAugmentedResNet18':\n",
    "            resnet_model = image_feature_extractor\n",
    "            feature_size = resnet_model.fc.in_features\n",
    "            resnet_model.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "        return image_feature_extractor, feature_size\n",
    "\n",
    "    def forward(self, img, csv):\n",
    "        csv_features = self.csv_feature_extractor(csv)\n",
    "\n",
    "        # Ensure csv_features has the correct shape\n",
    "        if len(csv_features.shape) == 1:\n",
    "            csv_features = csv_features.unsqueeze(0)\n",
    "\n",
    "        if self.fusion_method == 'early':\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img_csv_combined = torch.cat((img, csv_features), dim=1)\n",
    "            img_csv_features = self.early_fusion_layer(img_csv_combined)\n",
    "            output = self.classifier(img_csv_features)\n",
    "        else:\n",
    "            img_features = self.image_feature_extractor(img)\n",
    "            if self.model_name == 'AttentionAugmentedInceptionV3':\n",
    "                if isinstance(img_features, tuple):\n",
    "                    img_features = img_features.logits\n",
    "                    img_features = img_features.view(img_features.size(0), -1)\n",
    "\n",
    "            if self.fusion_method == 'late':\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            elif self.fusion_method == 'intermediate':\n",
    "                img_features = self.intermediate_layer(img_features)\n",
    "                combined_features = torch.cat((img_features, csv_features), dim=1)\n",
    "                output = self.fusion_layer(combined_features)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported fusion method\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache function\n",
    "def clear_cache():\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        torch.cache.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient accumulation\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs_img, inputs_csv, labels = data\n",
    "                inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "                outputs = model(inputs_img, inputs_csv)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= early_stopping_patience:\n",
    "        #         print(\"Early stopping due to no improvement in validation loss.\")\n",
    "        #         break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_fusion_model(model, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method='late', num_epochs=40, initial_lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    return train_model(model, criterion, optimizer, train_loader, val_loader, num_classes, csv_input_dim, device, fusion_method=fusion_method, num_epochs=num_epochs, initial_lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fusion_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_confidences = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs_img, inputs_csv, labels = data\n",
    "            inputs_img, inputs_csv, labels = inputs_img.to(device), inputs_csv.to(device), labels.to(device)\n",
    "            outputs = model(inputs_img, inputs_csv)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate confidence level\n",
    "            softmax_outputs = F.softmax(outputs, dim=1)\n",
    "            confidence, predicted = torch.max(softmax_outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_confidences.extend(confidence.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    return test_loss, test_accuracy, all_confidences, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "num_classes_inception = len(class_to_idx)\n",
    "num_classes_others = len(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features in the CSV data\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv input dimensions\n",
    "csv_input_dim = csv_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the csv hidden dimensions\n",
    "csv_hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the results dictionary\n",
    "crop_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = len(feature_columns)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results folder\n",
    "results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/RESULTS/Multimodal\"\n",
    "results_folder = os.path.join(results_base_dir,'Peach', 'CNN+BB', 'T1+NoES+BS064')\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Fusion Method: intermediate --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- CSV Model Type: simple ----------------\n",
      "Training InceptionV3 with intermediate fusion and CSV extractor simple\n",
      "Epoch 1/40, Train Loss: 1.0456, Train Accuracy: 61.46%, Val Loss: 1.2148, Val Accuracy: 65.62%\n",
      "Epoch 2/40, Train Loss: 0.4324, Train Accuracy: 87.50%, Val Loss: 1.0962, Val Accuracy: 75.00%\n",
      "Epoch 3/40, Train Loss: 0.1734, Train Accuracy: 94.79%, Val Loss: 1.8457, Val Accuracy: 78.12%\n"
     ]
    }
   ],
   "source": [
    "with open(results_folder + 'train_val_test_results.txt', 'w') as f:\n",
    "    for fusion_method in ['intermediate', 'late']:\n",
    "        output_line = f'-------------------------- Fusion Method: {fusion_method} --------------------------'\n",
    "        print(output_line)\n",
    "        f.write(output_line + '\\n')\n",
    "\n",
    "        for csv_model_type in ['simple', 'deep', 'conv_simple', 'conv_deep']:\n",
    "            cnn_feature_extractors = {\n",
    "                'InceptionV3': models.inception_v3(pretrained=True).to(device),\n",
    "                'ResNet152': models.resnet152(pretrained=True).to(device),\n",
    "                'VGG19': models.vgg19(pretrained=True).to(device),\n",
    "                'ViT': ViT(\n",
    "                    image_size=224,\n",
    "                    patch_size=16,\n",
    "                    num_classes=num_classes_others,\n",
    "                    dim=256,\n",
    "                    depth=6,\n",
    "                    heads=24,\n",
    "                    mlp_dim=2048,\n",
    "                    dropout=0.1,\n",
    "                    emb_dropout=0.1\n",
    "                ).to(device),\n",
    "                \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n",
    "                # 'AttentionAugmentedVGG19': attention_augmented_vgg('VGG19', num_classes=num_classes_others).to(device),\n",
    "                \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n",
    "            }\n",
    "\n",
    "            if 'InceptionV3' in cnn_feature_extractors:\n",
    "                cnn_feature_extractors['InceptionV3'].aux_logits = False\n",
    "\n",
    "            output_line = f'---------------- CSV Model Type: {csv_model_type} ----------------'\n",
    "            print(output_line)\n",
    "            f.write(output_line + '\\n')\n",
    "\n",
    "            for model_name, image_feature_extractor in cnn_feature_extractors.items():\n",
    "                image_feature_extractor.to(device)\n",
    "\n",
    "                output_line = f'Training {model_name} with {fusion_method} fusion and CSV extractor {csv_model_type}'\n",
    "                print(output_line)\n",
    "                f.write(output_line + '\\n')\n",
    "\n",
    "                fusion_model = FusionModel(\n",
    "                    model_name=model_name,\n",
    "                    image_feature_extractor=image_feature_extractor,\n",
    "                    csv_input_dim=csv_input_dim,\n",
    "                    csv_hidden_dim=csv_hidden_dim,\n",
    "                    num_classes=num_classes_others,\n",
    "                    fusion_method=fusion_method,\n",
    "                    csv_model_type=csv_model_type,\n",
    "                    seq_len=seq_len\n",
    "                ).to(device)\n",
    "\n",
    "                model = create_and_train_fusion_model(\n",
    "                    fusion_model,\n",
    "                    train_loader_others,\n",
    "                    val_loader_others,\n",
    "                    num_classes_others,\n",
    "                    csv_input_dim,\n",
    "                    device,\n",
    "                    fusion_method,\n",
    "                    initial_lr=0.001\n",
    "                ).to(device)\n",
    "\n",
    "                test_loss, test_accuracy, all_confidences, all_predictions, all_labels = evaluate_fusion_model(model, test_loader_others, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "                crop_results[f\"{model_name}_{fusion_method}_{csv_model_type}\"] = {\n",
    "                    'model': model,\n",
    "                    'model_name': model_name,\n",
    "                    'fusion_method': fusion_method,\n",
    "                    'csv_model_type': csv_model_type,\n",
    "                    'test_loss': test_loss,\n",
    "                    'test_accuracy': test_accuracy\n",
    "                }\n",
    "\n",
    "                output_line = f'{model_name} with {fusion_method} fusion and CSV extractor {csv_model_type} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%'\n",
    "                print(output_line)\n",
    "                f.write(output_line + '\\n')\n",
    "\n",
    "                del model\n",
    "                clear_cache()\n",
    "\n",
    "                print('\\n')\n",
    "                f.write('\\n')\n",
    "\n",
    "            print('\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        f.write('----------------------------------------------------------------------------------------------------------------------\\n')\n",
    "        print('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save figures\n",
    "def save_figure(fig, filename):\n",
    "    fig.savefig(os.path.join(results_folder, filename))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "def plot_accuracy_comparison(results):\n",
    "    accuracies = [result['test_accuracy'] for result in results.values()]\n",
    "    model_names = list(results.keys())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=90)  # Make x-axis labels diagonal\n",
    "    plt.show()\n",
    "    save_figure(fig, 'all_fusion_accuracy_comparison.png')\n",
    "\n",
    "    for fusion_method in ['late', 'intermediate']:\n",
    "        # Filter models based on the fusion method\n",
    "        filtered_results = {model: details for model, details in results.items() if details['fusion_method'] == fusion_method}\n",
    "\n",
    "        if filtered_results:  # Check if there are models with this fusion method\n",
    "            accuracies = [details['test_accuracy'] for details in filtered_results.values()]\n",
    "            model_names = list(filtered_results.keys())\n",
    "\n",
    "            fig = plt.figure(figsize=(20, 10))\n",
    "            plt.bar(model_names, accuracies)\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.xlabel('Model')\n",
    "            plt.xticks(rotation=90)  # Make x-axis labels diagonal\n",
    "            plt.title(f'Accuracy Comparison for {fusion_method.capitalize()} Fusion')\n",
    "            plt.show()\n",
    "            save_figure(fig, f'{fusion_method}_accuracy_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "plot_accuracy_comparison(crop_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader_inception, test_loader_others):\n",
    "    metrics_data = []\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images, csv_features)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "\n",
    "        metrics_data.append({\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)\n",
    "    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, model_name, num_images=5):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    class_labels = list(test_loader.dataset.class_to_idx.keys())\n",
    "\n",
    "    # Get a batch of images, CSV inputs, and labels\n",
    "    batch = next(iter(test_loader))\n",
    "    images, csv_inputs, labels = batch\n",
    "\n",
    "    images, csv_inputs, labels = images[:num_images].to(device), csv_inputs[:num_images].to(device), labels[:num_images]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, csv_inputs)\n",
    "        softmax_outputs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(softmax_outputs, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"True: {class_labels[labels[i]]}\\nPred: {class_labels[predicted[i].cpu()]}\\nConf: {confidence[i].cpu():.2f}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_classification_results_with_confidence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "    \n",
    "    print(f'Displaying results for {model_name}')\n",
    "    display_classification_results(crop_results[model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader, model_name):\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch in test_loader:\n",
    "        images, csv_features, labels = batch\n",
    "        images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, csv_features)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Ensure target names are in the correct order\n",
    "    target_names = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "    labels_list = [class_to_idx[name] for name in target_names]  # Get indices in the specified order\n",
    "\n",
    "    print(f'Classification Report for {model_name}:')\n",
    "    report = classification_report(all_labels, all_predicted, labels=labels_list, target_names=target_names)\n",
    "    print(report)\n",
    "\n",
    "    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n",
    "    with open(report_filename, 'w') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for model_name in crop_results.keys():\n",
    "    if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3']:\n",
    "        test_loader = test_loader_inception\n",
    "    else:\n",
    "        test_loader = test_loader_others\n",
    "        \n",
    "    print(f'Displaying classification report for {model_name}')\n",
    "    display_classification_report(crop_results[model_name]['model'], test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Plotting\n",
    "def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n",
    "    # Ensure the classes are in the correct order\n",
    "    classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels, labels=[class_to_idx[cls] for cls in classes])\n",
    "    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n",
    "    fig.delaxes(fig.axes[1])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Predicted Label', fontsize=50)\n",
    "    plt.ylabel('True Label', fontsize=50)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_confusion_matrix.png')\n",
    "\n",
    "def get_all_labels_and_preds(model, test_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, csv_features, labels = data\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images, csv_features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "            all_preds.extend(preds.cpu().numpy().astype(int))\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def generate_confusion_matrices(results, test_loader_inception, test_loader_others):\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        classes = ['Healthy', 'Grapholita molesta', 'Anarsia lineatella', 'Dead Tree']\n",
    "        model = model_info['model']\n",
    "        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n",
    "        plot_confusion_matrix(labels, pred_labels, classes, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrices(crop_results, test_loader_inception, test_loader_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize images\n",
    "def normalize_image(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the most incorrect predictions\n",
    "def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n",
    "    rows = int(np.ceil(np.sqrt(n_images)))\n",
    "    cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 20))\n",
    "\n",
    "    for i in range(rows * cols):\n",
    "        if i >= len(incorrect):\n",
    "            break\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        image, true_label, probs = incorrect[i]\n",
    "        image = image.permute(1, 2, 0)\n",
    "        true_prob = probs[true_label]\n",
    "        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n",
    "        true_class = classes[true_label]\n",
    "        incorrect_class = classes[incorrect_label]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.cpu().numpy())\n",
    "        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n",
    "                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "\n",
    "    plt.show()\n",
    "    save_figure(fig, f'{model_name}_most_incorrect.png')\n",
    "\n",
    "\n",
    "def get_all_details(model, test_loader):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, csv_features, labels in test_loader:\n",
    "            images, csv_features, labels = images.to(device), csv_features.to(device), labels.to(device)\n",
    "            outputs = model(images, csv_features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu())\n",
    "\n",
    "    return all_images, all_labels, all_preds, all_probs\n",
    "\n",
    "def plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36):\n",
    "    classes = list(test_loader_inception.dataset.class_to_idx.keys())\n",
    "    for model_name, model_info in results.items():\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "\n",
    "        model = model_info['model']\n",
    "        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n",
    "        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n",
    "        incorrect_examples = []\n",
    "\n",
    "        for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
    "            if not correct:\n",
    "                incorrect_examples.append((image, label, prob))\n",
    "\n",
    "        incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n",
    "        plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_incorrect_predictions(crop_results, test_loader_inception, test_loader_others, n_images=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all results\n",
    "def generate_all_results(results, test_loader_inception, test_loader_others):\n",
    "    plot_accuracy_comparison(results)\n",
    "    display_model_metrics_table(results, test_loader_inception, test_loader_others)\n",
    "    generate_confusion_matrices(results, test_loader_inception, test_loader_others)\n",
    "    plot_most_incorrect_predictions(results, test_loader_inception, test_loader_others, n_images=36)\n",
    "\n",
    "    for model_name, model_info in results.items():\n",
    "        model = model_info['model']\n",
    "        test_loader = test_loader_inception if model_name in ['InceptionV3', 'AttentionAugmentedInceptionV3'] else test_loader_others\n",
    "        display_classification_results(model, test_loader, model_name)\n",
    "        display_classification_report(model, test_loader, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the generate_all_results function\n",
    "generate_all_results(crop_results, test_loader_inception, test_loader_others)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
