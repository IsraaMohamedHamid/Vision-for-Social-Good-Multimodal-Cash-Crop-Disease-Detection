{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7048575,"sourceType":"datasetVersion","datasetId":4056201}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\n\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS, GPSTAGS\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import InceptionV3, ResNet152, VGG19\nfrom tensorflow.keras.utils import to_categorical, Sequence","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List all physical GPUs available\nphysical_devices = tf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(physical_devices))\n\n# Print details about the detected GPUs\nfor gpu in physical_devices:\n    print(\"GPU details:\", gpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Enable device placement logging\ntf.debugging.set_log_device_placement(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '/kaggle/input/multimodal-plant-disease-dataset-by-subham-divakar'\ncrop_root = os.path.join(base_dir, 'color')\nsplit_root = os.path.join(base_dir, 'split')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_ds_store(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file == '.DS_Store' or '.DS_Store' in file:\n                file_path = os.path.join(root, file)\n                print(f\"Removing {file_path}\")\n                os.remove(file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_ds_store(base_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_image_file(filename):\n    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(base_dir, val_split=0.4, test_split=0.1):\n    train_files = []\n    val_files = []\n    test_files = []\n\n    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n    for cls in classes:\n        print(f'Processing class: {cls}')\n        class_dir = os.path.join(base_dir, cls)\n\n        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n\n        if len(images) == 0:\n            print(f\"No images found for class {cls}. Skipping...\")\n            continue\n\n        random.shuffle(images)\n\n        try:\n            train, test = train_test_split(images, test_size=test_split)\n            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n        except ValueError as e:\n            print(f\"Not enough images to split for class {cls}: {e}\")\n            continue\n\n        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n\n    return train_files, val_files, test_files, classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_files, val_files, test_files, classes = split_data(crop_root)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train files: {len(train_files)}\")\nprint(f\"Validation files: {len(val_files)}\")\nprint(f\"Test files: {len(test_files)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inception_size = 299\nother_size = 224","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_gen_args = dict(rescale=1./255,\n                     shear_range=0.2,\n                     zoom_range=0.2,\n                     horizontal_flip=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    'InceptionV3': ImageDataGenerator(**data_gen_args),\n    'Others': ImageDataGenerator(**data_gen_args),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Sequence):\n    def __init__(self, file_paths, class_to_idx, batch_size=32, image_size=(224, 224), shuffle=True):\n        self.file_paths = file_paths\n        self.class_to_idx = class_to_idx\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.file_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_paths = self.file_paths[index * self.batch_size:(index + 1) * self.batch_size]\n        images = []\n        labels = []\n        for img_path, cls in batch_paths:\n            image = Image.open(img_path).resize(self.image_size)\n            image = np.array(image) / 255.0\n            label = self.class_to_idx[cls]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), to_categorical(np.array(labels), num_classes=len(self.class_to_idx))\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.file_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_to_idx = {cls: idx for idx, cls in enumerate(classes)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_inception = CustomDataset(train_files, class_to_idx, image_size=(inception_size, inception_size))\nval_dataset_inception = CustomDataset(val_files, class_to_idx, image_size=(inception_size, inception_size))\ntest_dataset_inception = CustomDataset(test_files, class_to_idx, image_size=(inception_size, inception_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_others = CustomDataset(train_files, class_to_idx, image_size=(other_size, other_size))\nval_dataset_others = CustomDataset(val_files, class_to_idx, image_size=(other_size, other_size))\ntest_dataset_others = CustomDataset(test_files, class_to_idx, image_size=(other_size, other_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(base_model, num_classes):\n    x = base_model.output\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(1024, activation='relu')(x)\n    predictions = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs=base_model.input, outputs=predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataset, val_dataset, epochs=40, initial_lr=0.001):\n    model.compile(optimizer=optimizers.Adam(lr=initial_lr),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[early_stopping])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, test_dataset):\n    results = model.evaluate(test_dataset)\n    test_loss, test_accuracy = results[0], results[1] * 100\n    return test_loss, test_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crops = ['Cherry']\nresults = {}\n\nfor crop in crops:\n    train_dataset_inception = CustomDataset(train_files, class_to_idx, image_size=(inception_size, inception_size))\n    val_dataset_inception = CustomDataset(val_files, class_to_idx, image_size=(inception_size, inception_size))\n    test_dataset_inception = CustomDataset(test_files, class_to_idx, image_size=(inception_size, inception_size))\n\n    train_dataset_others = CustomDataset(train_files, class_to_idx, image_size=(other_size, other_size))\n    val_dataset_others = CustomDataset(val_files, class_to_idx, image_size=(other_size, other_size))\n    test_dataset_others = CustomDataset(test_files, class_to_idx, image_size=(other_size, other_size))\n\n    num_classes = len(class_to_idx)\n\n    pretrained_models = {\n        'InceptionV3': InceptionV3(weights='imagenet', include_top=False, input_shape=(inception_size, inception_size, 3)),\n        'ResNet152': ResNet152(weights='imagenet', include_top=False, input_shape=(other_size, other_size, 3)),\n        'VGG19': VGG19(weights='imagenet', include_top=False, input_shape=(other_size, other_size, 3)),\n    }\n\n    crop_results = {}\n\n    for model_name, base_model in pretrained_models.items():\n        model = create_model(base_model, num_classes)\n        \n        if model_name == 'InceptionV3':\n            train_dataset = train_dataset_inception\n            val_dataset = val_dataset_inception\n            test_dataset = test_dataset_inception\n        else:\n            train_dataset = train_dataset_others\n            val_dataset = val_dataset_others\n            test_dataset = test_dataset_others\n\n        print(f'--------------- Training model: {model_name}')\n        train_model(model, train_dataset, val_dataset)\n\n        test_loss, test_accuracy = evaluate_model(model, test_dataset)\n\n        crop_results[model_name] = {\n            'model': model,\n            'test_loss': test_loss,\n            'test_accuracy': test_accuracy\n        }\n        print(f'{crop} - {model_name} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n        print(f'\\n')\n\n    results[crop] = crop_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_model_metrics_table(results, test_dataset):\n    metrics_data = []\n\n    for crop, crop_results in results.items():\n        for model_name, model_info in crop_results.items():\n            model = model_info['model']\n            all_labels = []\n            all_predicted = []\n\n            for images, labels in test_dataset:\n                outputs = model.predict(images)\n                predicted = np.argmax(outputs, axis=1)\n                all_labels.extend(np.argmax(labels, axis=1))\n                all_predicted.extend(predicted)\n\n            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n            \n            metrics_data.append({\n                'Crop': crop,\n                'Model': model_name,\n                'Precision': precision,\n                'Recall': recall,\n                'F1-score': f1\n            })\n\n    metrics_df = pd.DataFrame(metrics_data)\n    display(metrics_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_classification_report(model, test_dataset):\n    all_labels = []\n    all_predicted = []\n\n    for images, labels in test_dataset:\n        outputs = model.predict(images)\n        predicted = np.argmax(outputs, axis=1)\n        all_labels.extend(np.argmax(labels, axis=1))\n        all_predicted.extend(predicted)\n\n    report = classification_report(all_labels, all_predicted, target_names=[str(i) for i in range(test_dataset.num_classes)])\n    print(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_classification_results(model, test_dataset, num_images=5):\n    images, labels = next(iter(test_dataset))\n    outputs = model.predict(images[:num_images])\n    predicted = np.argmax(outputs, axis=1)\n    \n    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n    fig.suptitle('Classification Results', fontsize=16)\n    \n    for i in range(num_images):\n        ax = axes[i]\n        img = images[i]\n        img = np.clip(img, 0, 1)\n        ax.imshow(img)\n        ax.set_title(f'True: {np.argmax(labels[i])}\\n Pred: {predicted[i]}')\n        ax.axis('off')\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_model_metrics_table(results, test_dataset_inception)\nfor crop, crop_results in results.items():\n    for model_name in crop_results.keys():\n        print(f'Displaying results for {crop} - {model_name}')\n        display_classification_results(crop_results[model_name]['model'], test_dataset_inception)\n        print(f'Displaying classification report for {crop} - {model_name}')\n        display_classification_report(crop_results[model_name]['model'], test_dataset_inception)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}