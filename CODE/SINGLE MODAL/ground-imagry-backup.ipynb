{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:08:59.593560Z","iopub.status.busy":"2024-07-27T13:08:59.593261Z","iopub.status.idle":"2024-07-27T13:09:05.625131Z","shell.execute_reply":"2024-07-27T13:09:05.624160Z","shell.execute_reply.started":"2024-07-27T13:08:59.593532Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n","\n","from efficientnet_pytorch import EfficientNet\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, classification_report\n","\n","from PIL import Image\n","from PIL.ExifTags import TAGS, GPSTAGS\n","\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import datasets, models, transforms\n","from torchvision.datasets.folder import is_image_file\n","# Ensure these imports are included\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.optim.lr_scheduler import StepLR\n","\n","from vit_pytorch import ViT\n","\n","from AACN_Model import attention_augmented_resnet18, attention_augmented_efficientnetb0, attention_augmented_inceptionv3, attention_augmented_vit, attention_augmented_vgg"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.627047Z","iopub.status.busy":"2024-07-27T13:09:05.626426Z","iopub.status.idle":"2024-07-27T13:09:05.633414Z","shell.execute_reply":"2024-07-27T13:09:05.632171Z","shell.execute_reply.started":"2024-07-27T13:09:05.627014Z"},"trusted":true},"outputs":[],"source":["# Define main directories\n","base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/Data' #'/Users/izzymohamed/Downloads/shubham10divakar Multimodal-Plant-Disease-Dataset/Data'\n","crop_root = os.path.join(base_dir, 'color')\n","split_root = os.path.join(base_dir, 'split')"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.636232Z","iopub.status.busy":"2024-07-27T13:09:05.635874Z","iopub.status.idle":"2024-07-27T13:09:05.644500Z","shell.execute_reply":"2024-07-27T13:09:05.643479Z","shell.execute_reply.started":"2024-07-27T13:09:05.636202Z"},"trusted":true},"outputs":[],"source":["# Define function to remove .DS_Store files\n","def remove_ds_store(directory):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file == '.DS_Store' or '.DS_Store' in file:\n","                file_path = os.path.join(root, file)\n","                print(f\"Removing {file_path}\")\n","                os.remove(file_path)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.646082Z","iopub.status.busy":"2024-07-27T13:09:05.645687Z","iopub.status.idle":"2024-07-27T13:10:14.235460Z","shell.execute_reply":"2024-07-27T13:10:14.234651Z","shell.execute_reply.started":"2024-07-27T13:09:05.646050Z"},"trusted":true},"outputs":[],"source":["# Remove .DS_Store files from base directory\n","remove_ds_store(base_dir)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.236865Z","iopub.status.busy":"2024-07-27T13:10:14.236579Z","iopub.status.idle":"2024-07-27T13:10:14.241736Z","shell.execute_reply":"2024-07-27T13:10:14.240691Z","shell.execute_reply.started":"2024-07-27T13:10:14.236840Z"},"trusted":true},"outputs":[],"source":["def is_image_file(filename):\n","    # Assuming is_image_file is a function that checks if the file is an image\n","    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.243241Z","iopub.status.busy":"2024-07-27T13:10:14.242978Z","iopub.status.idle":"2024-07-27T13:10:14.259354Z","shell.execute_reply":"2024-07-27T13:10:14.258631Z","shell.execute_reply.started":"2024-07-27T13:10:14.243219Z"},"trusted":true},"outputs":[],"source":["# Function to split data into train, validation, and test sets\n","def split_data(base_dir, val_split=0.4, test_split=0.1):\n","    train_files = []\n","    val_files = []\n","    test_files = []\n","\n","    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n","    for cls in classes:\n","        print(f'Processing class: {cls}')\n","        class_dir = os.path.join(base_dir, cls)\n","\n","        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n","\n","        if len(images) == 0:\n","            print(f\"No images found for class {cls}. Skipping...\")\n","            continue\n","\n","        # Shuffle images to randomize the selection\n","        random.shuffle(images)\n","\n","        try:\n","            train, test = train_test_split(images, test_size=test_split)\n","            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n","        except ValueError as e:\n","            print(f\"Not enough images to split for class {cls}: {e}\")\n","            continue\n","\n","        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n","        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n","        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n","\n","    return train_files, val_files, test_files, classes"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.260981Z","iopub.status.busy":"2024-07-27T13:10:14.260493Z","iopub.status.idle":"2024-07-27T13:10:14.622675Z","shell.execute_reply":"2024-07-27T13:10:14.621706Z","shell.execute_reply.started":"2024-07-27T13:10:14.260957Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing class: Strawberry___healthy\n","Processing class: Grape___Black_rot\n","Processing class: Potato___Early_blight\n","Processing class: Blueberry___healthy\n","Processing class: Corn_(maize)___healthy\n","Processing class: Tomato___Target_Spot\n","Processing class: Peach___healthy\n","Processing class: Potato___Late_blight\n","Processing class: Tomato___Late_blight\n","Processing class: Tomato___Tomato_mosaic_virus\n","Processing class: Pepper,_bell___healthy\n","Processing class: Orange___Haunglongbing_(Citrus_greening)\n","Processing class: Tomato___Leaf_Mold\n","Processing class: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n","Processing class: Cherry_(including_sour)___Powdery_mildew\n","Processing class: Apple___Cedar_apple_rust\n","Processing class: Tomato___Bacterial_spot\n","Processing class: Grape___healthy\n","Processing class: Tomato___Early_blight\n","Processing class: Corn_(maize)___Common_rust_\n","Processing class: Grape___Esca_(Black_Measles)\n","Processing class: Raspberry___healthy\n","Processing class: Tomato___healthy\n","Processing class: Cherry_(including_sour)___healthy\n","Processing class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n","Processing class: Apple___Apple_scab\n","Processing class: Corn_(maize)___Northern_Leaf_Blight\n","Processing class: Tomato___Spider_mites Two-spotted_spider_mite\n","Processing class: Peach___Bacterial_spot\n","Processing class: Pepper,_bell___Bacterial_spot\n","Processing class: Tomato___Septoria_leaf_spot\n","Processing class: Squash___Powdery_mildew\n","Processing class: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n","Processing class: Apple___Black_rot\n","Processing class: Apple___healthy\n","Processing class: Strawberry___Leaf_scorch\n","Processing class: Potato___healthy\n","Processing class: Soybean___healthy\n"]}],"source":["# Split data\n","train_files, val_files, test_files, classes = split_data(crop_root)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.624109Z","iopub.status.busy":"2024-07-27T13:10:14.623815Z","iopub.status.idle":"2024-07-27T13:10:14.628646Z","shell.execute_reply":"2024-07-27T13:10:14.627718Z","shell.execute_reply.started":"2024-07-27T13:10:14.624085Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train files: 27124\n","Validation files: 21728\n","Test files: 5447\n"]}],"source":["# Use the lists of file paths for your dataset loading and transformations\n","print(f\"Train files: {len(train_files)}\")\n","print(f\"Validation files: {len(val_files)}\")\n","print(f\"Test files: {len(test_files)}\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.632417Z","iopub.status.busy":"2024-07-27T13:10:14.632130Z","iopub.status.idle":"2024-07-27T13:10:14.638617Z","shell.execute_reply":"2024-07-27T13:10:14.637715Z","shell.execute_reply.started":"2024-07-27T13:10:14.632389Z"},"trusted":true},"outputs":[],"source":["# Define the standard image sizes\n","inception_size = 299\n","other_size = 224"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.639961Z","iopub.status.busy":"2024-07-27T13:10:14.639683Z","iopub.status.idle":"2024-07-27T13:10:14.650118Z","shell.execute_reply":"2024-07-27T13:10:14.649245Z","shell.execute_reply.started":"2024-07-27T13:10:14.639938Z"},"trusted":true},"outputs":[],"source":["# Update the data transformations\n","data_transforms = {\n","    'InceptionV3': {\n","        'train': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'test': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","    },\n","    'Others': {\n","        'train': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'test': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","    }\n","}"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.652122Z","iopub.status.busy":"2024-07-27T13:10:14.651165Z","iopub.status.idle":"2024-07-27T13:10:14.664066Z","shell.execute_reply":"2024-07-27T13:10:14.663232Z","shell.execute_reply.started":"2024-07-27T13:10:14.652097Z"},"trusted":true},"outputs":[],"source":["# Create a custom dataset class to load images from the file lists\n","class CustomDataset(Dataset):\n","    def __init__(self, file_paths, class_to_idx, transform=None):\n","        self.file_paths = file_paths\n","        self.class_to_idx = class_to_idx\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path, cls = self.file_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.class_to_idx[cls]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.665307Z","iopub.status.busy":"2024-07-27T13:10:14.665078Z","iopub.status.idle":"2024-07-27T13:10:14.677497Z","shell.execute_reply":"2024-07-27T13:10:14.676790Z","shell.execute_reply.started":"2024-07-27T13:10:14.665287Z"},"trusted":true},"outputs":[],"source":["# Create a mapping from class names to indices\n","class_to_idx = {cls: idx for idx, cls in enumerate(classes)}"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.679006Z","iopub.status.busy":"2024-07-27T13:10:14.678657Z","iopub.status.idle":"2024-07-27T13:10:14.688325Z","shell.execute_reply":"2024-07-27T13:10:14.687476Z","shell.execute_reply.started":"2024-07-27T13:10:14.678977Z"},"trusted":true},"outputs":[],"source":["# Create datasets and data loaders\n","train_dataset_inception = CustomDataset(train_files, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n","val_dataset_inception = CustomDataset(val_files, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n","test_dataset_inception = CustomDataset(test_files, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n","\n","train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n","val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n","test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.690062Z","iopub.status.busy":"2024-07-27T13:10:14.689483Z","iopub.status.idle":"2024-07-27T13:10:14.698091Z","shell.execute_reply":"2024-07-27T13:10:14.697244Z","shell.execute_reply.started":"2024-07-27T13:10:14.690032Z"},"trusted":true},"outputs":[],"source":["# Loaders for other models\n","train_dataset_others = CustomDataset(train_files, class_to_idx, transform=data_transforms['Others']['train'])\n","val_dataset_others = CustomDataset(val_files, class_to_idx, transform=data_transforms['Others']['val'])\n","test_dataset_others = CustomDataset(test_files, class_to_idx, transform=data_transforms['Others']['test'])\n","\n","train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n","val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n","test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training and Evaluation"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.699828Z","iopub.status.busy":"2024-07-27T13:10:14.699293Z","iopub.status.idle":"2024-07-27T13:10:14.708077Z","shell.execute_reply":"2024-07-27T13:10:14.707202Z","shell.execute_reply.started":"2024-07-27T13:10:14.699798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: mps\n"]}],"source":["# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n","if torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","elif torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"Device: {device}\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.709322Z","iopub.status.busy":"2024-07-27T13:10:14.709092Z","iopub.status.idle":"2024-07-27T13:10:14.721120Z","shell.execute_reply":"2024-07-27T13:10:14.720300Z","shell.execute_reply.started":"2024-07-27T13:10:14.709302Z"},"trusted":true},"outputs":[],"source":["# Function to check if a file is valid\n","def is_valid_file(path):\n","    return not path.endswith('.DS_Store') or 'DS_Store' not in path"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.722909Z","iopub.status.busy":"2024-07-27T13:10:14.722272Z","iopub.status.idle":"2024-07-27T13:10:14.730619Z","shell.execute_reply":"2024-07-27T13:10:14.729938Z","shell.execute_reply.started":"2024-07-27T13:10:14.722862Z"},"trusted":true},"outputs":[],"source":["# Function to adjust learning rate\n","def adjust_learning_rate(optimizer, epoch, learning_rate):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n","    lr = learning_rate * (0.1 ** (epoch // 10))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# Training function with mixed precision and gradient accumulation\n","def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=40, accumulation_steps=4, initial_lr=0.001):\n","    scaler = GradScaler()  # For mixed precision training\n","    early_stopping_patience = 5  # Patience for early stopping\n","    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n","    patience_counter = 0  # Counter for early stopping\n","\n","    model.to(device)  # Move the model to the specified device (CPU or GPU)\n","    scheduler = StepLR(optimizer, step_size=10, gamma=0.7)  # Learning rate scheduler\n","\n","    for epoch in range(num_epochs):\n","        model.train()  # Set the model to training mode\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        \n","        # Zero gradients before each epoch\n","        optimizer.zero_grad()\n","        for i, data in enumerate(train_loader):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the specified device\n","\n","            with autocast():  # Enable mixed precision\n","                outputs = model(inputs)\n","                if isinstance(outputs, tuple):\n","                    outputs = outputs[0]  # Unpack outputs if it's a tuple\n","                loss = criterion(outputs, labels)  # Compute loss\n","\n","            # Backward pass with mixed precision scaling\n","            scaler.scale(loss).backward()\n","\n","            # Update weights and gradients after the specified accumulation steps\n","            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n","                scaler.step(optimizer)  # Update model weights\n","                scaler.update()  # Update scaler for next iteration\n","                optimizer.zero_grad()  # Zero gradients after each optimizer step\n","\n","            running_loss += loss.item()  # Accumulate loss\n","            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n","            total += labels.size(0)  # Total number of samples\n","            correct += (predicted == labels).sum().item()  # Count correct predictions\n","\n","        # Compute average training loss and accuracy\n","        train_loss = running_loss / len(train_loader)\n","        train_accuracy = 100 * correct / total\n","\n","        # Validation\n","        model.eval()  # Set the model to evaluation mode\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():  # Disable gradient calculation for validation\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)  # Move data to the specified device\n","\n","                with autocast():  # Enable mixed precision\n","                    outputs = model(inputs)\n","                    if isinstance(outputs, tuple):\n","                        outputs = outputs[0]  # Unpack outputs if it's a tuple\n","                    loss = criterion(outputs, labels)  # Compute validation loss\n","\n","                val_loss += loss.item()  # Accumulate validation loss\n","                _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n","                total += labels.size(0)  # Total number of samples\n","                correct += (predicted == labels).sum().item()  # Count correct predictions\n","\n","        # Compute average validation loss and accuracy\n","        val_loss /= len(val_loader)\n","        val_accuracy = 100 * correct / total\n","\n","        # Print metrics for the current epoch\n","        print(f'Epoch {epoch + 1}/{num_epochs}, '\n","              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n","              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss  # Update the best validation loss\n","            patience_counter = 0  # Reset patience counter\n","        else:\n","            patience_counter += 1  # Increment patience counter\n","            if patience_counter >= early_stopping_patience:\n","                print(\"Early stopping due to no improvement in validation loss.\")\n","                break  # Exit training loop if no improvement\n","\n","        scheduler.step()  # Update the learning rate\n","\n","    return model  # Return the trained model"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.748331Z","iopub.status.busy":"2024-07-27T13:10:14.747914Z","iopub.status.idle":"2024-07-27T13:10:14.762129Z","shell.execute_reply":"2024-07-27T13:10:14.761247Z","shell.execute_reply.started":"2024-07-27T13:10:14.748307Z"},"trusted":true},"outputs":[],"source":["# Function to create and train the model\n","def create_and_train_model(model, train_loader, val_loader, num_classes, device, num_epochs=40, initial_lr=0.001):\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n","    return train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=num_epochs, initial_lr=initial_lr)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.732101Z","iopub.status.busy":"2024-07-27T13:10:14.731766Z","iopub.status.idle":"2024-07-27T13:10:14.746726Z","shell.execute_reply":"2024-07-27T13:10:14.745935Z","shell.execute_reply.started":"2024-07-27T13:10:14.732072Z"},"trusted":true},"outputs":[],"source":["# Function to evaluate the models\n","def evaluate_model(model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            if isinstance(outputs, tuple):\n","                outputs = outputs[0]\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100 * correct / total\n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["# Clear cache function\n","def clear_cache():\n","    if torch.backends.mps.is_available():\n","        torch.mps.empty_cache()\n","    elif torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    else:\n","        torch.cache.empty_cache()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# Save the best model\n","def save_model(model, model_name, crop):\n","    model_dir = os.path.join(base_dir, 'saved_models')\n","    os.makedirs(model_dir, exist_ok=True)\n","    model_path = os.path.join(model_dir, f'{model_name}.pth')\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved at {model_path}\")"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["# Load the best model\n","def load_model(model, model_name, crop):\n","    model_dir = os.path.join(base_dir, 'saved_models')\n","    model_path = os.path.join(model_dir, f'{model_name}.pth')\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"Model loaded from {model_path}\")\n","    return model"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.778547Z","iopub.status.busy":"2024-07-27T13:10:14.778205Z","iopub.status.idle":"2024-07-27T13:10:14.786445Z","shell.execute_reply":"2024-07-27T13:10:14.785755Z","shell.execute_reply.started":"2024-07-27T13:10:14.778510Z"},"trusted":true},"outputs":[],"source":["# Function to find classes in a directory\n","def find_classes(dir):\n","    if not os.path.exists(dir):\n","        os.makedirs(dir, exist_ok=True)\n","        print(f\"Created directory: {dir}\")\n","    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d)) and not d.startswith('.')]\n","    classes.sort()\n","    class_to_idx = {classes[i]: i for i in range(len(classes))}\n","    return classes, class_to_idx"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Model"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["# Count the number of classes\n","num_classes_inception = len(class_to_idx)\n","num_classes_others = len(class_to_idx)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Set the number of features in the CSV data\n","num_heads = 8"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# Define the results dictionary\n","crop_results = {}"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Define the pretrained models\n","pretrained_models = {\n","    'InceptionV3': models.inception_v3(pretrained=True).to(device),\n","    'ResNet152': models.resnet152(pretrained=True).to(device),\n","    'VGG19': models.vgg19(pretrained=True).to(device),\n","    'ViT': ViT(\n","        image_size=224,     # Increased image size (old: 224)\n","        patch_size=16,      # Patch size remains the same; try smaller if needed (old: 16)\n","        num_classes=num_classes_others,\n","        dim=256,           # Increased model dimensionality (old: 1024)\n","        depth=6,           # Increased depth (old: 6)\n","        heads=24,           # Increased number of attention heads (old: 16)\n","        mlp_dim=2048,       # Increased MLP dimension (old: 2048)\n","        dropout=0.1,\n","        emb_dropout=0.1\n","    ).to(device),\n","    \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n","    'AttentionAugmentedVGG19': attention_augmented_vgg('VGG19',num_classes=num_classes_others).to(device),\n","    \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n","}"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.798371Z","iopub.status.busy":"2024-07-27T13:10:14.797964Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------- Training model: InceptionV3\n","Epoch 1/40, Train Loss: 0.3865, Train Accuracy: 88.76%, Val Loss: 0.1313, Val Accuracy: 96.04%\n","Epoch 2/40, Train Loss: 0.1362, Train Accuracy: 95.71%, Val Loss: 0.0838, Val Accuracy: 97.31%\n","Epoch 3/40, Train Loss: 0.0982, Train Accuracy: 96.86%, Val Loss: 0.0519, Val Accuracy: 98.38%\n","Epoch 4/40, Train Loss: 0.0641, Train Accuracy: 98.04%, Val Loss: 0.0641, Val Accuracy: 97.92%\n","Epoch 5/40, Train Loss: 0.0654, Train Accuracy: 97.83%, Val Loss: 0.0946, Val Accuracy: 97.24%\n","Epoch 6/40, Train Loss: 0.0594, Train Accuracy: 98.09%, Val Loss: 0.0354, Val Accuracy: 98.90%\n","Epoch 7/40, Train Loss: 0.0510, Train Accuracy: 98.37%, Val Loss: 0.0366, Val Accuracy: 98.89%\n","Epoch 8/40, Train Loss: 0.0467, Train Accuracy: 98.56%, Val Loss: 0.0404, Val Accuracy: 98.67%\n","Epoch 9/40, Train Loss: 0.0495, Train Accuracy: 98.37%, Val Loss: 0.0271, Val Accuracy: 99.12%\n","Epoch 10/40, Train Loss: 0.0276, Train Accuracy: 99.07%, Val Loss: 0.0401, Val Accuracy: 98.88%\n","Epoch 11/40, Train Loss: 0.0191, Train Accuracy: 99.42%, Val Loss: 0.0148, Val Accuracy: 99.51%\n","Epoch 12/40, Train Loss: 0.0139, Train Accuracy: 99.58%, Val Loss: 0.0366, Val Accuracy: 98.87%\n","Epoch 13/40, Train Loss: 0.0143, Train Accuracy: 99.51%, Val Loss: 0.0246, Val Accuracy: 99.30%\n","Epoch 14/40, Train Loss: 0.0167, Train Accuracy: 99.45%, Val Loss: 0.0183, Val Accuracy: 99.43%\n","Epoch 15/40, Train Loss: 0.0139, Train Accuracy: 99.59%, Val Loss: 0.0380, Val Accuracy: 98.99%\n","Epoch 16/40, Train Loss: 0.0209, Train Accuracy: 99.32%, Val Loss: 0.0307, Val Accuracy: 99.00%\n","Early stopping due to no improvement in validation loss.\n","InceptionV3 Test Loss: 0.0264, Test Accuracy: 99.23%\n","\n","\n","--------------- Training model: ResNet152\n","Epoch 1/40, Train Loss: 0.3895, Train Accuracy: 88.21%, Val Loss: 0.1379, Val Accuracy: 95.38%\n","Epoch 2/40, Train Loss: 0.1468, Train Accuracy: 95.16%, Val Loss: 0.1076, Val Accuracy: 96.67%\n","Epoch 3/40, Train Loss: 0.1002, Train Accuracy: 96.74%, Val Loss: 0.0620, Val Accuracy: 97.94%\n","Epoch 4/40, Train Loss: 0.0807, Train Accuracy: 97.36%, Val Loss: 0.0936, Val Accuracy: 97.07%\n","Epoch 5/40, Train Loss: 0.0659, Train Accuracy: 97.83%, Val Loss: 0.0765, Val Accuracy: 97.51%\n","Epoch 6/40, Train Loss: 0.0567, Train Accuracy: 98.13%, Val Loss: 0.0456, Val Accuracy: 98.45%\n","Epoch 7/40, Train Loss: 0.0475, Train Accuracy: 98.41%, Val Loss: 0.0733, Val Accuracy: 97.85%\n","Epoch 8/40, Train Loss: 0.0521, Train Accuracy: 98.34%, Val Loss: 0.0546, Val Accuracy: 98.22%\n","Epoch 9/40, Train Loss: 0.0391, Train Accuracy: 98.67%, Val Loss: 0.0866, Val Accuracy: 97.30%\n","Epoch 10/40, Train Loss: 0.0508, Train Accuracy: 98.32%, Val Loss: 1.3322, Val Accuracy: 94.09%\n","Epoch 11/40, Train Loss: 0.0181, Train Accuracy: 99.39%, Val Loss: 0.0258, Val Accuracy: 99.11%\n","Epoch 12/40, Train Loss: 0.0153, Train Accuracy: 99.50%, Val Loss: 0.0347, Val Accuracy: 98.88%\n","Epoch 13/40, Train Loss: 0.0077, Train Accuracy: 99.76%, Val Loss: 0.0169, Val Accuracy: 99.47%\n","Epoch 14/40, Train Loss: 0.0106, Train Accuracy: 99.64%, Val Loss: 0.0190, Val Accuracy: 99.34%\n","Epoch 15/40, Train Loss: 0.0192, Train Accuracy: 99.33%, Val Loss: 0.0388, Val Accuracy: 98.83%\n","Epoch 16/40, Train Loss: 0.0201, Train Accuracy: 99.35%, Val Loss: 0.0535, Val Accuracy: 98.38%\n","Epoch 17/40, Train Loss: 0.0325, Train Accuracy: 99.03%, Val Loss: 0.0392, Val Accuracy: 98.78%\n","Epoch 18/40, Train Loss: 0.0288, Train Accuracy: 99.09%, Val Loss: 0.0418, Val Accuracy: 98.67%\n","Early stopping due to no improvement in validation loss.\n","ResNet152 Test Loss: 0.0351, Test Accuracy: 98.84%\n","\n","\n","--------------- Training model: VGG19\n","Epoch 1/40, Train Loss: 3.1260, Train Accuracy: 17.47%, Val Loss: 2.5026, Val Accuracy: 30.37%\n","Epoch 2/40, Train Loss: 1.7867, Train Accuracy: 48.48%, Val Loss: 1.0553, Val Accuracy: 67.74%\n","Epoch 3/40, Train Loss: 0.9990, Train Accuracy: 69.30%, Val Loss: 0.7302, Val Accuracy: 77.30%\n","Epoch 4/40, Train Loss: 0.6920, Train Accuracy: 78.36%, Val Loss: 0.7372, Val Accuracy: 77.26%\n","Epoch 5/40, Train Loss: 0.5440, Train Accuracy: 82.83%, Val Loss: 0.5344, Val Accuracy: 83.28%\n","Epoch 6/40, Train Loss: 0.4469, Train Accuracy: 85.78%, Val Loss: 0.3654, Val Accuracy: 88.22%\n","Epoch 7/40, Train Loss: 0.3738, Train Accuracy: 88.08%, Val Loss: 0.3234, Val Accuracy: 89.62%\n","Epoch 8/40, Train Loss: 0.3385, Train Accuracy: 89.12%, Val Loss: 0.3326, Val Accuracy: 89.46%\n","Epoch 9/40, Train Loss: 0.3022, Train Accuracy: 90.23%, Val Loss: 0.2791, Val Accuracy: 90.94%\n","Epoch 10/40, Train Loss: 0.2718, Train Accuracy: 91.42%, Val Loss: 0.2917, Val Accuracy: 90.77%\n","Epoch 11/40, Train Loss: 0.1901, Train Accuracy: 93.77%, Val Loss: 0.2344, Val Accuracy: 92.80%\n","Epoch 12/40, Train Loss: 0.1633, Train Accuracy: 94.56%, Val Loss: 0.2263, Val Accuracy: 92.90%\n","Epoch 13/40, Train Loss: 0.1539, Train Accuracy: 94.88%, Val Loss: 0.2327, Val Accuracy: 92.82%\n","Epoch 14/40, Train Loss: 0.1592, Train Accuracy: 94.75%, Val Loss: 0.2017, Val Accuracy: 93.65%\n","Epoch 15/40, Train Loss: 0.1388, Train Accuracy: 95.38%, Val Loss: 0.1926, Val Accuracy: 93.95%\n","Epoch 16/40, Train Loss: 0.1284, Train Accuracy: 95.72%, Val Loss: 0.2284, Val Accuracy: 93.40%\n","Epoch 17/40, Train Loss: 0.1299, Train Accuracy: 95.77%, Val Loss: 0.2352, Val Accuracy: 93.14%\n","Epoch 18/40, Train Loss: 0.1374, Train Accuracy: 95.53%, Val Loss: 0.2398, Val Accuracy: 92.94%\n","Epoch 19/40, Train Loss: 0.1157, Train Accuracy: 96.17%, Val Loss: 0.2152, Val Accuracy: 93.38%\n","Epoch 20/40, Train Loss: 0.1221, Train Accuracy: 96.12%, Val Loss: 0.2362, Val Accuracy: 93.11%\n","Early stopping due to no improvement in validation loss.\n","VGG19 Test Loss: 0.2222, Test Accuracy: 93.46%\n","\n","\n","--------------- Training model: ViT\n","Epoch 1/40, Train Loss: 1.9691, Train Accuracy: 44.13%, Val Loss: 1.2091, Val Accuracy: 63.43%\n","Epoch 2/40, Train Loss: 0.9998, Train Accuracy: 69.14%, Val Loss: 0.7751, Val Accuracy: 75.90%\n","Epoch 3/40, Train Loss: 0.7170, Train Accuracy: 77.28%, Val Loss: 0.6870, Val Accuracy: 78.07%\n","Epoch 4/40, Train Loss: 0.5974, Train Accuracy: 80.59%, Val Loss: 0.5192, Val Accuracy: 83.23%\n","Epoch 5/40, Train Loss: 0.5208, Train Accuracy: 82.99%, Val Loss: 0.5293, Val Accuracy: 82.43%\n","Epoch 6/40, Train Loss: 0.4727, Train Accuracy: 84.24%, Val Loss: 0.4646, Val Accuracy: 84.55%\n","Epoch 7/40, Train Loss: 0.4340, Train Accuracy: 85.72%, Val Loss: 0.4265, Val Accuracy: 85.93%\n","Epoch 8/40, Train Loss: 0.4114, Train Accuracy: 86.27%, Val Loss: 0.4538, Val Accuracy: 84.97%\n","Epoch 9/40, Train Loss: 0.3766, Train Accuracy: 87.39%, Val Loss: 0.3935, Val Accuracy: 86.98%\n","Epoch 10/40, Train Loss: 0.3781, Train Accuracy: 87.32%, Val Loss: 0.3710, Val Accuracy: 87.72%\n","Epoch 11/40, Train Loss: 0.2978, Train Accuracy: 90.19%, Val Loss: 0.3108, Val Accuracy: 89.80%\n","Epoch 12/40, Train Loss: 0.2610, Train Accuracy: 91.23%, Val Loss: 0.3283, Val Accuracy: 89.00%\n","Epoch 13/40, Train Loss: 0.2439, Train Accuracy: 91.69%, Val Loss: 0.2863, Val Accuracy: 90.45%\n","Epoch 14/40, Train Loss: 0.2312, Train Accuracy: 91.91%, Val Loss: 0.2818, Val Accuracy: 90.97%\n","Epoch 15/40, Train Loss: 0.2185, Train Accuracy: 92.62%, Val Loss: 0.2762, Val Accuracy: 91.11%\n","Epoch 16/40, Train Loss: 0.2131, Train Accuracy: 92.63%, Val Loss: 0.2960, Val Accuracy: 90.35%\n","Epoch 17/40, Train Loss: 0.2016, Train Accuracy: 92.98%, Val Loss: 0.2664, Val Accuracy: 91.65%\n","Epoch 18/40, Train Loss: 0.1864, Train Accuracy: 93.66%, Val Loss: 0.2634, Val Accuracy: 91.47%\n","Epoch 19/40, Train Loss: 0.1892, Train Accuracy: 93.39%, Val Loss: 0.2639, Val Accuracy: 91.49%\n","Epoch 20/40, Train Loss: 0.1798, Train Accuracy: 93.95%, Val Loss: 0.2557, Val Accuracy: 92.13%\n","Epoch 21/40, Train Loss: 0.1360, Train Accuracy: 95.38%, Val Loss: 0.2157, Val Accuracy: 93.01%\n","Epoch 22/40, Train Loss: 0.1189, Train Accuracy: 95.86%, Val Loss: 0.2249, Val Accuracy: 92.75%\n","Epoch 23/40, Train Loss: 0.1134, Train Accuracy: 96.08%, Val Loss: 0.2194, Val Accuracy: 93.17%\n","Epoch 24/40, Train Loss: 0.1146, Train Accuracy: 95.94%, Val Loss: 0.2100, Val Accuracy: 93.28%\n","Epoch 25/40, Train Loss: 0.1027, Train Accuracy: 96.51%, Val Loss: 0.1876, Val Accuracy: 94.16%\n","Epoch 26/40, Train Loss: 0.1040, Train Accuracy: 96.46%, Val Loss: 0.2204, Val Accuracy: 93.13%\n","Epoch 27/40, Train Loss: 0.1007, Train Accuracy: 96.48%, Val Loss: 0.2112, Val Accuracy: 93.45%\n","Epoch 28/40, Train Loss: 0.1000, Train Accuracy: 96.49%, Val Loss: 0.2020, Val Accuracy: 93.91%\n","Epoch 29/40, Train Loss: 0.0958, Train Accuracy: 96.63%, Val Loss: 0.2192, Val Accuracy: 93.33%\n","Epoch 30/40, Train Loss: 0.0966, Train Accuracy: 96.54%, Val Loss: 0.2253, Val Accuracy: 93.21%\n","Early stopping due to no improvement in validation loss.\n","ViT Test Loss: 0.2087, Test Accuracy: 93.54%\n","\n","\n","--------------- Training model: AttentionAugmentedInceptionV3\n","Epoch 1/40, Train Loss: 0.4755, Train Accuracy: 87.43%, Val Loss: 0.1266, Val Accuracy: 96.16%\n","Epoch 2/40, Train Loss: 0.1490, Train Accuracy: 95.40%, Val Loss: 0.0912, Val Accuracy: 97.26%\n","Epoch 3/40, Train Loss: 0.1030, Train Accuracy: 96.80%, Val Loss: 0.1264, Val Accuracy: 96.45%\n","Epoch 4/40, Train Loss: 0.0791, Train Accuracy: 97.58%, Val Loss: 0.0743, Val Accuracy: 97.78%\n","Epoch 5/40, Train Loss: 0.0736, Train Accuracy: 97.73%, Val Loss: 0.0603, Val Accuracy: 98.12%\n","Epoch 6/40, Train Loss: 0.0638, Train Accuracy: 98.04%, Val Loss: 0.0478, Val Accuracy: 98.58%\n","Epoch 7/40, Train Loss: 0.0635, Train Accuracy: 97.98%, Val Loss: 0.0979, Val Accuracy: 97.03%\n","Epoch 8/40, Train Loss: 0.0570, Train Accuracy: 98.23%, Val Loss: 0.0478, Val Accuracy: 98.56%\n","Epoch 9/40, Train Loss: 0.0425, Train Accuracy: 98.63%, Val Loss: 0.0538, Val Accuracy: 98.56%\n","Epoch 10/40, Train Loss: 0.0473, Train Accuracy: 98.52%, Val Loss: 0.0652, Val Accuracy: 97.95%\n","Epoch 11/40, Train Loss: 0.0236, Train Accuracy: 99.24%, Val Loss: 0.0203, Val Accuracy: 99.47%\n","Epoch 12/40, Train Loss: 0.0158, Train Accuracy: 99.51%, Val Loss: 0.0205, Val Accuracy: 99.39%\n","Epoch 13/40, Train Loss: 0.0191, Train Accuracy: 99.43%, Val Loss: 0.0295, Val Accuracy: 99.14%\n","Epoch 14/40, Train Loss: 0.0202, Train Accuracy: 99.39%, Val Loss: 0.0231, Val Accuracy: 99.32%\n","Epoch 15/40, Train Loss: 0.0283, Train Accuracy: 99.10%, Val Loss: 0.0405, Val Accuracy: 98.86%\n","Epoch 16/40, Train Loss: 0.0219, Train Accuracy: 99.29%, Val Loss: 0.0303, Val Accuracy: 99.11%\n","Early stopping due to no improvement in validation loss.\n","AttentionAugmentedInceptionV3 Test Loss: 0.0315, Test Accuracy: 99.17%\n","\n","\n","--------------- Training model: AttentionAugmentedVGG19\n"]}],"source":["# Train the models\n","for model_name, base_model in pretrained_models.items():\n","\n","    base_model.to(device)  # Ensure the model is on the correct device\n","    \n","    if model_name == 'InceptionV3':\n","        base_model.AuxLogits.fc = nn.Linear(base_model.AuxLogits.fc.in_features, num_classes_inception)\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_inception)\n","        train_loader = train_loader_inception\n","        val_loader = val_loader_inception\n","        test_loader = test_loader_inception\n","    elif model_name == 'ViT':\n","        base_model.mlp_head = nn.Linear(base_model.mlp_head.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'ResNet152':\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'VGG19':\n","        base_model.classifier[-1] = nn.Linear(base_model.classifier[-1].in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'AttentionAugmentedResNet18':\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    else:\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","\n","    print(f'--------------- Training model: {model_name}')\n","    model = create_and_train_model(base_model, train_loader, val_loader, num_classes_others, device, initial_lr=0.001).to(device)\n","\n","    test_loss, test_accuracy = evaluate_model(model, test_loader, nn.CrossEntropyLoss(), device)\n","\n","    crop_results[model_name] = {\n","        'model': model,\n","        'test_loss': test_loss,\n","        'test_accuracy': test_accuracy\n","    }\n","    print(f'{model_name} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n","\n","    # Clean up: delete the model to free up memory (optional)\n","    del model\n","    clear_cache()\n","    \n","    print(f'\\n')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Display Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/Project/Vision-For-Social-Good/Results/Single Modal\"\n","results_folder = os.path.join(results_base_dir, 'T3')\n","os.makedirs(results_folder, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to save figures\n","def save_figure(fig, filename):\n","    fig.savefig(os.path.join(results_folder, filename))\n","    plt.close(fig)"]},{"cell_type":"markdown","metadata":{},"source":["### Accuracy Comparision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot comparison of accuracy for each model for each crop\n","def plot_accuracy_comparison(results):\n","    accuracies = [result['test_accuracy'] for result in results.values()]\n","    model_names = list(results.keys())\n","\n","    fig = plt.figure(figsize=(20, 10))\n","    plt.bar(model_names, accuracies)\n","    plt.ylabel('Accuracy (%)')\n","    plt.xlabel('Model')\n","    plt.show()\n","    save_figure(fig, 'accuracy_comparison.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot comparison of accuracy for each model for each crop\n","plot_accuracy_comparison(crop_results)"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics Table"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to display F1, precision, and recall of all models as a table\n","def display_model_metrics_table(results, test_loader):\n","    metrics_data = []\n","    \n","    for model_name, model_info in crop_results.items():\n","        model = model_info['model']\n","        device = next(model.parameters()).device  # Get the device of the model\n","        model.eval()  # Set the model to evaluation mode\n","\n","        all_labels = []\n","        all_predicted = []\n","\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            with torch.no_grad():\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predicted.extend(predicted.cpu().numpy())\n","\n","        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n","        \n","        metrics_data.append({\n","            'Model': model_name,\n","            'Precision': precision,\n","            'Recall': recall,\n","            'F1-score': f1\n","        })\n","\n","    metrics_df = pd.DataFrame(metrics_data)\n","    display(metrics_df)  # Display the DataFrame in Jupyter Notebook\n","    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display the table of metrics for all models\n","display_model_metrics_table(crop_results, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display some correctly and incorrectly classified images\n","def display_classification_results(model, test_loader, num_images=5):\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","    class_labels = list(test_loader.dataset.class_to_idx.keys())\n","    \n","    images, labels = next(iter(test_loader))\n","    images, labels = images[:num_images].to(device), labels[:num_images]  # Move tensors to the model's device\n","    \n","    with torch.no_grad():\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","    \n","    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n","    # fig.suptitle(f'{model_name} - Classification Results', fontsize=28)\n","    \n","    for i in range(num_images):\n","        ax = axes[i]\n","        img = images[i].cpu().numpy().transpose((1, 2, 0))  # Move tensor back to CPU for visualization\n","        img = np.clip(img, 0, 1)\n","        ax.imshow(img)\n","        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')  # Access CPU tensor for labels\n","        ax.axis('off')\n","\n","    plt.show()\n","    save_figure(fig, f'{model_name}_classification_results.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display results for each crop\n","for model_name in crop_results.keys():\n","    print(f'Displaying results for {model_name}')\n","    display_classification_results(crop_results[model_name]['model'], test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to display the classification report of a given model\n","def display_classification_report(model, test_loader, model_name):\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    all_labels = []\n","    all_predicted = []\n","\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_predicted.extend(predicted.cpu().numpy())\n","\n","    report = classification_report(all_labels, all_predicted, target_names=list(test_loader.dataset.class_to_idx.keys()))\n","    \n","    print(report)\n","    \n","    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n","    with open(report_filename, 'w') as f:\n","        f.write(report)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display results for each crop\n","for model_name in crop_results.keys():\n","    print(f'Displaying classification report for {model_name}')\n","    display_classification_report(crop_results[model_name]['model'], test_loader, model_name)"]},{"cell_type":"markdown","metadata":{},"source":["### Confusion Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n","    fig = plt.figure(figsize=(50, 50))\n","    # fig.suptitle(f'{model_name} - Confusion Matrix\\n', fontsize=28, y=0.83)\n","    ax = fig.add_subplot(1, 1, 1)\n","    cm = confusion_matrix(labels, pred_labels)\n","    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n","    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n","    fig.delaxes(fig.axes[1])  # Delete colorbar\n","    plt.xticks(rotation=90)\n","    plt.xlabel('Predicted Label', fontsize=50)\n","    plt.ylabel('True Label', fontsize=50)\n","\n","    plt.show()\n","    save_figure(fig, f'{model_name}_confusion_matrix.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to extract all labels and predictions\n","def get_all_labels_and_preds(model, test_loader):\n","    all_labels = []\n","    all_preds = []\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","    return all_labels, all_preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate and plot confusion matrices\n","def generate_confusion_matrices(results, test_loader):\n","    classes = list(test_loader.dataset.class_to_idx.keys())\n","    for model_name, model_info in results.items():\n","        model = model_info['model']\n","        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n","        plot_confusion_matrix(labels, pred_labels, classes, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generate_confusion_matrices(crop_results, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Incorrect Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to normalize images\n","def normalize_image(image):\n","    image = image - image.min()\n","    image = image / image.max()\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to plot the most incorrect predictions\n","def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n","    rows = int(np.ceil(np.sqrt(n_images)))\n","    cols = int(np.ceil(n_images / rows))\n","\n","    fig = plt.figure(figsize=(25, 20))\n","    # fig.suptitle(f'{model_name} - Most Incorrect\\n', fontsize=28)\n","\n","    for i in range(rows * cols):\n","        if i >= len(incorrect):\n","            break\n","        ax = fig.add_subplot(rows, cols, i + 1)\n","        image, true_label, probs = incorrect[i]\n","        image = image.permute(1, 2, 0)\n","        true_prob = probs[true_label]\n","        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n","        true_class = classes[true_label]\n","        incorrect_class = classes[incorrect_label]\n","\n","        if normalize:\n","            image = normalize_image(image)\n","\n","        ax.imshow(image.cpu().numpy())\n","        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n","                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    fig.subplots_adjust(hspace=0.7)\n","    \n","    plt.show()\n","    save_figure(fig, f'{model_name}_most_incorrect.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_all_details(model, test_loader):\n","    all_labels = []\n","    all_preds = []\n","    all_probs = []\n","    all_images = []\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            probs = F.softmax(outputs, dim=1)\n","\n","            all_images.extend(images.cpu())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            all_probs.extend(probs.cpu())\n","\n","    return all_images, all_labels, all_preds, all_probs\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the number of images to display\n","N_IMAGES = 36"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use this function to get the details\n","def plot_most_incorrect_predictions(results, test_loader, n_images=36):\n","    classes = list(test_loader.dataset.class_to_idx.keys())\n","    for model_name, model_info in results.items():\n","        model = model_info['model']\n","        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n","        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n","        incorrect_examples = []\n","\n","        for image, label, prob, correct in zip(images, labels, probs, corrects):\n","            if not correct:\n","                incorrect_examples.append((image, label, prob))\n","\n","    incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n","    plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_most_incorrect_predictions(crop_results, test_loader, N_IMAGES)"]},{"cell_type":"markdown","metadata":{},"source":["### Representations and Dimensionality Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import decomposition, manifold\n","\n","def get_representations(model, iterator):\n","    model.eval()\n","    outputs = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for x, y in iterator:\n","            x = x.to(device)\n","            y_pred = model(x)\n","            outputs.append(y_pred.cpu())\n","            labels.append(y)\n","\n","    outputs = torch.cat(outputs, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","    return outputs, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_pca(data, n_components=2):\n","    pca = decomposition.PCA(n_components=n_components)\n","    pca_data = pca.fit_transform(data)\n","    return pca_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_representations(data, labels, classes, n_images=None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","        labels = labels[:n_images]\n","\n","    fig = plt.figure(figsize=(15, 15))\n","    # fig.suptitle(f'{model_name} - PCA', fontsize=28, y=0.95)\n","    ax = fig.add_subplot(111)\n","    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='hsv')\n","    plt.show()\n","    save_figure(fig, f'{model_name}_pca.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs, labels = get_representations(model, train_loader)\n","\n","for model_name in crop_results.keys():\n","    output_pca_data = get_pca(outputs)\n","    plot_representations(output_pca_data, labels, classes)  # Adjusted to pass only three arguments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_tsne(data, n_components=2, n_images=None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","    tsne = manifold.TSNE(n_components=n_components, random_state=0)\n","    tsne_data = tsne.fit_transform(data)\n","    return tsne_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for model_name in crop_results.keys():\n","    output_tsne_data = get_tsne(outputs)\n","    plot_representations(output_tsne_data, labels, classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Filter Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to plot filtered images\n","def plot_filtered_images(images, filters, model_name, n_filters=None, normalize=True):\n","    images = torch.cat([i.unsqueeze(0) for i in images], dim=0).cpu()\n","    filters = filters.cpu()\n","\n","    if n_filters is not None:\n","        filters = filters[:n_filters]\n","\n","    n_images = images.shape[0]\n","    n_filters = filters.shape[0]\n","\n","    filtered_images = F.conv2d(images, filters)\n","\n","    fig = plt.figure(figsize=(30, 30))\n","    # fig.suptitle(f'{model_name} - Filtered Images', fontsize=28, y=0.8)\n","\n","    for i in range(n_images):\n","        image = images[i]\n","        if normalize:\n","            image = normalize_image(image)\n","        ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters))\n","        ax.imshow(image.permute(1, 2, 0).numpy())\n","        ax.set_title('Original')\n","        ax.axis('off')\n","\n","        for j in range(n_filters):\n","            image = filtered_images[i][j]\n","            if normalize:\n","                image = normalize_image(image)\n","            ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters) + j + 1)\n","            ax.imshow(image.numpy(), cmap='bone')\n","            ax.set_title(f'Filter {j + 1}')\n","            ax.axis('off')\n","\n","    fig.subplots_adjust(hspace=-0.7)\n","    plt.show()\n","    save_figure(fig, f'{model_name}_filtered_images.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N_FILTERS = 7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage within the existing loop\n","conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n","\n","for model_name, model_info in crop_results.items():\n","    model = model_info['model']\n","    if model_name in conv_models:\n","        if hasattr(model, 'conv1'):\n","            filters = model.conv1.weight.data\n","        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n","            filters = model.features[0].weight.data\n","        else:\n","            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n","            filters = None\n","    else:\n","        filters = None  # No convolutional filters in models like ViT\n","\n","    if filters is not None:\n","        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n","        plot_filtered_images(images, filters, model_name, n_filters=N_FILTERS)"]},{"cell_type":"markdown","metadata":{},"source":["### Filter Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_filters(filters, normalize=True):\n","    filters = filters.cpu()\n","    n_filters = filters.shape[0]\n","    rows = int(np.sqrt(n_filters))\n","    cols = int(np.sqrt(n_filters))\n","\n","    fig = plt.figure(figsize=(30, 15))\n","    # fig.suptitle(f'{model_name} - Filters', fontsize=28, y=0.95)\n","\n","    for i in range(rows * cols):\n","        image = filters[i]\n","        if normalize:\n","            image = normalize_image(image)\n","        ax = fig.add_subplot(rows, cols, i + 1)\n","        ax.imshow(image.permute(1, 2, 0))\n","        ax.axis('off')\n","\n","    fig.subplots_adjust(wspace=-0.9)\n","    plt.show()\n","    save_figure(fig, f'{model_name}_filters.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage within the existing loop\n","conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n","\n","for model_name, model_info in crop_results.items():\n","    model = model_info['model']\n","    if model_name in conv_models:\n","        if hasattr(model, 'conv1'):\n","            filters = model.conv1.weight.data\n","        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n","            filters = model.features[0].weight.data\n","        else:\n","            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n","            filters = None\n","    else:\n","        filters = None  # No convolutional filters in models like ViT\n","\n","    if filters is not None:\n","        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n","        plot_filters(filters)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4056201,"sourceId":7048575,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":94664,"modelInstanceId":69528,"sourceId":82772,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
