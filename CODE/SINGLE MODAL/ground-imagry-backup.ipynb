{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:08:59.593560Z","iopub.status.busy":"2024-07-27T13:08:59.593261Z","iopub.status.idle":"2024-07-27T13:09:05.625131Z","shell.execute_reply":"2024-07-27T13:09:05.624160Z","shell.execute_reply.started":"2024-07-27T13:08:59.593532Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n","\n","from efficientnet_pytorch import EfficientNet\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, classification_report\n","\n","from PIL import Image\n","from PIL.ExifTags import TAGS, GPSTAGS\n","\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import datasets, models, transforms\n","from torchvision.datasets.folder import is_image_file\n","# Ensure these imports are included\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.optim.lr_scheduler import StepLR\n","\n","from vit_pytorch import ViT\n","\n","from AACN_Model import attention_augmented_resnet18, attention_augmented_efficientnetb0, attention_augmented_inceptionv3, attention_augmented_vit, attention_augmented_vgg"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.627047Z","iopub.status.busy":"2024-07-27T13:09:05.626426Z","iopub.status.idle":"2024-07-27T13:09:05.633414Z","shell.execute_reply":"2024-07-27T13:09:05.632171Z","shell.execute_reply.started":"2024-07-27T13:09:05.627014Z"},"trusted":true},"outputs":[],"source":["# Define main directories\n","base_dir = '/Users/izzymohamed/Desktop/Vision For Social Good/EXTRA/CODE/shubham10divakar Multimodal-Plant-Disease-Dataset/Data' #'/Users/izzymohamed/Downloads/shubham10divakar Multimodal-Plant-Disease-Dataset/Data'\n","crop_root = os.path.join(base_dir, 'color')\n","split_root = os.path.join(base_dir, 'split')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.636232Z","iopub.status.busy":"2024-07-27T13:09:05.635874Z","iopub.status.idle":"2024-07-27T13:09:05.644500Z","shell.execute_reply":"2024-07-27T13:09:05.643479Z","shell.execute_reply.started":"2024-07-27T13:09:05.636202Z"},"trusted":true},"outputs":[],"source":["# Define function to remove .DS_Store files\n","def remove_ds_store(directory):\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            if file == '.DS_Store' or '.DS_Store' in file:\n","                file_path = os.path.join(root, file)\n","                print(f\"Removing {file_path}\")\n","                os.remove(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:09:05.646082Z","iopub.status.busy":"2024-07-27T13:09:05.645687Z","iopub.status.idle":"2024-07-27T13:10:14.235460Z","shell.execute_reply":"2024-07-27T13:10:14.234651Z","shell.execute_reply.started":"2024-07-27T13:09:05.646050Z"},"trusted":true},"outputs":[],"source":["# Remove .DS_Store files from base directory\n","remove_ds_store(base_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.236865Z","iopub.status.busy":"2024-07-27T13:10:14.236579Z","iopub.status.idle":"2024-07-27T13:10:14.241736Z","shell.execute_reply":"2024-07-27T13:10:14.240691Z","shell.execute_reply.started":"2024-07-27T13:10:14.236840Z"},"trusted":true},"outputs":[],"source":["def is_image_file(filename):\n","    # Assuming is_image_file is a function that checks if the file is an image\n","    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.243241Z","iopub.status.busy":"2024-07-27T13:10:14.242978Z","iopub.status.idle":"2024-07-27T13:10:14.259354Z","shell.execute_reply":"2024-07-27T13:10:14.258631Z","shell.execute_reply.started":"2024-07-27T13:10:14.243219Z"},"trusted":true},"outputs":[],"source":["# Function to split data into train, validation, and test sets\n","def split_data(base_dir, val_split=0.4, test_split=0.1):\n","    train_files = []\n","    val_files = []\n","    test_files = []\n","\n","    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n","    for cls in classes:\n","        print(f'Processing class: {cls}')\n","        class_dir = os.path.join(base_dir, cls)\n","\n","        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n","\n","        if len(images) == 0:\n","            print(f\"No images found for class {cls}. Skipping...\")\n","            continue\n","\n","        # Shuffle images to randomize the selection\n","        random.shuffle(images)\n","\n","        try:\n","            train, test = train_test_split(images, test_size=test_split)\n","            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n","        except ValueError as e:\n","            print(f\"Not enough images to split for class {cls}: {e}\")\n","            continue\n","\n","        train_files.extend([(os.path.join(class_dir, img), cls) for img in train])\n","        val_files.extend([(os.path.join(class_dir, img), cls) for img in val])\n","        test_files.extend([(os.path.join(class_dir, img), cls) for img in test])\n","\n","    return train_files, val_files, test_files, classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.260981Z","iopub.status.busy":"2024-07-27T13:10:14.260493Z","iopub.status.idle":"2024-07-27T13:10:14.622675Z","shell.execute_reply":"2024-07-27T13:10:14.621706Z","shell.execute_reply.started":"2024-07-27T13:10:14.260957Z"},"trusted":true},"outputs":[],"source":["# Split data\n","train_files, val_files, test_files, classes = split_data(crop_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.624109Z","iopub.status.busy":"2024-07-27T13:10:14.623815Z","iopub.status.idle":"2024-07-27T13:10:14.628646Z","shell.execute_reply":"2024-07-27T13:10:14.627718Z","shell.execute_reply.started":"2024-07-27T13:10:14.624085Z"},"trusted":true},"outputs":[],"source":["# Use the lists of file paths for your dataset loading and transformations\n","print(f\"Train files: {len(train_files)}\")\n","print(f\"Validation files: {len(val_files)}\")\n","print(f\"Test files: {len(test_files)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.632417Z","iopub.status.busy":"2024-07-27T13:10:14.632130Z","iopub.status.idle":"2024-07-27T13:10:14.638617Z","shell.execute_reply":"2024-07-27T13:10:14.637715Z","shell.execute_reply.started":"2024-07-27T13:10:14.632389Z"},"trusted":true},"outputs":[],"source":["# Define the standard image sizes\n","inception_size = 299\n","other_size = 224"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.639961Z","iopub.status.busy":"2024-07-27T13:10:14.639683Z","iopub.status.idle":"2024-07-27T13:10:14.650118Z","shell.execute_reply":"2024-07-27T13:10:14.649245Z","shell.execute_reply.started":"2024-07-27T13:10:14.639938Z"},"trusted":true},"outputs":[],"source":["# Update the data transformations\n","data_transforms = {\n","    'InceptionV3': {\n","        'train': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'test': transforms.Compose([\n","            transforms.Resize((inception_size, inception_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","    },\n","    'Others': {\n","        'train': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","        'test': transforms.Compose([\n","            transforms.Resize((other_size, other_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ]),\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.652122Z","iopub.status.busy":"2024-07-27T13:10:14.651165Z","iopub.status.idle":"2024-07-27T13:10:14.664066Z","shell.execute_reply":"2024-07-27T13:10:14.663232Z","shell.execute_reply.started":"2024-07-27T13:10:14.652097Z"},"trusted":true},"outputs":[],"source":["# Create a custom dataset class to load images from the file lists\n","class CustomDataset(Dataset):\n","    def __init__(self, file_paths, class_to_idx, transform=None):\n","        self.file_paths = file_paths\n","        self.class_to_idx = class_to_idx\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path, cls = self.file_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.class_to_idx[cls]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.665307Z","iopub.status.busy":"2024-07-27T13:10:14.665078Z","iopub.status.idle":"2024-07-27T13:10:14.677497Z","shell.execute_reply":"2024-07-27T13:10:14.676790Z","shell.execute_reply.started":"2024-07-27T13:10:14.665287Z"},"trusted":true},"outputs":[],"source":["# Create a mapping from class names to indices\n","class_to_idx = {cls: idx for idx, cls in enumerate(classes)}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.679006Z","iopub.status.busy":"2024-07-27T13:10:14.678657Z","iopub.status.idle":"2024-07-27T13:10:14.688325Z","shell.execute_reply":"2024-07-27T13:10:14.687476Z","shell.execute_reply.started":"2024-07-27T13:10:14.678977Z"},"trusted":true},"outputs":[],"source":["# Create datasets and data loaders\n","train_dataset_inception = CustomDataset(train_files, class_to_idx, transform=data_transforms['InceptionV3']['train'])\n","val_dataset_inception = CustomDataset(val_files, class_to_idx, transform=data_transforms['InceptionV3']['val'])\n","test_dataset_inception = CustomDataset(test_files, class_to_idx, transform=data_transforms['InceptionV3']['test'])\n","\n","train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n","val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n","test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.690062Z","iopub.status.busy":"2024-07-27T13:10:14.689483Z","iopub.status.idle":"2024-07-27T13:10:14.698091Z","shell.execute_reply":"2024-07-27T13:10:14.697244Z","shell.execute_reply.started":"2024-07-27T13:10:14.690032Z"},"trusted":true},"outputs":[],"source":["# Loaders for other models\n","train_dataset_others = CustomDataset(train_files, class_to_idx, transform=data_transforms['Others']['train'])\n","val_dataset_others = CustomDataset(val_files, class_to_idx, transform=data_transforms['Others']['val'])\n","test_dataset_others = CustomDataset(test_files, class_to_idx, transform=data_transforms['Others']['test'])\n","\n","train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n","val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n","test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.699828Z","iopub.status.busy":"2024-07-27T13:10:14.699293Z","iopub.status.idle":"2024-07-27T13:10:14.708077Z","shell.execute_reply":"2024-07-27T13:10:14.707202Z","shell.execute_reply.started":"2024-07-27T13:10:14.699798Z"},"trusted":true},"outputs":[],"source":["# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n","if torch.backends.mps.is_available():\n","    device = torch.device(\"mps\")\n","elif torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"Device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.709322Z","iopub.status.busy":"2024-07-27T13:10:14.709092Z","iopub.status.idle":"2024-07-27T13:10:14.721120Z","shell.execute_reply":"2024-07-27T13:10:14.720300Z","shell.execute_reply.started":"2024-07-27T13:10:14.709302Z"},"trusted":true},"outputs":[],"source":["# Function to check if a file is valid\n","def is_valid_file(path):\n","    return not path.endswith('.DS_Store') or 'DS_Store' not in path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.722909Z","iopub.status.busy":"2024-07-27T13:10:14.722272Z","iopub.status.idle":"2024-07-27T13:10:14.730619Z","shell.execute_reply":"2024-07-27T13:10:14.729938Z","shell.execute_reply.started":"2024-07-27T13:10:14.722862Z"},"trusted":true},"outputs":[],"source":["# Function to adjust learning rate\n","def adjust_learning_rate(optimizer, epoch, learning_rate):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n","    lr = learning_rate * (0.1 ** (epoch // 10))\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training function with mixed precision and gradient accumulation\n","def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=40, accumulation_steps=4, initial_lr=0.001):\n","    scaler = GradScaler()\n","    early_stopping_patience = 5\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    model.to(device)\n","    scheduler = StepLR(optimizer, step_size=10, gamma=0.7)  # Example scheduler\n","\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        # adjust_learning_rate(optimizer, epoch, initial_lr)\n","        \n","        # optimizer.zero_grad()\n","        for i, data in enumerate(train_loader):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # with autocast():\n","            outputs = model(inputs)\n","            if isinstance(outputs, tuple):\n","                outputs = outputs[0]\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","                \n","            scaler.scale(loss).backward()\n","            \n","            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = running_loss / len(train_loader)\n","        train_accuracy = 100 * correct / total\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                with autocast():\n","                    outputs = model(inputs)\n","                    if isinstance(outputs, tuple):\n","                        outputs = outputs[0]\n","                    loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss /= len(val_loader)\n","        val_accuracy = 100 * correct / total\n","\n","        print(f'Epoch {epoch + 1}/{num_epochs}, '\n","              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n","              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= early_stopping_patience:\n","                print(\"Early stopping due to no improvement in validation loss.\")\n","                break\n","        \n","        scheduler.step()\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.748331Z","iopub.status.busy":"2024-07-27T13:10:14.747914Z","iopub.status.idle":"2024-07-27T13:10:14.762129Z","shell.execute_reply":"2024-07-27T13:10:14.761247Z","shell.execute_reply.started":"2024-07-27T13:10:14.748307Z"},"trusted":true},"outputs":[],"source":["# Function to create and train the model\n","def create_and_train_model(model, train_loader, val_loader, num_classes, device, num_epochs=40, initial_lr=0.001):\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n","    return train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=num_epochs, initial_lr=initial_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.732101Z","iopub.status.busy":"2024-07-27T13:10:14.731766Z","iopub.status.idle":"2024-07-27T13:10:14.746726Z","shell.execute_reply":"2024-07-27T13:10:14.745935Z","shell.execute_reply.started":"2024-07-27T13:10:14.732072Z"},"trusted":true},"outputs":[],"source":["# Function to evaluate the models\n","def evaluate_model(model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            if isinstance(outputs, tuple):\n","                outputs = outputs[0]\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_accuracy = 100 * correct / total\n","    return test_loss, test_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Clear cache function\n","def clear_cache():\n","    if torch.backends.mps.is_available():\n","        torch.mps.empty_cache()\n","    elif torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","    else:\n","        torch.cache.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the best model\n","def save_model(model, model_name, crop):\n","    model_dir = os.path.join(base_dir, 'saved_models')\n","    os.makedirs(model_dir, exist_ok=True)\n","    model_path = os.path.join(model_dir, f'{model_name}.pth')\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved at {model_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the best model\n","def load_model(model, model_name, crop):\n","    model_dir = os.path.join(base_dir, 'saved_models')\n","    model_path = os.path.join(model_dir, f'{model_name}.pth')\n","    model.load_state_dict(torch.load(model_path))\n","    print(f\"Model loaded from {model_path}\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.778547Z","iopub.status.busy":"2024-07-27T13:10:14.778205Z","iopub.status.idle":"2024-07-27T13:10:14.786445Z","shell.execute_reply":"2024-07-27T13:10:14.785755Z","shell.execute_reply.started":"2024-07-27T13:10:14.778510Z"},"trusted":true},"outputs":[],"source":["# Function to find classes in a directory\n","def find_classes(dir):\n","    if not os.path.exists(dir):\n","        os.makedirs(dir, exist_ok=True)\n","        print(f\"Created directory: {dir}\")\n","    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d)) and not d.startswith('.')]\n","    classes.sort()\n","    class_to_idx = {classes[i]: i for i in range(len(classes))}\n","    return classes, class_to_idx"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Count the number of classes\n","num_classes_inception = len(class_to_idx)\n","num_classes_others = len(class_to_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set the number of features in the CSV data\n","num_heads = 8"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the results dictionary\n","crop_results = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the pretrained models\n","pretrained_models = {\n","    'InceptionV3': models.inception_v3(pretrained=True).to(device),\n","    'ResNet152': models.resnet152(pretrained=True).to(device),\n","    'VGG19': models.vgg19(pretrained=True).to(device),\n","    'ViT': ViT(\n","        image_size=224,\n","        patch_size=16,\n","        num_classes=num_classes_others,\n","        dim=1024,\n","        depth=6,\n","        heads=16,\n","        mlp_dim=2048,\n","        dropout=0.1,\n","        emb_dropout=0.1\n","    ).to(device),\n","    \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True).to(device),\n","    'AttentionAugmentedVGG19': attention_augmented_vgg('VGG19',num_classes=num_classes_others).to(device),\n","    \"AttentionAugmentedResNet18\": attention_augmented_resnet18(num_classes=num_classes_others, attention=[False, True, True, True], num_heads=8).to(device),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T13:10:14.798371Z","iopub.status.busy":"2024-07-27T13:10:14.797964Z"},"trusted":true},"outputs":[],"source":["# Train the models\n","for model_name, base_model in pretrained_models.items():\n","\n","    base_model.to(device)  # Ensure the model is on the correct device\n","    \n","    if model_name == 'InceptionV3':\n","        base_model.AuxLogits.fc = nn.Linear(base_model.AuxLogits.fc.in_features, num_classes_inception)\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_inception)\n","        train_loader = train_loader_inception\n","        val_loader = val_loader_inception\n","        test_loader = test_loader_inception\n","    elif model_name == 'ViT':\n","        base_model.mlp_head = nn.Linear(base_model.mlp_head.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'ResNet152':\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'VGG19':\n","        base_model.classifier[-1] = nn.Linear(base_model.classifier[-1].in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    elif model_name == 'AttentionAugmentedResNet18':\n","        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_others)\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","    else:\n","        train_loader = train_loader_others\n","        val_loader = val_loader_others\n","        test_loader = test_loader_others\n","\n","    print(f'--------------- Training model: {model_name}')\n","    model = create_and_train_model(base_model, train_loader, val_loader, num_classes_others, device, initial_lr=0.001)\n","\n","    test_loss, test_accuracy = evaluate_model(model, test_loader, nn.CrossEntropyLoss(), device)\n","\n","    crop_results[model_name] = {\n","        'model': model,\n","        'test_loss': test_loss,\n","        'test_accuracy': test_accuracy\n","    }\n","    print(f'{model_name} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n","\n","    # Clean up: delete the model to free up memory (optional)\n","    del model\n","    clear_cache()\n","    \n","    print(f'\\n')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Display Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_base_dir = \"/Users/izzymohamed/Desktop/Vision For Social Good/EXTRA/CODE/shubham10divakar Multimodal-Plant-Disease-Dataset/Results/Single Modal\"\n","results_folder = os.path.join(results_base_dir, 'T3')\n","os.makedirs(results_folder, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to save figures\n","def save_figure(fig, filename):\n","    fig.savefig(os.path.join(results_folder, filename))\n","    plt.close(fig)"]},{"cell_type":"markdown","metadata":{},"source":["### Accuracy Comparision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot comparison of accuracy for each model for each crop\n","def plot_accuracy_comparison(results):\n","    accuracies = [result['test_accuracy'] for result in results.values()]\n","    model_names = list(results.keys())\n","\n","    fig = plt.figure(figsize=(20, 10))\n","    plt.bar(model_names, accuracies)\n","    plt.ylabel('Accuracy (%)')\n","    plt.xlabel('Model')\n","    plt.show()\n","    save_figure(fig, 'accuracy_comparison.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot comparison of accuracy for each model for each crop\n","plot_accuracy_comparison(crop_results)"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics Table"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to display F1, precision, and recall of all models as a table\n","def display_model_metrics_table(results, test_loader):\n","    metrics_data = []\n","    \n","    for model_name, model_info in crop_results.items():\n","        model = model_info['model']\n","        device = next(model.parameters()).device  # Get the device of the model\n","        model.eval()  # Set the model to evaluation mode\n","\n","        all_labels = []\n","        all_predicted = []\n","\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            with torch.no_grad():\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs, 1)\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predicted.extend(predicted.cpu().numpy())\n","\n","        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n","        \n","        metrics_data.append({\n","            'Model': model_name,\n","            'Precision': precision,\n","            'Recall': recall,\n","            'F1-score': f1\n","        })\n","\n","    metrics_df = pd.DataFrame(metrics_data)\n","    display(metrics_df)  # Display the DataFrame in Jupyter Notebook\n","    metrics_df.to_csv(os.path.join(results_folder, 'model_metrics.csv'), index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display the table of metrics for all models\n","display_model_metrics_table(crop_results, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display some correctly and incorrectly classified images\n","def display_classification_results(model, test_loader, num_images=5):\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","    class_labels = list(test_loader.dataset.class_to_idx.keys())\n","    \n","    images, labels = next(iter(test_loader))\n","    images, labels = images[:num_images].to(device), labels[:num_images]  # Move tensors to the model's device\n","    \n","    with torch.no_grad():\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","    \n","    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n","    # fig.suptitle(f'{model_name} - Classification Results', fontsize=28)\n","    \n","    for i in range(num_images):\n","        ax = axes[i]\n","        img = images[i].cpu().numpy().transpose((1, 2, 0))  # Move tensor back to CPU for visualization\n","        img = np.clip(img, 0, 1)\n","        ax.imshow(img)\n","        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')  # Access CPU tensor for labels\n","        ax.axis('off')\n","\n","    plt.show()\n","    save_figure(fig, f'{model_name}_classification_results.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display results for each crop\n","for model_name in crop_results.keys():\n","    print(f'Displaying results for {model_name}')\n","    display_classification_results(crop_results[model_name]['model'], test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to display the classification report of a given model\n","def display_classification_report(model, test_loader, model_name):\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    all_labels = []\n","    all_predicted = []\n","\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","\n","        all_labels.extend(labels.cpu().numpy())\n","        all_predicted.extend(predicted.cpu().numpy())\n","\n","    report = classification_report(all_labels, all_predicted, target_names=list(test_loader.dataset.class_to_idx.keys()))\n","    \n","    print(report)\n","    \n","    report_filename = os.path.join(results_folder, f'{model_name}_classification_report.txt')\n","    with open(report_filename, 'w') as f:\n","        f.write(report)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Display results for each crop\n","for model_name in crop_results.keys():\n","    print(f'Displaying classification report for {model_name}')\n","    display_classification_report(crop_results[model_name]['model'], test_loader, model_name)"]},{"cell_type":"markdown","metadata":{},"source":["### Confusion Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def plot_confusion_matrix(labels, pred_labels, classes, model_name):\n","    fig = plt.figure(figsize=(50, 50))\n","    # fig.suptitle(f'{model_name} - Confusion Matrix\\n', fontsize=28, y=0.83)\n","    ax = fig.add_subplot(1, 1, 1)\n","    cm = confusion_matrix(labels, pred_labels)\n","    cm_display = ConfusionMatrixDisplay(cm, display_labels=classes)\n","    cm_display.plot(values_format='d', cmap='Blues', ax=ax)\n","    fig.delaxes(fig.axes[1])  # Delete colorbar\n","    plt.xticks(rotation=90)\n","    plt.xlabel('Predicted Label', fontsize=50)\n","    plt.ylabel('True Label', fontsize=50)\n","\n","    plt.show()\n","    save_figure(fig, f'{model_name}_confusion_matrix.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to extract all labels and predictions\n","def get_all_labels_and_preds(model, test_loader):\n","    all_labels = []\n","    all_preds = []\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","    return all_labels, all_preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate and plot confusion matrices\n","def generate_confusion_matrices(results, test_loader):\n","    classes = list(test_loader.dataset.class_to_idx.keys())\n","    for model_name, model_info in results.items():\n","        model = model_info['model']\n","        labels, pred_labels = get_all_labels_and_preds(model, test_loader)\n","        plot_confusion_matrix(labels, pred_labels, classes, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generate_confusion_matrices(crop_results, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Incorrect Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to normalize images\n","def normalize_image(image):\n","    image = image - image.min()\n","    image = image / image.max()\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to plot the most incorrect predictions\n","def plot_most_incorrect(incorrect, classes, n_images, model_name, normalize=True):\n","    rows = int(np.ceil(np.sqrt(n_images)))\n","    cols = int(np.ceil(n_images / rows))\n","\n","    fig = plt.figure(figsize=(25, 20))\n","    # fig.suptitle(f'{model_name} - Most Incorrect\\n', fontsize=28)\n","\n","    for i in range(rows * cols):\n","        if i >= len(incorrect):\n","            break\n","        ax = fig.add_subplot(rows, cols, i + 1)\n","        image, true_label, probs = incorrect[i]\n","        image = image.permute(1, 2, 0)\n","        true_prob = probs[true_label]\n","        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n","        true_class = classes[true_label]\n","        incorrect_class = classes[incorrect_label]\n","\n","        if normalize:\n","            image = normalize_image(image)\n","\n","        ax.imshow(image.cpu().numpy())\n","        ax.set_title(f'true label:\\n{true_class} ({true_prob:.3f})\\n'\n","                     f'pred label:\\n{incorrect_class} ({incorrect_prob:.3f})', fontsize=10)\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    fig.subplots_adjust(hspace=0.7)\n","    \n","    plt.show()\n","    save_figure(fig, f'{model_name}_most_incorrect.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_all_details(model, test_loader):\n","    all_labels = []\n","    all_preds = []\n","    all_probs = []\n","    all_images = []\n","    device = next(model.parameters()).device  # Get the device of the model\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            probs = F.softmax(outputs, dim=1)\n","\n","            all_images.extend(images.cpu())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            all_probs.extend(probs.cpu())\n","\n","    return all_images, all_labels, all_preds, all_probs\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the number of images to display\n","N_IMAGES = 36"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use this function to get the details\n","def plot_most_incorrect_predictions(results, test_loader, n_images=36):\n","    classes = list(test_loader.dataset.class_to_idx.keys())\n","    for model_name, model_info in results.items():\n","        model = model_info['model']\n","        images, labels, pred_labels, probs = get_all_details(model, test_loader)\n","        corrects = torch.eq(torch.tensor(labels), torch.tensor(pred_labels))\n","        incorrect_examples = []\n","\n","        for image, label, prob, correct in zip(images, labels, probs, corrects):\n","            if not correct:\n","                incorrect_examples.append((image, label, prob))\n","\n","    incorrect_examples.sort(key=lambda x: torch.max(x[2], dim=0)[0], reverse=True)\n","    plot_most_incorrect(incorrect_examples[:n_images], classes, n_images, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_most_incorrect_predictions(crop_results, test_loader, N_IMAGES)"]},{"cell_type":"markdown","metadata":{},"source":["### Representations and Dimensionality Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import decomposition, manifold\n","\n","def get_representations(model, iterator):\n","    model.eval()\n","    outputs = []\n","    labels = []\n","\n","    with torch.no_grad():\n","        for x, y in iterator:\n","            x = x.to(device)\n","            y_pred = model(x)\n","            outputs.append(y_pred.cpu())\n","            labels.append(y)\n","\n","    outputs = torch.cat(outputs, dim=0)\n","    labels = torch.cat(labels, dim=0)\n","    return outputs, labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_pca(data, n_components=2):\n","    pca = decomposition.PCA(n_components=n_components)\n","    pca_data = pca.fit_transform(data)\n","    return pca_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_representations(data, labels, classes, n_images=None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","        labels = labels[:n_images]\n","\n","    fig = plt.figure(figsize=(15, 15))\n","    # fig.suptitle(f'{model_name} - PCA', fontsize=28, y=0.95)\n","    ax = fig.add_subplot(111)\n","    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='hsv')\n","    plt.show()\n","    save_figure(fig, f'{model_name}_pca.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs, labels = get_representations(model, train_loader)\n","\n","for model_name in crop_results.keys():\n","    output_pca_data = get_pca(outputs)\n","    plot_representations(output_pca_data, labels, classes)  # Adjusted to pass only three arguments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_tsne(data, n_components=2, n_images=None):\n","    if n_images is not None:\n","        data = data[:n_images]\n","    tsne = manifold.TSNE(n_components=n_components, random_state=0)\n","    tsne_data = tsne.fit_transform(data)\n","    return tsne_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for model_name in crop_results.keys():\n","    output_tsne_data = get_tsne(outputs)\n","    plot_representations(output_tsne_data, labels, classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Filter Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to plot filtered images\n","def plot_filtered_images(images, filters, model_name, n_filters=None, normalize=True):\n","    images = torch.cat([i.unsqueeze(0) for i in images], dim=0).cpu()\n","    filters = filters.cpu()\n","\n","    if n_filters is not None:\n","        filters = filters[:n_filters]\n","\n","    n_images = images.shape[0]\n","    n_filters = filters.shape[0]\n","\n","    filtered_images = F.conv2d(images, filters)\n","\n","    fig = plt.figure(figsize=(30, 30))\n","    # fig.suptitle(f'{model_name} - Filtered Images', fontsize=28, y=0.8)\n","\n","    for i in range(n_images):\n","        image = images[i]\n","        if normalize:\n","            image = normalize_image(image)\n","        ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters))\n","        ax.imshow(image.permute(1, 2, 0).numpy())\n","        ax.set_title('Original')\n","        ax.axis('off')\n","\n","        for j in range(n_filters):\n","            image = filtered_images[i][j]\n","            if normalize:\n","                image = normalize_image(image)\n","            ax = fig.add_subplot(n_images, n_filters + 1, i + 1 + (i * n_filters) + j + 1)\n","            ax.imshow(image.numpy(), cmap='bone')\n","            ax.set_title(f'Filter {j + 1}')\n","            ax.axis('off')\n","\n","    fig.subplots_adjust(hspace=-0.7)\n","    plt.show()\n","    save_figure(fig, f'{model_name}_filtered_images.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N_FILTERS = 7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage within the existing loop\n","conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n","\n","for model_name, model_info in crop_results.items():\n","    model = model_info['model']\n","    if model_name in conv_models:\n","        if hasattr(model, 'conv1'):\n","            filters = model.conv1.weight.data\n","        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n","            filters = model.features[0].weight.data\n","        else:\n","            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n","            filters = None\n","    else:\n","        filters = None  # No convolutional filters in models like ViT\n","\n","    if filters is not None:\n","        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n","        plot_filtered_images(images, filters, model_name, n_filters=N_FILTERS)"]},{"cell_type":"markdown","metadata":{},"source":["### Filter Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_filters(filters, normalize=True):\n","    filters = filters.cpu()\n","    n_filters = filters.shape[0]\n","    rows = int(np.sqrt(n_filters))\n","    cols = int(np.sqrt(n_filters))\n","\n","    fig = plt.figure(figsize=(30, 15))\n","    # fig.suptitle(f'{model_name} - Filters', fontsize=28, y=0.95)\n","\n","    for i in range(rows * cols):\n","        image = filters[i]\n","        if normalize:\n","            image = normalize_image(image)\n","        ax = fig.add_subplot(rows, cols, i + 1)\n","        ax.imshow(image.permute(1, 2, 0))\n","        ax.axis('off')\n","\n","    fig.subplots_adjust(wspace=-0.9)\n","    plt.show()\n","    save_figure(fig, f'{model_name}_filters.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage within the existing loop\n","conv_models = ['ResNet152', 'VGG19', 'InceptionV3', 'AttentionAugmentedInceptionV3']  # Add models expected to have conv layers\n","\n","for model_name, model_info in crop_results.items():\n","    model = model_info['model']\n","    if model_name in conv_models:\n","        if hasattr(model, 'conv1'):\n","            filters = model.conv1.weight.data\n","        elif hasattr(model, 'features') and hasattr(model.features, '0'):\n","            filters = model.features[0].weight.data\n","        else:\n","            print(f\"Model {model_name} structure is not recognized for convolutional layers.\")\n","            filters = None\n","    else:\n","        filters = None  # No convolutional filters in models like ViT\n","\n","    if filters is not None:\n","        images = [image for image, label in [train_dataset_others[i] for i in range(N_IMAGES)]]\n","        plot_filters(filters)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4056201,"sourceId":7048575,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":94664,"modelInstanceId":69528,"sourceId":82772,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
