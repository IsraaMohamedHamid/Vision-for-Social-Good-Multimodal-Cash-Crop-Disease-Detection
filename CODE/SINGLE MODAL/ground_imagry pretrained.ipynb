{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS, GPSTAGS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "from AACN_Model import attention_augmented_resnet50, attention_augmented_efficientnetb0, attention_augmented_inceptionv3, attention_augmented_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Downloads/Cherry v2'\n",
    "crop_root = os.path.join(base_dir, 'Ground_RGB_Photos/All')\n",
    "split_root = os.path.join(base_dir, 'Ground_RGB_Photos/Split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to remove .DS_Store files\n",
    "def remove_ds_store(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == '.DS_Store' or '.DS_Store' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Removing {file_path}\")\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .DS_Store files from base directory\n",
    "remove_ds_store(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    # Assuming is_image_file is a function that checks if the file is an image\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation, and test sets\n",
    "def split_data(base_dir, train_dir, val_dir, test_dir, val_split=0.2, test_split=0.1):\n",
    "\n",
    "    # Remove existing directories if they exist\n",
    "    if os.path.exists(train_dir):\n",
    "        shutil.rmtree(train_dir)\n",
    "    if os.path.exists(val_dir):\n",
    "        shutil.rmtree(val_dir)\n",
    "    if os.path.exists(test_dir):\n",
    "        shutil.rmtree(test_dir)\n",
    "\n",
    "    # Create new directories\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    for cls in classes:\n",
    "        print(f'Processing class: {cls}')\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "        \n",
    "        train_cls_dir = os.path.join(train_dir, cls)\n",
    "        val_cls_dir = os.path.join(val_dir, cls)\n",
    "        test_cls_dir = os.path.join(test_dir, cls)\n",
    "\n",
    "        os.makedirs(train_cls_dir, exist_ok=True)\n",
    "        os.makedirs(val_cls_dir, exist_ok=True)\n",
    "        os.makedirs(test_cls_dir, exist_ok=True)\n",
    "\n",
    "        images = [f for f in os.listdir(class_dir) if is_image_file(os.path.join(class_dir, f))]\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(f\"No images found for class {cls}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            train, test = train_test_split(images, test_size=test_split)\n",
    "            train, val = train_test_split(train, test_size=val_split / (1 - test_split))\n",
    "        except ValueError as e:\n",
    "            print(f\"Not enough images to split for class {cls}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for img in train:\n",
    "            shutil.copy(os.path.join(class_dir, img), os.path.join(train_cls_dir, img))\n",
    "        for img in val:\n",
    "            shutil.copy(os.path.join(class_dir, img), os.path.join(val_cls_dir, img))\n",
    "        for img in test:\n",
    "            shutil.copy(os.path.join(class_dir, img), os.path.join(test_cls_dir, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, validation, and test directories\n",
    "train_dir = os.path.join(split_root, 'train_set')\n",
    "val_dir = os.path.join(split_root, 'val_set')\n",
    "test_dir = os.path.join(split_root, 'test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class: Healthy\n",
      "Processing class: Armillaria_Stage_1\n",
      "Processing class: Armillaria_Stage_2\n",
      "Processing class: Armillaria_Stage_3\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "split_data(crop_root, train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'InceptionV3': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.CenterCrop(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    },\n",
    "    'Others': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(20),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `crops` and directories (`train_dir`, `val_dir`, `test_dir`) are defined\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a file is valid\n",
    "def is_valid_file(path):\n",
    "    return not path.endswith('.DS_Store') or 'DS_Store' not in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust learning rate\n",
    "def adjust_learning_rate(optimizer, epoch, learning_rate):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 10 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the models\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def create_and_train_model(model, train_loader, val_loader, num_classes, device, num_epochs=40, initial_lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=5e-4, momentum=0.9)\n",
    "    early_stopping_patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        adjust_learning_rate(optimizer, epoch, initial_lr)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    outputs = outputs[0]\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define crops and initialize results dictionary\n",
    "crops = ['Cherry'] #'Armillaria_Stage_1', 'Armillaria_Stage_2', 'Armillaria_Stage_3', 'Healthy'\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find classes in a directory\n",
    "def find_classes(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "        print(f\"Created directory: {dir}\")\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d)) and not d.startswith('.')]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Crop: Cherry ---------------------------------------------\n",
      "\n",
      "--------------- Training model: EfficientNetB0\n",
      "Epoch 1/40, Train Loss: 0.4094, Train Accuracy: 90.87%, Val Loss: 0.5567, Val Accuracy: 92.72%\n",
      "Epoch 2/40, Train Loss: 0.2886, Train Accuracy: 92.85%, Val Loss: 0.4340, Val Accuracy: 92.72%\n",
      "Epoch 3/40, Train Loss: 0.2623, Train Accuracy: 92.70%, Val Loss: 0.3918, Val Accuracy: 92.72%\n",
      "Epoch 4/40, Train Loss: 0.2615, Train Accuracy: 92.85%, Val Loss: 0.2980, Val Accuracy: 92.72%\n",
      "Epoch 5/40, Train Loss: 0.2418, Train Accuracy: 92.95%, Val Loss: 0.3344, Val Accuracy: 92.89%\n",
      "Epoch 6/40, Train Loss: 0.2406, Train Accuracy: 92.90%, Val Loss: 0.3261, Val Accuracy: 88.39%\n",
      "Epoch 7/40, Train Loss: 0.2300, Train Accuracy: 93.10%, Val Loss: 0.2657, Val Accuracy: 92.55%\n",
      "Epoch 8/40, Train Loss: 0.2372, Train Accuracy: 92.90%, Val Loss: 0.2541, Val Accuracy: 91.85%\n",
      "Epoch 9/40, Train Loss: 0.2209, Train Accuracy: 93.50%, Val Loss: 0.2626, Val Accuracy: 93.41%\n",
      "Epoch 10/40, Train Loss: 0.2027, Train Accuracy: 93.15%, Val Loss: 0.5360, Val Accuracy: 74.35%\n",
      "Epoch 11/40, Train Loss: 0.1879, Train Accuracy: 94.04%, Val Loss: 0.3207, Val Accuracy: 89.77%\n",
      "Epoch 12/40, Train Loss: 0.1751, Train Accuracy: 94.29%, Val Loss: 0.2389, Val Accuracy: 92.89%\n",
      "Epoch 13/40, Train Loss: 0.1610, Train Accuracy: 94.59%, Val Loss: 0.2376, Val Accuracy: 93.24%\n",
      "Epoch 14/40, Train Loss: 0.1631, Train Accuracy: 94.74%, Val Loss: 0.2408, Val Accuracy: 93.24%\n",
      "Epoch 15/40, Train Loss: 0.1458, Train Accuracy: 95.19%, Val Loss: 0.2390, Val Accuracy: 93.24%\n",
      "Epoch 16/40, Train Loss: 0.1463, Train Accuracy: 94.99%, Val Loss: 0.2341, Val Accuracy: 93.07%\n",
      "Epoch 17/40, Train Loss: 0.1380, Train Accuracy: 94.99%, Val Loss: 0.2412, Val Accuracy: 93.24%\n",
      "Epoch 18/40, Train Loss: 0.1294, Train Accuracy: 95.73%, Val Loss: 0.2365, Val Accuracy: 92.89%\n",
      "Epoch 19/40, Train Loss: 0.1404, Train Accuracy: 94.84%, Val Loss: 0.5866, Val Accuracy: 93.93%\n",
      "Epoch 20/40, Train Loss: 0.1200, Train Accuracy: 95.78%, Val Loss: 0.2368, Val Accuracy: 92.20%\n",
      "Epoch 21/40, Train Loss: 0.1212, Train Accuracy: 95.93%, Val Loss: 0.2362, Val Accuracy: 93.07%\n",
      "Early stopping due to no improvement in validation loss.\n",
      "Cherry - EfficientNetB0 Test Loss: 0.2367, Test Accuracy: 91.03%\n",
      "\n",
      "\n",
      "--------------- Training model: InceptionV3\n",
      "Epoch 1/40, Train Loss: 0.3539, Train Accuracy: 92.16%, Val Loss: 0.3078, Val Accuracy: 92.72%\n",
      "Epoch 2/40, Train Loss: 0.3094, Train Accuracy: 92.90%, Val Loss: 0.4489, Val Accuracy: 92.72%\n",
      "Epoch 3/40, Train Loss: 0.3212, Train Accuracy: 92.85%, Val Loss: 24.4161, Val Accuracy: 92.72%\n",
      "Epoch 4/40, Train Loss: 0.3411, Train Accuracy: 92.80%, Val Loss: 0.5863, Val Accuracy: 92.72%\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each crop\n",
    "for crop in crops:\n",
    "    crop_train_dir = train_dir  # os.path.join(train_dir, crop)\n",
    "    crop_val_dir = val_dir  # os.path.join(val_dir, crop)\n",
    "    crop_test_dir = test_dir  # os.path.join(test_dir, crop)\n",
    "\n",
    "    if not os.path.exists(crop_train_dir) or not os.listdir(crop_train_dir):\n",
    "        print(f\"No data found in training directory {crop_train_dir}. Skipping {crop}.\")\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(crop_val_dir) or not os.listdir(crop_val_dir):\n",
    "        print(f\"No data found in validation directory {crop_val_dir}. Skipping {crop}.\")\n",
    "        continue\n",
    "\n",
    "    # Load datasets with the new transformations\n",
    "    train_dataset_inception = datasets.ImageFolder(crop_train_dir, transform=data_transforms['InceptionV3']['train'])\n",
    "    val_dataset_inception = datasets.ImageFolder(crop_val_dir, transform=data_transforms['InceptionV3']['val'])\n",
    "    test_dataset_inception = datasets.ImageFolder(crop_test_dir, transform=data_transforms['InceptionV3']['test'])\n",
    "\n",
    "    train_loader_inception = DataLoader(train_dataset_inception, batch_size=32, shuffle=True)\n",
    "    val_loader_inception = DataLoader(val_dataset_inception, batch_size=32, shuffle=True)\n",
    "    test_loader_inception = DataLoader(test_dataset_inception, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Loaders for other models\n",
    "    train_dataset_others = datasets.ImageFolder(crop_train_dir, transform=data_transforms['Others']['train'])\n",
    "    val_dataset_others = datasets.ImageFolder(crop_val_dir, transform=data_transforms['Others']['val'])\n",
    "    test_dataset_others = datasets.ImageFolder(crop_test_dir, transform=data_transforms['Others']['test'])\n",
    "\n",
    "    train_loader_others = DataLoader(train_dataset_others, batch_size=32, shuffle=True)\n",
    "    val_loader_others = DataLoader(val_dataset_others, batch_size=32, shuffle=True)\n",
    "    test_loader_others = DataLoader(test_dataset_others, batch_size=32, shuffle=False)\n",
    "\n",
    "    num_classes_inception = len(train_dataset_inception.classes)\n",
    "    num_classes_others = len(train_dataset_others.classes)\n",
    " \n",
    "    num_heads = 8\n",
    "\n",
    "    test_dataset_inception.class_to_idx = train_dataset_inception.class_to_idx\n",
    "    test_dataset_others.class_to_idx = train_dataset_others.class_to_idx\n",
    "\n",
    "    pretrained_models = {\n",
    "        'EfficientNetB0': EfficientNet.from_pretrained('efficientnet-b0'),\n",
    "        'InceptionV3': models.inception_v3(pretrained=True),\n",
    "        'ResNet50': models.resnet50(pretrained=True),\n",
    "        \"AttentionAugmentedResNet50\": attention_augmented_resnet50(num_classes=num_classes_others, attention=[False,True,True,True], num_heads=num_heads),\n",
    "        \"AttentionAugmentedInceptionV3\": attention_augmented_inceptionv3(attention=True),\n",
    "        # \"AttentionAugmentedEfficientNetB0\": attention_augmented_efficientnetb0(attention=True),\n",
    "        # \"AttentionAugmentedViT\": attention_augmented_vit(attention=True),\n",
    "        'ViT': ViT(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_classes=num_classes_others,\n",
    "            dim=1024,\n",
    "            depth=6,\n",
    "            heads=16,\n",
    "            mlp_dim=2048,\n",
    "            dropout=0.1,\n",
    "            emb_dropout=0.1\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    crop_results = {}\n",
    "\n",
    "    print(f'--------------------------------------------- Crop: {crop} ---------------------------------------------\\n')\n",
    "\n",
    "    for model_name, base_model in pretrained_models.items():\n",
    "        if model_name == 'InceptionV3':\n",
    "            base_model.AuxLogits.fc = nn.Linear(base_model.AuxLogits.fc.in_features, num_classes_inception)\n",
    "            base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_inception)\n",
    "            train_loader = train_loader_inception\n",
    "            val_loader = val_loader_inception\n",
    "            test_loader = test_loader_inception\n",
    "        elif model_name == 'EfficientNetB0':\n",
    "            base_model._fc = nn.Linear(base_model._fc.in_features, num_classes_others)\n",
    "            train_loader = train_loader_others\n",
    "            val_loader = val_loader_others\n",
    "            test_loader = test_loader_others\n",
    "        elif model_name == 'ViT':\n",
    "            base_model.mlp_head = nn.Linear(base_model.mlp_head.in_features, num_classes_others)\n",
    "            train_loader = train_loader_others\n",
    "            val_loader = val_loader_others\n",
    "            test_loader = test_loader_others\n",
    "        elif model_name == 'ResNet50':\n",
    "            base_model.fc = nn.Linear(base_model.fc.in_features, num_classes_others)\n",
    "            train_loader = train_loader_others\n",
    "            val_loader = val_loader_others\n",
    "            test_loader = test_loader_others\n",
    "        else:\n",
    "            train_loader = train_loader_others\n",
    "            val_loader = val_loader_others\n",
    "            test_loader = test_loader_others\n",
    "\n",
    "        print(f'--------------- Training model: {model_name}')\n",
    "        model = create_and_train_model(base_model, train_loader, val_loader, num_classes_others, device, initial_lr=0.001)\n",
    "\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, nn.CrossEntropyLoss(), device)\n",
    "\n",
    "        crop_results[model_name] = {\n",
    "            'model': model,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "        print(f'{crop} - {model_name} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "        print(f'\\n')\n",
    "\n",
    "    results[crop] = crop_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "for crop, crop_results in results.items():\n",
    "    accuracies = [result['test_accuracy'] for result in crop_results.values()]\n",
    "    model_names = list(crop_results.keys())\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(model_names, accuracies)\n",
    "    plt.title(f'Model test accuracy comparison for {crop}')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display F1, precision, and recall of all models as a table\n",
    "def display_model_metrics_table(results, test_loader):\n",
    "    metrics_data = []\n",
    "    \n",
    "    for crop, crop_results in results.items():\n",
    "        for model_name, model_info in crop_results.items():\n",
    "            model = model_info['model']\n",
    "            device = next(model.parameters()).device  # Get the device of the model\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "            all_labels = []\n",
    "            all_predicted = []\n",
    "\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n",
    "            \n",
    "            metrics_data.append({\n",
    "                'Crop': crop,\n",
    "                'Model': model_name,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-score': f1\n",
    "            })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    display(metrics_df)  # Display the DataFrame in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the classification report of a given model\n",
    "def display_classification_report(model, test_loader):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predicted, target_names=test_loader.dataset.classes)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, test_loader, num_images=5):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    class_labels = test_loader.dataset.classes\n",
    "    \n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images]  # Move tensors to the model's device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 8))\n",
    "    fig.suptitle('Classification Results', fontsize=16)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))  # Move tensor back to CPU for visualization\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'True: {class_labels[labels[i]]}\\n Pred: {class_labels[predicted[i].cpu()]}')  # Access CPU tensor for labels\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table of metrics for all models\n",
    "display_model_metrics_table(results, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for crop, crop_results in results.items():\n",
    "    for model_name in crop_results.keys():\n",
    "        print(f'Displaying results for {crop} - {model_name}')\n",
    "        display_classification_results(crop_results[model_name]['model'], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each crop\n",
    "for crop, crop_results in results.items():\n",
    "    for model_name in crop_results.keys():\n",
    "        print(f'Displaying classification report for {crop} - {model_name}')\n",
    "        display_classification_report(crop_results[model_name]['model'], test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
