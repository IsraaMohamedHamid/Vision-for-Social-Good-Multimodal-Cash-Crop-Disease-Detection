{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2, InceptionV3, ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main directories\n",
    "base_dir = '/Users/izzymohamed/Downloads/Dataset for Crop Pest and Disease Detection/CCMT Dataset-Augmented'\n",
    "\n",
    "# Define crop directories\n",
    "crop_root = base_dir + '/Cashew'\n",
    "\n",
    "# Define train and test directories\n",
    "train_set_dir = crop_root + '/train_set'\n",
    "test_set_dir = crop_root + '/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all directories inside the main root\n",
    "def list_directories(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        level = root.replace(path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "\n",
    "# Call the function with the path you want to explore\n",
    "# list_directories('base_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split training data into training and validation sets\n",
    "def split_train_val(base_train_dir, train_dir, val_dir, val_split=0.2):\n",
    "    classes = os.listdir(base_train_dir)\n",
    "    for cls in classes:\n",
    "        # print(cls)\n",
    "        # If cls is .DS_Store continue\n",
    "        if cls == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        class_train_dir = os.path.join(base_train_dir, cls)\n",
    "        os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
    "        \n",
    "        images = os.listdir(class_train_dir)\n",
    "        train, val = train_test_split(images, test_size=val_split)\n",
    "        \n",
    "        for img in train:\n",
    "            shutil.copy(os.path.join(class_train_dir, img), os.path.join(train_dir, cls, img))\n",
    "        for img in val:\n",
    "            shutil.copy(os.path.join(class_train_dir, img), os.path.join(val_dir, cls, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train model\n",
    "def create_and_train_model(base_model, train_generator, validation_generator, num_classes, epochs=10, fine_tune_epochs=5):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator\n",
    "    )\n",
    "    \n",
    "    for layer in base_model.layers[-50:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history_fine = model.fit(\n",
    "        train_generator,\n",
    "        epochs=fine_tune_epochs,\n",
    "        validation_data=validation_generator\n",
    "    )\n",
    "    \n",
    "    return model, history, history_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each crop directory and train models\n",
    "crops = ['Cassava', 'Tomato', 'Cashew', 'Maize']\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing crop: Cassava\n",
      "Found 22558 images belonging to 7 classes.\n",
      "Found 10692 images belonging to 7 classes.\n",
      "Found 7510 images belonging to 5 classes.\n",
      "Training model: MobileNetV2 for crop: Cassava\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 852ms/step - accuracy: 0.5789 - loss: 1.1473 - val_accuracy: 0.6721 - val_loss: 0.8560\n",
      "Epoch 2/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 849ms/step - accuracy: 0.6769 - loss: 0.8258 - val_accuracy: 0.7045 - val_loss: 0.7577\n",
      "Epoch 3/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 815ms/step - accuracy: 0.6942 - loss: 0.7747 - val_accuracy: 0.7183 - val_loss: 0.7228\n",
      "Epoch 4/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 853ms/step - accuracy: 0.7035 - loss: 0.7495 - val_accuracy: 0.7415 - val_loss: 0.6507\n",
      "Epoch 5/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 797ms/step - accuracy: 0.7128 - loss: 0.7195 - val_accuracy: 0.7624 - val_loss: 0.6135\n",
      "Epoch 6/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 914ms/step - accuracy: 0.7215 - loss: 0.6972 - val_accuracy: 0.7519 - val_loss: 0.6149\n",
      "Epoch 7/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 755ms/step - accuracy: 0.7201 - loss: 0.6922 - val_accuracy: 0.7374 - val_loss: 0.6605\n",
      "Epoch 8/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 1s/step - accuracy: 0.7291 - loss: 0.6679 - val_accuracy: 0.7589 - val_loss: 0.6157\n",
      "Epoch 9/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 1s/step - accuracy: 0.7372 - loss: 0.6578 - val_accuracy: 0.7569 - val_loss: 0.6129\n",
      "Epoch 10/10\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 874ms/step - accuracy: 0.7402 - loss: 0.6426 - val_accuracy: 0.7227 - val_loss: 0.7325\n",
      "Epoch 1/5\n",
      "\u001b[1m572/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2:08\u001b[0m 965ms/step - accuracy: 0.6866 - loss: 0.8267"
     ]
    }
   ],
   "source": [
    "# Loop through each crop directory and train models\n",
    "for crop in crops:\n",
    "    print(f'Processing crop: {crop}')\n",
    "    \n",
    "    crop_train_dir = os.path.join(base_dir, crop, 'train_set')\n",
    "    crop_test_dir = os.path.join(base_dir, crop, 'test_set')\n",
    "    \n",
    "    train_dir = f'temp_{crop}_train'\n",
    "    validation_dir = f'temp_{crop}_val'\n",
    "    \n",
    "    split_train_val(crop_train_dir, train_dir, validation_dir)\n",
    "    \n",
    "    # ImageDataGenerator for data augmentation and preprocessing\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Data generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        crop_test_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    \n",
    "    # Models to compare\n",
    "    pretrained_models = {\n",
    "        'MobileNetV2': MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
    "        'InceptionV3': InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
    "        'ResNet50': ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    }\n",
    "\n",
    "    crop_results = {}\n",
    "\n",
    "    for model_name, base_model in pretrained_models.items():\n",
    "        print(f'Training model: {model_name} for crop: {crop}')\n",
    "        model, history, history_fine = create_and_train_model(base_model, train_generator, validation_generator, num_classes)\n",
    "        loss, accuracy = model.evaluate(test_generator)\n",
    "        crop_results[model_name] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'history_fine': history_fine,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    results[crop] = crop_results\n",
    "\n",
    "    # Clean up temporary directories\n",
    "    shutil.rmtree(train_dir)\n",
    "    shutil.rmtree(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of accuracy for each model for each crop\n",
    "for crop, crop_results in results.items():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model_name, result in crop_results.items():\n",
    "        plt.plot(result['history'].history['val_accuracy'] + result['history_fine'].history['val_accuracy'], label=model_name)\n",
    "\n",
    "    plt.title(f'Model validation accuracy comparison for {crop}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Print test accuracy for each model\n",
    "    for model_name, result in crop_results.items():\n",
    "        print(f'{crop} - {model_name} Test Accuracy: {result[\"accuracy\"]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some correctly and incorrectly classified images\n",
    "def display_classification_results(model, generator, num_images=5):\n",
    "    class_labels = list(generator.class_indices.keys())\n",
    "    images, labels = next(generator)\n",
    "    predictions = model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(labels, axis=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(20, 8))\n",
    "    fig.suptitle('Correctly and Incorrectly Classified Images', fontsize=16)\n",
    "    \n",
    "    correct = np.where(predicted_classes == true_classes)[0]\n",
    "    incorrect = np.where(predicted_classes != true_classes)[0]\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        if i < len(correct):\n",
    "            axes[0, i].imshow(images[correct[i]])\n",
    "            axes[0, i].set_title(f'True: {class_labels[true_classes[correct[i]]]}, Pred: {class_labels[predicted_classes[correct[i]]]}')\n",
    "            axes[0, i].axis('off')\n",
    "        if i < len(incorrect):\n",
    "            axes[1, i].imshow(images[incorrect[i]])\n",
    "            axes[1, i].set_title(f'True: {class_labels[true_classes[incorrect[i]]]}, Pred: {class_labels[predicted_classes[incorrect[i]]]}')\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for MobileNetV2 for each crop\n",
    "for crop, crop_results in results.items():\n",
    "    print(f'Displaying results for {crop} - MobileNetV2')\n",
    "    display_classification_results(crop_results['MobileNetV2']['model'], test_generator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
